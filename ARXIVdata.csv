title,link,description
Parametrized Quantum Circuits of Synonymous Sentences in Quantum Natural Language Processing. (arXiv:2102.02204v1 [quant-ph]),http://arxiv.org/abs/2102.02204,"<p>In this paper, we develop a compositional vector-based semantics of positive
transitive sentences in quantum natural language processing for a non-English
language, i.e. Persian, to compare the parametrized quantum circuits of two
synonymous sentences in two languages, English and Persian. By considering
grammar+meaning of a transitive sentence, we translate DisCoCat diagram via
ZX-calculus into quantum circuit form. Also, we use a bigraph method to rewrite
DisCoCat diagram and turn into quantum circuit in the semantic side.
</p>
"
Harvest -- An Open Source Toolkit for Extracting Posts and Post Metadata from Web Forums. (arXiv:2102.02240v1 [cs.IR]),http://arxiv.org/abs/2102.02240,"<p>Automatic extraction of forum posts and metadata is a crucial but challenging
task since forums do not expose their content in a standardized structure.
Content extraction methods, therefore, often need customizations such as
adaptations to page templates and improvements of their extraction code before
they can be deployed to new forums. Most of the current solutions are also
built for the more general case of content extraction from web pages and lack
key features important for understanding forum content such as the
identification of author metadata and information on the thread structure.
</p>
<p>This paper, therefore, presents a method that determines the XPath of forum
posts, eliminating incorrect mergers and splits of the extracted posts that
were common in systems from the previous generation. Based on the individual
posts further metadata such as authors, forum URL and structure are extracted.
We also introduce Harvest, a new open source toolkit that implements the
presented methods and create a gold standard extracted from 52 different Web
forums for evaluating our approach. A comprehensive evaluation reveals that
Harvest clearly outperforms competing systems.
</p>
"
Information-theoretic Key Encapsulation and its Application to Secure Communication. (arXiv:2102.02243v1 [cs.CR]),http://arxiv.org/abs/2102.02243,"<p>A hybrid encryption scheme is a public key encryption system that consists of
a public-key part called the key encapsulation mechanism (KEM), and a
(symmetric) secret-key part called data encapsulation mechanism (DEM): the
public-key part is used to generate a shared secret key between the two
parties, and the symmetric key part is used to encrypt the message. Hybrid
encryption schemes are widely used for secure communication over the Internet.
In this paper, we initiate the study of hybrid encryption in preprocessing
model which assumes access to initial correlated variables by all the parties
(including the eavesdropper). We define information theoretic KEM (iKEM) that
together with a (computationally) secure DEM results in a hybrid encryption
scheme in preprocessing model. We define security of each building block and
prove a composition theorem that guarantees security of the final encryption
system. We showthat iKEM can be realized by a one-message SKA (OW-SKA)protocol
with an extended security definition. Using a OW-SKA that satisfies this
extended definition of security effectively allows the secret key that is
generated by the OW-SKA to be used with symmetric key encryption system such as
AES in counter mode. We discuss our results and future work including providing
stronger security for the final encryption, and using information theoretic DEM
to construct information theoretic encryption systems.
</p>
"
Bounds and Genericity of Sum-Rank-Metric Codes. (arXiv:2102.02244v1 [cs.IT]),http://arxiv.org/abs/2102.02244,"<p>We derive simplified sphere-packing and Gilbert-Varshamov bounds for codes in
the sum-rank metric, which can be computed more efficently than previous
ones.They give rise to asymptotic bounds that cover the asymptotic setting that
has not yet been considered in the literature: families of sum-rank-metric
codes whose block size grows in the code length. We also provide two genericity
results: we show that random linear codes achieve almost the sum-rank-metric
Gilbert-Varshamov bound with high probability. Furthermore, we derive bounds on
the probability that a random linear code attains the sum-rank-metric Singleton
bound, showing that for large enough extension field, almost all linear codes
achieve it.
</p>
"
The Forgotten Document-Oriented Database Management Systems: An Overview and Benchmark of Native XML DODBMSes in Comparison with JSON DODBMSes. (arXiv:2102.02246v1 [cs.DB]),http://arxiv.org/abs/2102.02246,"<p>In the current context of Big Data, a multitude of new NoSQL solutions for
storing, managing, and extracting information and patterns from semi-structured
data have been proposed and implemented. These solutions were developed to
relieve the issue of rigid data structures present in relational databases, by
introducing semi-structured and flexible schema design. As current data
generated by different sources and devices, especially from IoT sensors and
actuators, use either XML or JSON format, depending on the application,
database technologies that store and query semi-structured data in XML format
are needed. Thus, Native XML Databases, which were initially designed to
manipulate XML data using standardized querying languages, i.e., XQuery and
XPath, were rebranded as NoSQL Document-Oriented Databases Systems. Currently,
the majority of these solutions have been replaced with the more modern JSON
based Database Management Systems. However, we believe that XML-based solutions
can still deliver performance in executing complex queries on heterogeneous
collections. Unfortunately nowadays, research lacks a clear comparison of the
scalability and performance for database technologies that store and query
documents in XML versus the more modern JSON format. Moreover, to the best of
our knowledge, there are no Big Data-compliant benchmarks for such database
technologies. In this paper, we present a comparison for selected
Document-Oriented Database Systems that either use the XML format to encode
documents, i.e., BaseX, eXist-db, and Sedna, or the JSON format, i.e., MongoDB,
CouchDB, and Couchbase. To underline the performance differences we also
propose a benchmark that uses a heterogeneous complex schema on a large DBLP
corpus.
</p>
"
Low-cost attacks on Ethereum 2.0 by sub-1/3 stakeholders. (arXiv:2102.02247v1 [cs.CR]),http://arxiv.org/abs/2102.02247,"<p>We outline two dishonest strategies that can be cheaply executed on the
Ethereum 2.0 beacon chain, even by validators holding less than one-third of
the total stake: malicious chain reorganizations (""reorgs"") and finality
delays. In a malicious reorg, an attacker withholds their blocks and
attestations before releasing them at an opportune time in order to force a
chain reorganization, which they can take advantage of by double-spending or
front-running transactions. To execute a finality delay an attacker uses
delayed block releases and withholding of attestations to increase the mean and
variance of the time it takes blocks to become finalized. This impacts the
efficiency and predictability of the system. We provide a probabilistic and
cost analysis for each of these attacks, considering a validator with 30% of
the total stake.
</p>
"
Optimal Excitation Matching Strategy for Sub-Arrayed Phased Linear Arrays Generating Arbitrary-Shaped Beams. (arXiv:2102.02258v1 [eess.SY]),http://arxiv.org/abs/2102.02258,"<p>The design of phased arrays able to generate arbitrary-shaped beams through a
sub-arrayed architecture is addressed here. The synthesis problem is cast in
the excitation matching framework, so as to yield clustered phased arrays
providing optimal trade-offs between the complexity of the array architecture
(i.e., the minimum number of control points at the sub-array level) and the
matching of a reference pattern. A synthesis tool based on the k-means
algorithm is proposed for jointly optimizing the sub-array configuration and
the complex sub-array coefficients. Selected numerical results, including
pencil beams with sidelobe notches and asymmetric lobes as well as shaped main
lobes, are reported and discussed to highlight the peculiarities of the
proposed approach also in comparison with some extensions to complex
excitations of state-of-the-art sub-array design methods.
</p>
"
Modular Design of Hexagonal Phased Arrays Through Diamond Tiles. (arXiv:2102.02262v1 [eess.SY]),http://arxiv.org/abs/2102.02262,"<p>The modular design of planar phased array antennas with hexagonal apertures
is addressed by means of innovative diamond-shaped tiling techniques. Both
tiling configuration and subarray coefficients are optimized to fit
user-defined power-mask constraints on the radiation pattern. Toward this end,
suitable surface-tiling mathematical theorems are customized to the problem at
hand to guarantee optimal performance in case of low/medium-size arrays, while
the computationally hard tiling of large arrays is yielded thanks to an
effective integer-coded GA-based exploration of the arising high-cardinality
solution spaces. By considering ideal as well as real array models, a set of
representative benchmark problems is dealt with to assess the effectiveness of
the proposed architectures and tiling strategies. Moreover, comparisons with
alternative tiling architectures are also performed to show to the interested
readers the advantages and the potentialities of the diamond subarraying of
hexagonal apertures.
</p>
"
DEFT: Detection Embeddings for Tracking. (arXiv:2102.02267v1 [cs.CV]),http://arxiv.org/abs/2102.02267,"<p>Most modern multiple object tracking (MOT) systems follow the
tracking-by-detection paradigm, consisting of a detector followed by a method
for associating detections into tracks. There is a long history in tracking of
combining motion and appearance features to provide robustness to occlusions
and other challenges, but typically this comes with the trade-off of a more
complex and slower implementation. Recent successes on popular 2D tracking
benchmarks indicate that top-scores can be achieved using a state-of-the-art
detector and relatively simple associations relying on single-frame spatial
offsets -- notably outperforming contemporary methods that leverage learned
appearance features to help re-identify lost tracks. In this paper, we propose
an efficient joint detection and tracking model named DEFT, or ""Detection
Embeddings for Tracking."" Our approach relies on an appearance-based object
matching network jointly-learned with an underlying object detection network.
An LSTM is also added to capture motion constraints. DEFT has comparable
accuracy and speed to the top methods on 2D online tracking leaderboards while
having significant advantages in robustness when applied to more challenging
tracking data. DEFT raises the bar on the nuScenes monocular 3D tracking
challenge, more than doubling the performance of the previous top method. Code
is publicly available.
</p>
"
A Heuristic for Dynamic Output Predictive Control Design for Uncertain Nonlinear Systems. (arXiv:2102.02268v1 [eess.SY]),http://arxiv.org/abs/2102.02268,"<p>In this paper, a simple heuristic is proposed for the design of uncertainty
aware predictive controllers for nonlinear models involving uncertain
parameters. The method relies on Machine Learning-based approximation of ideal
deterministic MPC solutions with perfectly known parameters. An efficient
construction of the learning data set from these off-line solutions is proposed
in which each solution provides many samples in the learning data. This enables
a drastic reduction of the required number of Non Linear Programming problems
to be solved off-line while explicitly exploiting the statistics of the
parameters dispersion. The learning data is then used to design a fast on-line
output dynamic feedback that explicitly incorporate information of the
statistics of the parameters dispersion. An example is provided to illustrate
the efficiency and the relevance of the proposed framework. It is in particular
shown that the proposed solution recovers up to 78\% of the expected advantage
of having a perfect knowledge of the parameters compared to nominal design.
</p>
"
Numerical Differentiation using local Chebyshev-Approximation. (arXiv:2102.02269v1 [math.NA]),http://arxiv.org/abs/2102.02269,"<p>In applied mathematics, especially in optimization, functions are often only
provided as so called ""Black-Boxes"" provided by software packages, or very
complex algorithms, which make automatic differentation very complicated or
even impossible. Hence one seeks the numerical approximation of the derivative.
</p>
<p>Unfortunately numerical differentation is a difficult task in itself, and it
is well known that it is numerical instable. There are many works on this
topic, including the usage of (global) Chebyshev approximations. Chebyshev
approximations have the great property that they converge very fast, if the
function is smooth. Nevertheless those approches have several drawbacks, since
in practice functions are not smooth, and a global approximation needs many
function evalutions.
</p>
<p>Nevertheless there is hope. Since functions in real world applications are
most times smooth except for finite points, corners or edges. This motivates to
use a local Chebyshev approach, where the function is only approximated
locally, and hence the Chebyshev approximations still yields a fast
approximation of the desired function. We will study such an approch in this
work, and will provide a numerical example
</p>
"
Confusion2vec 2.0: Enriching Ambiguous Spoken Language Representations with Subwords. (arXiv:2102.02270v1 [cs.CL]),http://arxiv.org/abs/2102.02270,"<p>Word vector representations enable machines to encode human language for
spoken language understanding and processing. Confusion2vec, motivated from
human speech production and perception, is a word vector representation which
encodes ambiguities present in human spoken language in addition to semantics
and syntactic information. Confusion2vec provides a robust spoken language
representation by considering inherent human language ambiguities. In this
paper, we propose a novel word vector space estimation by unsupervised learning
on lattices output by an automatic speech recognition (ASR) system. We encode
each word in confusion2vec vector space by its constituent subword character
n-grams. We show the subword encoding helps better represent the acoustic
perceptual ambiguities in human spoken language via information modeled on
lattice structured ASR output. The usefulness of the proposed Confusion2vec
representation is evaluated using semantic, syntactic and acoustic analogy and
word similarity tasks. We also show the benefits of subword modeling for
acoustic ambiguity representation on the task of spoken language intent
detection. The results significantly outperform existing word vector
representations when evaluated on erroneous ASR outputs. We demonstrate that
Confusion2vec subword modeling eliminates the need for retraining/adapting the
natural language understanding models on ASR transcripts.
</p>
"
Neural Recursive Belief States in Multi-Agent Reinforcement Learning. (arXiv:2102.02274v1 [cs.LG]),http://arxiv.org/abs/2102.02274,"<p>In multi-agent reinforcement learning, the problem of learning to act is
particularly difficult because the policies of co-players may be heavily
conditioned on information only observed by them. On the other hand, humans
readily form beliefs about the knowledge possessed by their peers and leverage
beliefs to inform decision-making. Such abilities underlie individual success
in a wide range of Markov games, from bluffing in Poker to conditional
cooperation in the Prisoner's Dilemma, to convention-building in Bridge.
Classical methods are usually not applicable to complex domains due to the
intractable nature of hierarchical beliefs (i.e. beliefs of other agents'
beliefs). We propose a scalable method to approximate these belief structures
using recursive deep generative models, and to use the belief models to obtain
representations useful to acting in complex tasks. Our agents trained with
belief models outperform model-free baselines with equivalent representational
capacity using common training paradigms. We also show that higher-order belief
models outperform agents with lower-order models.
</p>
"
Modelling and simulation of a wave energy converter. (arXiv:2102.02275v1 [math.AP]),http://arxiv.org/abs/2102.02275,"<p>In this work we present the mathematical model and simulations of a
particular wave energy converter, the so-called oscillating water column. In
this device, waves governed by the one-dimensional nonlinear shallow water
equations arrive from offshore, encounter a step in the bottom and then arrive
into a chamber to change the volume of the air to activate the turbine. The
system is reformulated as two transmission problems: one is related to the wave
motion over the stepped topography and the other one is related to the
wave-structure interaction at the entrance of the chamber. We finally use the
characteristic equations of Riemann invariants to obtain the discretized
transmission conditions and we implement the Lax-Friedrichs scheme to get
numerical solutions.
</p>
"
Insiders and Outsiders in Research on Machine Learning and Society. (arXiv:2102.02279v1 [cs.CY]),http://arxiv.org/abs/2102.02279,"<p>A subset of machine learning research intersects with societal issues,
including fairness, accountability and transparency, as well as the use of
machine learning for social good. In this work, we analyze the scholars
contributing to this research at the intersection of machine learning and
society through the lens of the sociology of science. By analyzing the
authorship of all machine learning papers posted to arXiv, we show that
compared to researchers from overrepresented backgrounds (defined by gender and
race/ethnicity), researchers from underrepresented backgrounds are more likely
to conduct research at this intersection than other kinds of machine learning
research. This state of affairs leads to contention between two perspectives on
insiders and outsiders in the scientific enterprise: outsiders being those
outside the group being studied, and outsiders being those who have not
participated as researchers in an area historically. This contention manifests
as an epistemic question on the validity of knowledge derived from lived
experience in machine learning research, and predicts boundary work that we see
in a real-world example.
</p>
"
Downbeat Tracking with Tempo-Invariant Convolutional Neural Networks. (arXiv:2102.02282v1 [cs.SD]),http://arxiv.org/abs/2102.02282,"<p>The human ability to track musical downbeats is robust to changes in tempo,
and it extends to tempi never previously encountered. We propose a
deterministic time-warping operation that enables this skill in a convolutional
neural network (CNN) by allowing the network to learn rhythmic patterns
independently of tempo. Unlike conventional deep learning approaches, which
learn rhythmic patterns at the tempi present in the training dataset, the
patterns learned in our model are tempo-invariant, leading to better tempo
generalisation and more efficient usage of the network capacity. We test the
generalisation property on a synthetic dataset created by rendering the Groove
MIDI Dataset using FluidSynth, split into a training set containing the
original performances and a test set containing tempo-scaled versions rendered
with different SoundFonts (test-time augmentation). The proposed model
generalises nearly perfectly to unseen tempi (F-measure of 0.89 on both
training and test sets), whereas a comparable conventional CNN achieves similar
accuracy only for the training set (0.89) and drops to 0.54 on the test set.
The generalisation advantage of the proposed model extends to real music, as
shown by results on the GTZAN and Ballroom datasets.
</p>
"
HiCOPS: High Performance Computing Framework for Tera-Scale Database Search of Mass Spectrometry based Omics Data. (arXiv:2102.02286v1 [cs.DC]),http://arxiv.org/abs/2102.02286,"<p>Database-search algorithms, that deduce peptides from Mass Spectrometry (MS)
data, have tried to improve the computational efficiency to accomplish larger,
and more complex systems biology studies. Existing serial, and high-performance
computing (HPC) search engines, otherwise highly successful, are known to
exhibit poor-scalability with increasing size of theoretical search-space
needed for increased complexity of modern non-model, multi-species MS-based
omics analysis. Consequently, the bottleneck for computational techniques is
the communication costs of moving the data between hierarchy of memory, or
processing units, and not the arithmetic operations. This post-Moore change in
architecture, and demands of modern systems biology experiments have dampened
the overall effectiveness of the existing HPC workflows. We present a novel
efficient parallel computational method, and its implementation on
memory-distributed architectures for peptide identification tool called HiCOPS,
that enables more than 100-fold improvement in speed over most existing HPC
proteome database search tools. HiCOPS empowers the supercomputing database
search concept for comprehensive identification of peptides, and all their
modified forms within a reasonable time-frame. We demonstrate this by searching
Gigabytes of experimental MS data against Terabytes of databases where HiCOPS
completes peptide identification in few minutes using 72 parallel nodes (1728
cores) compared to several weeks required by existing state-of-the-art tools
using 1 node (24 cores); 100 minutes vs 5 weeks; 500x speedup. Finally, we
formulate a theoretical framework for our overhead-avoiding strategy, and
report superior performance evaluation results for key metrics including
execution time, CPU utilization, speedups, and I/O efficiency. We also
demonstrate superior performance as compared to all existing HPC strategies.
</p>
"
Echo-SyncNet: Self-supervised Cardiac View Synchronization in Echocardiography. (arXiv:2102.02287v1 [cs.CV]),http://arxiv.org/abs/2102.02287,"<p>In echocardiography (echo), an electrocardiogram (ECG) is conventionally used
to temporally align different cardiac views for assessing critical
measurements. However, in emergencies or point-of-care situations, acquiring an
ECG is often not an option, hence motivating the need for alternative temporal
synchronization methods. Here, we propose Echo-SyncNet, a self-supervised
learning framework to synchronize various cross-sectional 2D echo series
without any external input. The proposed framework takes advantage of both
intra-view and inter-view self supervisions. The former relies on
spatiotemporal patterns found between the frames of a single echo cine and the
latter on the interdependencies between multiple cines. The combined
supervisions are used to learn a feature-rich embedding space where multiple
echo cines can be temporally synchronized. We evaluate the framework with
multiple experiments: 1) Using data from 998 patients, Echo-SyncNet shows
promising results for synchronizing Apical 2 chamber and Apical 4 chamber
cardiac views; 2) Using data from 3070 patients, our experiments reveal that
the learned representations of Echo-SyncNet outperform a supervised deep
learning method that is optimized for automatic detection of fine-grained
cardiac phase; 3) We show the usefulness of the learned representations in a
one-shot learning scenario of cardiac keyframe detection. Without any
fine-tuning, keyframes in 1188 validation patient studies are identified by
synchronizing them with only one labeled reference study. We do not make any
prior assumption about what specific cardiac views are used for training and
show that Echo-SyncNet can accurately generalize to views not present in its
training set. Project repository: github.com/fatemehtd/Echo-SyncNet.
</p>
"
Nearest Neighbor-based Importance Weighting. (arXiv:2102.02291v1 [cs.LG]),http://arxiv.org/abs/2102.02291,"<p>Importance weighting is widely applicable in machine learning in general and
in techniques dealing with data covariate shift problems in particular. A
novel, direct approach to determine such importance weighting is presented. It
relies on a nearest neighbor classification scheme and is relatively
straightforward to implement. Comparative experiments on various classification
tasks demonstrate the effectiveness of our so-called nearest neighbor weighting
(NNeW) scheme. Considering its performance, our procedure can act as a simple
and effective baseline method for importance weighting.
</p>
"
Predicting the probability distribution of bus travel time to move towards reliable planning of public transport services. (arXiv:2102.02292v1 [cs.LG]),http://arxiv.org/abs/2102.02292,"<p>An important aspect of the quality of a public transport service is its
reliability, which is defined as the invariability of the service attributes.
Preventive measures taken during planning can reduce risks of unreliability
throughout operations. In order to tackle reliability during the service
planning phase, a key piece of information is the long-term prediction of the
density of the travel time, which conveys the uncertainty of travel times. We
introduce a reliable approach to one of the problems of service planning in
public transport, namely the Multiple Depot Vehicle Scheduling Problem (MDVSP),
which takes as input a set of trips and the probability density function
(p.d.f.) of the travel time of each trip in order to output delay-tolerant
vehicle schedules. This work empirically compares probabilistic models for the
prediction of the conditional p.d.f. of the travel time, as a first step
towards reliable MDVSP solutions. Two types of probabilistic models, namely
similarity-based density estimation models and a smoothed Logistic Regression
for probabilistic classification model, are compared on a dataset of more than
41,000 trips and 50 bus routes of the city of Montr\'eal. The result of a vast
majority of probabilistic models outperforms that of a Random Forests model,
which is not inherently probabilistic, thus highlighting the added value of
modeling the conditional p.d.f. of the travel time with probabilistic models. A
similarity-based density estimation model using a $k$ Nearest Neighbors method
and a Kernel Density Estimation predicted the best estimate of the true
conditional p.d.f. on this dataset.
</p>
"
Variational Bayes survival analysis for unemployment modelling. (arXiv:2102.02295v1 [stat.AP]),http://arxiv.org/abs/2102.02295,"<p>Mathematical modelling of unemployment dynamics attempts to predict the
probability of a job seeker finding a job as a function of time. This is
typically achieved by using information in unemployment records. These records
are right censored, making survival analysis a suitable approach for parameter
estimation. The proposed model uses a deep artificial neural network (ANN) as a
non-linear hazard function. Through embedding, high-cardinality categorical
features are analysed efficiently. The posterior distribution of the ANN
parameters are estimated using a variational Bayes method. The model is
evaluated on a time-to-employment data set spanning from 2011 to 2020 provided
by the Slovenian public employment service. It is used to determine the
employment probability over time for each individual on the record. Similar
models could be applied to other questions with multi-dimensional,
high-cardinality categorical data including censored records. Such data is
often encountered in personal records, for example in medical records.
</p>
"
Parallax estimation for push-frame satellite imagery: application to super-resolution and 3D surface modeling from Skysat products. (arXiv:2102.02301v1 [cs.CV]),http://arxiv.org/abs/2102.02301,"<p>Recent constellations of satellites, including the Skysat constellation, are
able to acquire bursts of images. This new acquisition mode allows for modern
image restoration techniques, including multi-frame super-resolution. As the
satellite moves during the acquisition of the burst, elevation changes in the
scene translate into noticeable parallax. This parallax hinders the results of
the restoration. To cope with this issue, we propose a novel parallax
estimation method. The method is composed of a linear Plane+Parallax
decomposition of the apparent motion and a multi-frame optical flow algorithm
that exploits all frames simultaneously. Using SkySat L1A images, we show that
the estimated per-pixel displacements are important for applying multi-frame
super-resolution on scenes containing elevation changes and that can also be
used to estimate a coarse 3D surface model.
</p>
"
"Cleora: A Simple, Strong and Scalable Graph Embedding Scheme. (arXiv:2102.02302v1 [cs.LG])",http://arxiv.org/abs/2102.02302,"<p>The area of graph embeddings is currently dominated by contrastive learning
methods, which demand formulation of an explicit objective function and
sampling of positive and negative examples. This creates a conceptual and
computational overhead. Simple, classic unsupervised approaches like
Multidimensional Scaling (MSD) or the Laplacian eigenmap skip the necessity of
tedious objective optimization, directly exploiting data geometry.
Unfortunately, their reliance on very costly operations such as matrix
eigendecomposition make them unable to scale to large graphs that are common in
today's digital world. In this paper we present Cleora: an algorithm which gets
the best of two worlds, being both unsupervised and highly scalable. We show
that high quality embeddings can be produced without the popular step-wise
learning framework with example sampling. An intuitive learning objective of
our algorithm is that a node should be similar to its neighbors, without
explicitly pushing disconnected nodes apart. The objective is achieved by
iterative weighted averaging of node neigbors' embeddings, followed by
normalization across dimensions. Thanks to the averaging operation the
algorithm makes rapid strides across the embedding space and usually reaches
optimal embeddings in just a few iterations. Cleora runs faster than other
state-of-the-art CPU algorithms and produces embeddings of competitive quality
as measured on downstream tasks: link prediction and node classification. We
show that Cleora learns a data abstraction that is similar to contrastive
methods, yet at much lower computational cost. We open-source Cleora under the
MIT license allowing commercial use under https://github.com/Synerise/cleora.
</p>
"
Improved Cooperation by Exploiting a Common Signal. (arXiv:2102.02304v1 [cs.MA]),http://arxiv.org/abs/2102.02304,"<p>Can artificial agents benefit from human conventions? Human societies manage
to successfully self-organize and resolve the tragedy of the commons in
common-pool resources, in spite of the bleak prediction of non-cooperative game
theory. On top of that, real-world problems are inherently large-scale and of
low observability. One key concept that facilitates human coordination in such
settings is the use of conventions. Inspired by human behavior, we investigate
the learning dynamics and emergence of temporal conventions, focusing on
common-pool resources. Extra emphasis was given in designing a realistic
evaluation setting: (a) environment dynamics are modeled on real-world
fisheries, (b) we assume decentralized learning, where agents can observe only
their own history, and (c) we run large-scale simulations (up to 64 agents).
</p>
<p>Uncoupled policies and low observability make cooperation hard to achieve; as
the number of agents grow, the probability of taking a correct gradient
direction decreases exponentially. By introducing an arbitrary common signal
(e.g., date, time, or any periodic set of numbers) as a means to couple the
learning process, we show that temporal conventions can emerge and agents reach
sustainable harvesting strategies. The introduction of the signal consistently
improves the social welfare (by 258% on average, up to 3306%), the range of
environmental parameters where sustainability can be achieved (by 46% on
average, up to 300%), and the convergence speed in low abundance settings (by
13% on average, up to 53%).
</p>
"
Typing Errors in Factual Knowledge Graphs: Severity and Possible Ways Out. (arXiv:2102.02307v1 [cs.DB]),http://arxiv.org/abs/2102.02307,"<p>Factual knowledge graphs (KGs) such as DBpedia and Wikidata have served as
part of various downstream tasks and are also widely adopted by artificial
intelligence research communities as benchmark datasets. However, we found
these KGs to be surprisingly noisy. In this study, we question the quality of
these KGs, where the typing error rate is estimated to be 27% for
coarse-grained types on average, and even 73% for certain fine-grained types.
In pursuit of solutions, we propose an active typing error detection algorithm
that maximizes the utilization of both gold and noisy labels. We also
comprehensively discuss and compare unsupervised, semi-supervised, and
supervised paradigms to deal with typing errors in factual KGs. The outcomes of
this study provide guidelines for researchers to use noisy factual KGs. To help
practitioners deploy the techniques and conduct further research, we published
our code and data.
</p>
"
Fuzzing Hardware Like Software. (arXiv:2102.02308v1 [cs.AR]),http://arxiv.org/abs/2102.02308,"<p>Hardware flaws are permanent and potent: hardware cannot be patched once
fabricated, and any flaws may undermine any software executing on top.
Consequently, verification time dominates implementation time. The gold
standard in hardware Design Verification (DV) is concentrated at two extremes:
random dynamic verification and formal verification. Both struggle to root out
the subtle flaws in complex hardware that often manifest as security
vulnerabilities. The root problem with random verification is its undirected
nature, making it inefficient, while formal verification is constrained by the
state-space explosion problem, making it infeasible against complex designs.
What is needed is a solution that is directed, yet under-constrained.
</p>
<p>Instead of making incremental improvements to existing DV approaches, we
leverage the observation that existing software fuzzers already provide such a
solution, and adapt them for hardware DV. Specifically, we translate RTL
hardware to a software model and fuzz that model. The central challenge we
address is how best to mitigate the differences between the hardware execution
model and software execution model. This includes: 1) how to represent test
cases, 2) what is the hardware equivalent of a crash, 3) what is an appropriate
coverage metric, and 4) how to create a general-purpose fuzzing harness for
hardware.
</p>
<p>To evaluate our approach, we fuzz four IP blocks from Google's OpenTitan SoC.
Our experiments reveal a two orders-of-magnitude reduction in run time to
achieve Finite State Machine (FSM) coverage over traditional dynamic
verification schemes. Moreover, with our design-agnostic harness, we achieve
over 88% HDL line coverage in three out of four of our designs -- even without
any initial seeds.
</p>
"
Causal Sufficiency and Actual Causation. (arXiv:2102.02311v1 [cs.AI]),http://arxiv.org/abs/2102.02311,"<p>Pearl opened the door to formally defining actual causation using causal
models. His approach rests on two strategies: first, capturing the widespread
intuition that X=x causes Y=y iff X=x is a Necessary Element of a Sufficient
Set for Y=y, and second, showing that his definition gives intuitive answers on
a wide set of problem cases. This inspired dozens of variations of his
definition of actual causation, the most prominent of which are due to Halpern
&amp; Pearl. Yet all of them ignore Pearl's first strategy, and the second strategy
taken by itself is unable to deliver a consensus. This paper offers a way out
by going back to the first strategy: it offers six formal definitions of causal
sufficiency and two interpretations of necessity. Combining the two gives
twelve new definitions of actual causation. Several interesting results about
these definitions and their relation to the various Halpern &amp; Pearl definitions
are presented. Afterwards the second strategy is evaluated as well. In order to
maximize neutrality, the paper relies mostly on the examples and intuitions of
Halpern &amp; Pearl. One definition comes out as being superior to all others, and
is therefore suggested as a new definition of actual causation.
</p>
"
Real-Time Optimal Trajectory Planning for Autonomous Vehicles and Lap Time Simulation Using Machine Learning. (arXiv:2102.02315v1 [cs.RO]),http://arxiv.org/abs/2102.02315,"<p>The widespread development of driverless vehicles has led to the formation of
autonomous racing competitions, where the high speeds and fierce rivalry in
motorsport provide a testbed to accelerate technology development. A particular
challenge for an autonomous vehicle is that of identifying a target trajectory
- or in the case of a racing car, the ideal racing line. Many existing
approaches to identifying the racing line are either not the time-optimal
solutions, or have solution times which are computationally expensive, thus
rendering them unsuitable for real-time application using on-board processing
hardware. This paper describes a machine learning approach to generating an
accurate prediction of the racing line in real-time on desktop processing
hardware. The proposed algorithm is a dense feed-forward neural network,
trained using a dataset comprising racing lines for a large number of circuits
calculated via a traditional optimal control lap time simulation. The network
is capable of predicting the racing line with a mean absolute error of
+/-0.27m, meaning that the accuracy outperforms a human driver, and is
comparable to other parts of the autonomous vehicle control system. The system
generates predictions within 33ms, making it over 9,000 times faster than
traditional methods of finding the optimal racing line. Results suggest that a
data-driven approach may therefore be favourable for real-time generation of
near-optimal racing lines than traditional computational methods.
</p>
"
System Intelligence for UAV-Based Mission Critical with Challenging 5G/B5G Connectivity. (arXiv:2102.02318v1 [cs.NI]),http://arxiv.org/abs/2102.02318,"<p>Unmanned aerial vehicles (UAVs) and communication systems are fundamental
elements in Mission Critical services, such as search and rescue. In this
article, we introduce an architecture for managing and orchestrating 5G and
beyond networks that operate over a heterogeneous infrastructure with UAVs'
aid. UAVs are used for collecting and processing data, as well as improving
communications. The proposed System Intelligence (SI) architecture was designed
to comply with recent standardization works, especially the ETSI Experiential
Networked Intelligence specifications. Another contribution of this article is
an evaluation using a testbed based on a virtualized non-standalone 5G core and
a 4G Radio Access Network (RAN) implemented with open-source software. The
experimental results indicate, for instance, that SI can substantially improve
the latency of UAV-based services by splitting deep neural networks between UAV
and edge or cloud equipment. Other experiments explore the slicing of RAN
resources and efficient placement of virtual network functions to assess the
benefits of incorporating intelligence in UAV-based mission-critical services.
</p>
"
"One Label, One Billion Faces: Usage and Consistency of Racial Categories in Computer Vision. (arXiv:2102.02320v1 [cs.CV])",http://arxiv.org/abs/2102.02320,"<p>Computer vision is widely deployed, has highly visible, society altering
applications, and documented problems with bias and representation. Datasets
are critical for benchmarking progress in fair computer vision, and often
employ broad racial categories as population groups for measuring group
fairness. Similarly, diversity is often measured in computer vision datasets by
ascribing and counting categorical race labels. However, racial categories are
ill-defined, unstable temporally and geographically, and have a problematic
history of scientific use. Although the racial categories used across datasets
are superficially similar, the complexity of human race perception suggests the
racial system encoded by one dataset may be substantially inconsistent with
another. Using the insight that a classifier can learn the racial system
encoded by a dataset, we conduct an empirical study of computer vision datasets
supplying categorical race labels for face images to determine the
cross-dataset consistency and generalization of racial categories. We find that
each dataset encodes a substantially unique racial system, despite nominally
equivalent racial categories, and some racial categories are systemically less
consistent than others across datasets. We find evidence that racial categories
encode stereotypes, and exclude ethnic groups from categories on the basis of
nonconformity to stereotypes. Representing a billion humans under one racial
category may obscure disparities and create new ones by encoding stereotypes of
racial systems. The difficulty of adequately converting the abstract concept of
race into a tool for measuring fairness underscores the need for a method more
flexible and culturally aware than racial categories.
</p>
"
Query Complexity of Least Absolute Deviation Regression via Robust Uniform Convergence. (arXiv:2102.02322v1 [cs.LG]),http://arxiv.org/abs/2102.02322,"<p>Consider a regression problem where the learner is given a large collection
of $d$-dimensional data points, but can only query a small subset of the
real-valued labels. How many queries are needed to obtain a $1+\epsilon$
relative error approximation of the optimum? While this problem has been
extensively studied for least squares regression, little is known for other
losses. An important example is least absolute deviation regression ($\ell_1$
regression) which enjoys superior robustness to outliers compared to least
squares. We develop a new framework for analyzing importance sampling methods
in regression problems, which enables us to show that the query complexity of
least absolute deviation regression is $\Theta(d/\epsilon^2)$ up to logarithmic
factors. We further extend our techniques to show the first bounds on the query
complexity for any $\ell_p$ loss with $p\in(1,2)$. As a key novelty in our
analysis, we introduce the notion of robust uniform convergence, which is a new
approximation guarantee for the empirical loss. While it is inspired by uniform
convergence in statistical learning, our approach additionally incorporates a
correction term to avoid unnecessary variance due to outliers. This can be
viewed as a new connection between statistical learning theory and variance
reduction techniques in stochastic optimization, which should be of independent
interest.
</p>
"
Effects of Number of Filters of Convolutional Layers on Speech Recognition Model Accuracy. (arXiv:2102.02326v1 [cs.LG]),http://arxiv.org/abs/2102.02326,"<p>Inspired by the progress of the End-to-End approach [1], this paper
systematically studies the effects of Number of Filters of convolutional layers
on the model prediction accuracy of CNN+RNN (Convolutional Neural Networks
adding to Recurrent Neural Networks) for ASR Models (Automatic Speech
Recognition). Experimental results show that only when the CNN Number of
Filters exceeds a certain threshold value is adding CNN to RNN able to improve
the performance of the CNN+RNN speech recognition model, otherwise some
parameter ranges of CNN can render it useless to add the CNN to the RNN model.
Our results show a strong dependency of word accuracy on the Number of Filters
of convolutional layers. Based on the experimental results, the paper suggests
a possible hypothesis of Sound-2-Vector Embedding (Convolutional Embedding) to
explain the above observations.
</p>
<p>Based on this Embedding hypothesis and the optimization of parameters, the
paper develops an End-to-End speech recognition system which has a high word
accuracy but also has a light model-weight. The developed LVCSR (Large
Vocabulary Continuous Speech Recognition) model has achieved quite a high word
accuracy of 90.2% only by its Acoustic Model alone, without any assistance from
intermediate phonetic representation and any Language Model. Its acoustic model
contains only 4.4 million weight parameters, compared to the 35~68 million
acoustic-model weight parameters in DeepSpeech2 [2] (one of the top
state-of-the-art LVCSR models) which can achieve a word accuracy of 91.5%. The
light-weighted model is good for improving the transcribing computing
efficiency and also useful for mobile devices, Driverless Vehicles, etc. Our
model weight is reduced to ~10% the size of DeepSpeech2, but our model accuracy
remains close to that of DeepSpeech2. If combined with a Language Model, our
LVCSR system is able to achieve 91.5% word accuracy.
</p>
"
Modeling Complex Financial Products. (arXiv:2102.02329v1 [cs.LG]),http://arxiv.org/abs/2102.02329,"<p>The objective of this paper is to explore how financial big data and machine
learning methods can be applied to model and understand complex financial
products. We focus on residential mortgage backed securities, resMBS, that were
at the heart of the 2008 US financial crisis. The securities are contained
within a prospectus and have a complex payoff structure. Multiple financial
institutions form a supply chain to create the prospectuses. We provide insight
into the performance of the resMBS securities through a series of increasingly
complex models. First, models at the security level directly identify salient
features of resMBS securities that impact their performance. Second, we extend
the model to include prospectus level features. We are the first to demonstrate
that the composition of the prospectus is associated with the performance of
securities. Finally, to develop a deeper understanding of the role of the
supply chain, we use unsupervised probabilistic methods, in particular, dynamic
topics models (DTM), to understand community formation and temporal evolution
along the chain. A comprehensive model provides insight into the impact of DTM
communities on the issuance and evolution of prospectuses, and eventually the
performance of resMBS securities.
</p>
"
Function Delivery Network: Extending Serverless Computing for Heterogeneous Platforms. (arXiv:2102.02330v1 [cs.DC]),http://arxiv.org/abs/2102.02330,"<p>Serverless computing has rapidly grown following the launch of Amazon's
Lambda platform. Function-as-a-Service (FaaS) a key enabler of serverless
computing allows an application to be decomposed into simple, standalone
functions that are executed on a FaaS platform. The FaaS platform is
responsible for deploying and facilitating resources to the functions. Several
of today's cloud applications spread over heterogeneous connected computing
resources and are highly dynamic in their structure and resource requirements.
However, FaaS platforms are limited to homogeneous clusters and homogeneous
functions and do not account for the data access behavior of functions before
scheduling.
</p>
<p>We introduce an extension of FaaS to heterogeneous clusters and to support
heterogeneous functions through a network of distributed heterogeneous target
platforms called Function Delivery Network (FDN). A target platform is a
combination of a cluster of homogeneous nodes and a FaaS platform on top of it.
FDN provides Function-Delivery-as-a-Service (FDaaS), delivering the function to
the right target platform. We showcase the opportunities such as varied target
platform's characteristics, possibility of collaborative execution between
multiple target platforms, and localization of data that the FDN offers in
fulfilling two objectives: Service Level Objective (SLO) requirements and
energy efficiency when scheduling functions by evaluating over five distributed
target platforms using the FDNInspector, a tool developed by us for
benchmarking distributed target platforms. Scheduling functions on an edge
target platform in our evaluation reduced the overall energy consumption by 17x
without violating the SLO requirements in comparison to scheduling on a
high-end target platform.
</p>
"
The Enigma of Complexity. (arXiv:2102.02332v1 [cs.NE]),http://arxiv.org/abs/2102.02332,"<p>In this paper we examine the concept of complexity as it applies to
generative art and design. Complexity has many different, discipline specific
definitions, such as complexity in physical systems (entropy), algorithmic
measures of information complexity and the field of ""complex systems"". We apply
a series of different complexity measures to three different generative art
datasets and look at the correlations between complexity and individual
aesthetic judgement by the artist (in the case of two datasets) or the
physically measured complexity of 3D forms. Our results show that the degree of
correlation is different for each set and measure, indicating that there is no
overall ""better"" measure. However, specific measures do perform well on
individual datasets, indicating that careful choice can increase the value of
using such measures. We conclude by discussing the value of direct measures in
generative and evolutionary art, reinforcing recent findings from neuroimaging
and psychology which suggest human aesthetic judgement is informed by many
extrinsic factors beyond the measurable properties of the object being judged.
</p>
"
Self-Supervised Claim Identification for Automated Fact Checking. (arXiv:2102.02335v1 [cs.CL]),http://arxiv.org/abs/2102.02335,"<p>We propose a novel, attention-based self-supervised approach to identify
""claim-worthy"" sentences in a fake news article, an important first step in
automated fact-checking. We leverage ""aboutness"" of headline and content using
attention mechanism for this task. The identified claims can be used for
downstream task of claim verification for which we are releasing a benchmark
dataset of manually selected compelling articles with veracity labels and
associated evidence. This work goes beyond stylistic analysis to identifying
content that influences reader belief. Experiments with three datasets show the
strength of our model. Data and code available at
https://github.com/architapathak/Self-Supervised-ClaimIdentification
</p>
"
On the Approximation Power of Two-Layer Networks of Random ReLUs. (arXiv:2102.02336v1 [cs.LG]),http://arxiv.org/abs/2102.02336,"<p>This paper considers the following question: how well can depth-two ReLU
networks with randomly initialized bottom-level weights represent smooth
functions? We give near-matching upper- and lower-bounds for
$L_2$-approximation in terms of the Lipschitz constant, the desired accuracy,
and the dimension of the problem, as well as similar results in terms of
Sobolev norms. Our positive results employ tools from harmonic analysis and
ridgelet representation theory, while our lower-bounds are based on (robust
versions of) dimensionality arguments.
</p>
"
Environment Predictive Coding for Embodied Agents. (arXiv:2102.02337v1 [cs.CV]),http://arxiv.org/abs/2102.02337,"<p>We introduce environment predictive coding, a self-supervised approach to
learn environment-level representations for embodied agents. In contrast to
prior work on self-supervised learning for images, we aim to jointly encode a
series of images gathered by an agent as it moves about in 3D environments. We
learn these representations via a zone prediction task, where we intelligently
mask out portions of an agent's trajectory and predict them from the unmasked
portions, conditioned on the agent's camera poses. By learning such
representations on a collection of videos, we demonstrate successful transfer
to multiple downstream navigation-oriented tasks. Our experiments on the
photorealistic 3D environments of Gibson and Matterport3D show that our method
outperforms the state-of-the-art on challenging tasks with only a limited
budget of experience.
</p>
"
Microscopic Patterns in the 2D Phase-Field-Crystal Model. (arXiv:2102.02338v1 [math.NA]),http://arxiv.org/abs/2102.02338,"<p>Using the recently developed theory of rigorously validated numerics, we
address the Phase-Field-Crystal (PFC) model at the microscopic (atomistic)
level. We show the existence of critical points and local minimizers associated
with ""classical"" candidates, grain boundaries, and localized patterns. We
further address the dynamical relationships between the observed patterns for
fixed parameters and across parameter space, then formulate several conjectures
on the dynamical connections (or orbits) between steady states.
</p>
"
MUFASA: Multimodal Fusion Architecture Search for Electronic Health Records. (arXiv:2102.02340v1 [cs.LG]),http://arxiv.org/abs/2102.02340,"<p>One important challenge of applying deep learning to electronic health
records (EHR) is the complexity of their multimodal structure. EHR usually
contains a mixture of structured (codes) and unstructured (free-text) data with
sparse and irregular longitudinal features -- all of which doctors utilize when
making decisions. In the deep learning regime, determining how different
modality representations should be fused together is a difficult problem, which
is often addressed by handcrafted modeling and intuition. In this work, we
extend state-of-the-art neural architecture search (NAS) methods and propose
MUltimodal Fusion Architecture SeArch (MUFASA) to simultaneously search across
multimodal fusion strategies and modality-specific architectures for the first
time. We demonstrate empirically that our MUFASA method outperforms established
unimodal NAS on public EHR data with comparable computation costs. In addition,
MUFASA produces architectures that outperform Transformer and Evolved
Transformer. Compared with these baselines on CCS diagnosis code prediction,
our discovered models improve top-5 recall from 0.88 to 0.91 and demonstrate
the ability to generalize to other EHR tasks. Studying our top architecture in
depth, we provide empirical evidence that MUFASA's improvements are derived
from its ability to both customize modeling for each data modality and find
effective fusion strategies.
</p>
"
Horizontally Fused Training Array: An Effective Hardware Utilization Squeezer for Training Novel Deep Learning Models. (arXiv:2102.02344v1 [cs.LG]),http://arxiv.org/abs/2102.02344,"<p>Driven by the tremendous effort in researching novel deep learning (DL)
algorithms, the training cost of developing new models increases staggeringly
in recent years. To reduce this training cost and optimize the cluster-wide
hardware resource usage, we analyze GPU cluster usage statistics from a
well-known research institute. Our study reveals that single-accelerator
training jobs can dominate the cluster-wide resource consumption when launched
repetitively (e.g., for hyper-parameter tuning) while severely underutilizing
the hardware. This is because DL researchers and practitioners often lack the
required expertise to independently optimize their own workloads. Fortunately,
we observe that such workloads have the following unique characteristics: (i)
the models among jobs often have the same types of operators with the same
shapes, and (ii) the inter-model horizontal fusion of such operators is
mathematically equivalent to other already well-optimized operators. Thus, to
help DL researchers and practitioners effectively and easily improve the
hardware utilization of their novel DL training workloads, we propose
Horizontally Fused Training Array (HFTA). HFTA is a new DL framework extension
library that horizontally fuses the models from different repetitive jobs
deeply down to operators, and then trains those models simultaneously on a
shared accelerator. On three emerging DL training workloads and
state-of-the-art accelerators (GPUs and TPUs), HFTA demonstrates strong
effectiveness in squeezing out hardware utilization and achieves up to $15.1
\times$ higher training throughput vs. the standard practice of running each
job on a separate accelerator.
</p>
"
"On Multi-Human Multi-Robot Remote Interaction: A Study of Transparency, Inter-Human Communication, and Information Loss in Remote Interaction. (arXiv:2102.02351v1 [cs.RO])",http://arxiv.org/abs/2102.02351,"<p>In this paper, we investigate how to design an effective interface for remote
multi-human multi-robot interaction. While significant research exists on
interfaces for individual human operators, little research exists for the
multi-human case. Yet, this is a critical problem to solve to make complex,
large-scale missions achievable in which direct human involvement is impossible
or undesirable, and robot swarms act as a semi-autonomous agents. This paper's
contribution is twofold. The first contribution is an exploration of the design
space of computer-based interfaces for multi-human multi-robot operations. In
particular, we focus on information transparency and on the factors that affect
inter-human communication in ideal conditions, i.e., without communication
issues. Our second contribution concerns the same problem, but considering
increasing degrees of information loss, defined as intermittent reception of
data with noticeable gaps between individual receipts. We derived a set of
design recommendations based on two user studies involving 48 participants.
</p>
"
Entropic Dynamics of Networks. (arXiv:2102.02355v1 [physics.soc-ph]),http://arxiv.org/abs/2102.02355,"<p>Here we present the entropic dynamics formalism for networks. That is, a
framework for the dynamics of graphs meant to represent a network derived from
the principle of maximum entropy and the rate of transition is obtained taking
into account the natural information geometry of probability distributions. We
apply this framework to the Gibbs distribution of random graphs obtained with
constraints on the node connectivity. The information geometry for this graph
ensemble is calculated and the dynamical process is obtained as a diffusion
equation. We compare the steady state of this dynamics to degree distributions
found on real-world networks.
</p>
"
A Practical Coding Scheme for the BSC with Feedback. (arXiv:2102.02358v1 [cs.IT]),http://arxiv.org/abs/2102.02358,"<p>We provide a practical implementation of the rubber method of Ahlswede et al.
for binary alphabets. The idea is to create the ""skeleton"" sequence therein via
an arithmetic decoder designed for a particular $k$-th order Markov chain. For
the stochastic binary symmetric channel, we show that the scheme is nearly
optimal in a strong sense for certain parameters.
</p>
"
Operational Semantics with Hierarchical Abstract Syntax Graphs. (arXiv:2102.02363v1 [cs.PL]),http://arxiv.org/abs/2102.02363,"<p>This is a motivating tutorial introduction to a semantic analysis of
programming languages using a graphical language as the representation of
terms, and graph rewriting as a representation of reduction rules. We show how
the graphical language automatically incorporates desirable features, such as
alpha-equivalence and how it can describe pure computation, imperative store,
and control features in a uniform framework. The graph semantics combines some
of the best features of structural operational semantics and abstract machines,
while offering powerful new methods for reasoning about contextual equivalence.
</p>
<p>All technical details are available in an extended technical report by Muroya
and the author and in Muroya's doctoral dissertation.
</p>
"
On Stochastic Rewriting and Combinatorics via Rule-Algebraic Methods. (arXiv:2102.02364v1 [cs.LO]),http://arxiv.org/abs/2102.02364,"<p>Building upon the rule-algebraic stochastic mechanics framework, we present
new results on the relationship of stochastic rewriting systems described in
terms of continuous-time Markov chains, their embedded discrete-time Markov
chains and certain types of generating function expressions in combinatorics.
We introduce a number of generating function techniques that permit a novel
form of static analysis for rewriting systems based upon marginalizing
distributions over the states of the rewriting systems via pattern-counting
observables.
</p>
"
Wind Field Reconstruction with Adaptive Random Fourier Features. (arXiv:2102.02365v1 [math.NA]),http://arxiv.org/abs/2102.02365,"<p>We investigate the use of spatial interpolation methods for reconstructing
the horizontal near-surface wind field given a sparse set of measurements. In
particular, random Fourier features is compared to a set of benchmark methods
including Kriging and Inverse distance weighting. Random Fourier features is a
linear model $\beta(\pmb x) = \sum_{k=1}^K \beta_k e^{i\omega_k \pmb x}$
approximating the velocity field, with frequencies $\omega_k$ randomly sampled
and amplitudes $\beta_k$ trained to minimize a loss function. We include a
physically motivated divergence penalty term $|\nabla \cdot \beta(\pmb x)|^2$,
as well as a penalty on the Sobolev norm. We derive a bound on the
generalization error and derive a sampling density that minimizes the bound.
Following (<a href=""/abs/2007.10683"">arXiv:2007.10683</a> [math.NA]), we devise an adaptive
Metropolis-Hastings algorithm for sampling the frequencies of the optimal
distribution. In our experiments, our random Fourier features model outperforms
the benchmark models.
</p>
"
Parallel Independence in Attributed Graph Rewriting. (arXiv:2102.02366v1 [cs.LO]),http://arxiv.org/abs/2102.02366,"<p>In order to define graph transformations by the simultaneous application of
concurrent rules, we have adopted in previous work a structure of attributed
graphs stable by unions. We analyze the consequences on parallel independence,
a property that characterizes the possibility to resort to sequential
rewriting. This property turns out to depend not only on the left-hand side of
rules, as in algebraic approaches to graph rewriting, but also on their
right-hand side. It is then shown that, of three possible definitions of
parallel rewriting, only one is convenient in the light of parallel
independence.
</p>
"
Finite Difference Weerakoon-Fernando Method to solve nonlinear equations without using derivatives. (arXiv:2102.02367v1 [math.NA]),http://arxiv.org/abs/2102.02367,"<p>This research was mainly conducted to explore the possibility of formulating
an efficient algorithm to find roots of nonlinear equations without using the
derivative of the function. The Weerakoon-Fernando method had been taken as the
base in this project to find a new method without the derivative since
Weerakoon-Fernando method gives 3rd order convergence. After several
unsuccessful attempts we were able to formulate the Finite Difference
Weerakoon-Fernando Method (FDWFM) presented here. We noticed that the FDWFM
approaches the root faster than any other existing method in the absence of the
derivatives as an example, the popular nonlinear equation solver such as secant
method (order of convergence is 1.618) in the absence of the derivative. And
the FDWFM had three function evaluations and secant method had two function
evaluations. By implementing FDWFM on nonlinear equations with complex roots
and also on systems of nonlinear equations, we received very encouraging
results. When applying the FDWFM to systems of nonlinear equations, we resolved
the involvement of the Jacobian problem by following the procedure in the
Broyden's method. The computational order of convergence of the FDWFM was close
to 2.5 for all these cases. This will undoubtedly provide scientists the
efficient numerical algorithm, that doesn't need the derivative of the function
to solve nonlinear equations, that they were searching for over centuries.
</p>
"
Verifying Security Vulnerabilities in Large Software Systems using Multi-Core k-Induction. (arXiv:2102.02368v1 [cs.CR]),http://arxiv.org/abs/2102.02368,"<p>Computer-based systems have been used to solve several domain problems, such
as industrial, military, education, and wearable. Those systems need
high-quality software to guarantee security and safety. We advocate that
Bounded Model Checking (BMC) techniques can detect security vulnerabilities in
the early stages of development processes. However, this technique struggles to
scale up and verify large software commonly found on computer-based systems.
Here, we develop and evaluate a pragmatic approach to verify large software
systems using a state-of-the-art bounded model checker. In particular, we
pre-process the input source-code files and then guide the model checker to
explore the code systematically. We also present a multi-core implementation of
the k-induction proof algorithm to verify and falsify large software systems
iteratively. Our experimental results using the Efficient SMT-based Model
Checker (ESBMC) show that our approach can guide ESBMC to efficiently verify
large software systems. We evaluate our approach using the PuTTY application to
verify 136 files and 2803 functions in less than 86 minutes, and the SlimGuard
allocator, where we have found real security vulnerabilities confirmed by the
developers. We conclude that our approach can successfully guide a bounded
model checker to verify large software systems systematically.
</p>
"
MeInGame: Create a Game Character Face from a Single Portrait. (arXiv:2102.02371v1 [cs.CV]),http://arxiv.org/abs/2102.02371,"<p>Many deep learning based 3D face reconstruction methods have been proposed
recently, however, few of them have applications in games. Current game
character customization systems either require players to manually adjust
considerable face attributes to obtain the desired face, or have limited
freedom of facial shape and texture. In this paper, we propose an automatic
character face creation method that predicts both facial shape and texture from
a single portrait, and it can be integrated into most existing 3D games.
Although 3D Morphable Face Model (3DMM) based methods can restore accurate 3D
faces from single images, the topology of 3DMM mesh is different from the
meshes used in most games. To acquire fidelity texture, existing methods
require a large amount of face texture data for training, while building such
datasets is time-consuming and laborious. Besides, such a dataset collected
under laboratory conditions may not generalized well to in-the-wild situations.
To tackle these problems, we propose 1) a low-cost facial texture acquisition
method, 2) a shape transfer algorithm that can transform the shape of a 3DMM
mesh to games, and 3) a new pipeline for training 3D game face reconstruction
networks. The proposed method not only can produce detailed and vivid game
characters similar to the input portrait, but can also eliminate the influence
of lighting and occlusions. Experiments show that our method outperforms
state-of-the-art methods used in games.
</p>
"
Coevolution of theoretical and applied research: a case study of graphene research by temporal and geographic analysis. (arXiv:2102.02372v1 [cs.DL]),http://arxiv.org/abs/2102.02372,"<p>As a part of science of science (SciSci) research, the evolution of
scientific disciplines has been attracting a great deal of attention recently.
This kind of discipline level analysis not only give insights of one particular
field but also shed light on general principles of scientific enterprise. In
this paper we focus on graphene research, a fast growing field covers both
theoretical and applied study. Using co-clustering method, we split graphene
literature into two groups and confirm that one group is about theoretical
research (T) and another corresponds to applied research (A). We analyze the
proportion of T/A and found applied research becomes more and more popular
after 2007. Geographical analysis demonstrated that countries have different
preference in terms of T/A and they reacted differently to research trend. The
interaction between two groups has been analyzed and shows that T extremely
relies on T and A heavily relies on A, however the situation is very stable for
T but changed markedly for A. No geographic difference is found for the
interaction dynamics. Our results give a comprehensive picture of graphene
research evolution and also provide a general framework which is able to
analyze other disciplines.
</p>
"
Sampling in Combinatorial Spaces with SurVAE Flow Augmented MCMC. (arXiv:2102.02374v1 [cs.LG]),http://arxiv.org/abs/2102.02374,"<p>Hybrid Monte Carlo is a powerful Markov Chain Monte Carlo method for sampling
from complex continuous distributions. However, a major limitation of HMC is
its inability to be applied to discrete domains due to the lack of gradient
signal. In this work, we introduce a new approach based on augmenting Monte
Carlo methods with SurVAE Flows to sample from discrete distributions using a
combination of neural transport methods like normalizing flows and variational
dequantization, and the Metropolis-Hastings rule. Our method first learns a
continuous embedding of the discrete space using a surjective map and
subsequently learns a bijective transformation from the continuous space to an
approximately Gaussian distributed latent variable. Sampling proceeds by
simulating MCMC chains in the latent space and mapping these samples to the
target discrete space via the learned transformations. We demonstrate the
efficacy of our algorithm on a range of examples from statistics, computational
physics and machine learning, and observe improvements compared to alternative
algorithms.
</p>
"
A survey of motion planning algorithms for intelligent robotics. (arXiv:2102.02376v1 [cs.RO]),http://arxiv.org/abs/2102.02376,"<p>We investigate and analyze principles of typical motion planning algorithms.
These include traditional planning algorithms, supervised learning, optimal
value reinforcement learning, policy gradient reinforcement learning.
Traditional planning algorithms we investigated include graph search
algorithms, sampling-based algorithms, and interpolating curve algorithms.
Supervised learning algorithms include MSVM, LSTM, MCTS and CNN. Optimal value
reinforcement learning algorithms include Q learning, DQN, double DQN, dueling
DQN. Policy gradient algorithms include policy gradient method, actor-critic
algorithm, A3C, A2C, DPG, DDPG, TRPO and PPO. New general criteria are also
introduced to evaluate performance and application of motion planning
algorithms by analytical comparisons. Convergence speed and stability of
optimal value and policy gradient algorithms are specially analyzed. Future
directions are presented analytically according to principles and analytical
comparisons of motion planning algorithms. This paper provides researchers with
a clear and comprehensive understanding about advantages, disadvantages,
relationships, and future of motion planning algorithms in robotics, and paves
ways for better motion planning algorithms.
</p>
"
Mainstreaming of conspiracy theories and misinformation. (arXiv:2102.02382v1 [physics.soc-ph]),http://arxiv.org/abs/2102.02382,"<p>Parents - particularly moms - increasingly consult social media for support
when taking decisions about their young children, and likely also when advising
other family members such as elderly relatives. Minimizing malignant online
influences is therefore crucial to securing their assent for policies ranging
from vaccinations, masks and social distancing against the pandemic, to
household best practices against climate change, to acceptance of future 5G
towers nearby. Here we show how a strengthening of bonds across online
communities during the pandemic, has led to non-Covid-19 conspiracy theories
(e.g. fluoride, chemtrails, 5G) attaining heightened access to mainstream
parent communities. Alternative health communities act as the critical conduits
between conspiracy theorists and parents, and make the narratives more
palatable to the latter. We demonstrate experimentally that these
inter-community bonds can perpetually generate new misinformation, irrespective
of any changes in factual information. Our findings show explicitly why
Facebook's current policies have failed to stop the mainstreaming of
non-Covid-19 and Covid-19 conspiracy theories and misinformation, and why
targeting the largest communities will not work. A simple yet exactly solvable
and empirically grounded mathematical model, shows how modest tailoring of
mainstream communities' couplings could prevent them from tipping against
establishment guidance. Our conclusions should also apply to other social media
platforms and topics.
</p>
"
A Possible Artificial Intelligence Ecosystem Avatar: the Moorea case (IDEA). (arXiv:2102.02384v1 [cs.AI]),http://arxiv.org/abs/2102.02384,"<p>High-throughput data collection techniques and largescale (cloud) computing
are transforming our understanding of ecosystems at all scales by allowing the
integration of multimodal data such as physics, chemistry, biology, ecology,
fishing, economics and other social sciences in a common computational
framework. We focus in this paper on a large scale data assimilation and
prediction backbone based on Deep Stacking Networks (DSN) in the frame of the
IDEA (Island Digital Ecosystem Avatars) project (Moorea Island), based on the
subdivision of the island in watersheds and lagoon units. We also describe
several kinds of raw data that can train and constrain such an ecosystem avatar
model, as well as second level data such as ecological or physical indexes /
indicators.
</p>
"
First- and Second-Moment Constrained Gaussian Channels. (arXiv:2102.02388v1 [cs.IT]),http://arxiv.org/abs/2102.02388,"<p>This paper studies the channel capacity of intensity-modulation
direct-detection (IM/DD) visible light communication (VLC) systems under both
optical and electrical power constraints. Specifically, it derives the
asymptotic capacities in the high and low signal-to-noise ratio (SNR) regimes
under peak, first-moment, and second-moment constraints. The results show that
first- and second-moment constraints are never simultaneously active in the
asymptotic low-SNR regime, and only in few cases in the asymptotic high-SNR
regime. Moreover, the second-moment constraint is more stringent in the
asymptotic low-SNR regime than in the high-SNR regime.
</p>
"
A Universal Framework for Featurization of Atomistic Systems. (arXiv:2102.02390v1 [physics.chem-ph]),http://arxiv.org/abs/2102.02390,"<p>Molecular dynamics simulations are an invaluable tool in numerous scientific
fields. However, the ubiquitous classical force fields cannot describe reactive
systems, and quantum molecular dynamics are too computationally demanding to
treat large systems or long timescales. Reactive force fields based on physics
or machine learning can be used to bridge the gap in time and length scales,
but these force fields require substantial effort to construct and are highly
specific to given chemical composition and application. The extreme flexibility
of machine learning models promises to yield reactive force fields that provide
a more general description of chemical bonding. However, a significant
limitation of machine learning models is the use of element-specific features,
leading to models that scale poorly with the number of elements. This work
introduces the Gaussian multi-pole (GMP) featurization scheme that utilizes
physically-relevant multi-pole expansions of the electron density around atoms
to yield feature vectors that interpolate between element types and have a
fixed dimension regardless of the number of elements present. We combine GMP
with neural networks to directly compare it to the widely-used Behler-Parinello
symmetry functions for the MD17 dataset, revealing that it exhibits improved
accuracy and computational efficiency. Further, we demonstrate that GMP-based
models can achieve chemical accuracy for the QM9 dataset, and their accuracy
remains reasonable even when extrapolating to new elements. Finally, we test
GMP-based models for the Open Catalysis Project (OCP) dataset, revealing
comparable performance and improved learning rates when compared to graph
convolutional deep learning models. The results indicate that this
featurization scheme fills a critical gap in the construction of efficient and
transferable reactive force fields.
</p>
"
Refined Grey-Box Fuzzing with SIVO. (arXiv:2102.02394v1 [cs.CR]),http://arxiv.org/abs/2102.02394,"<p>We design and implement from scratch a new fuzzer called SIVO that refines
multiple stages of grey-box fuzzing. First, SIVO refines data-flow fuzzing in
two ways: (a) it provides a new taint inference engine that requires only
logarithmic in the input size number of tests to infer the dependency of all
program branches on the input bytes, and (b) it deploys a novel method for
inverting branches by solving directly and efficiently systems of inequalities.
Second, our fuzzer refines accurate tracking and detection of code coverage
with simple and easily implementable methods. Finally, SIVO refines selection
of parameters and strategies by parameterizing all stages of fuzzing and then
dynamically selecting optimal values during fuzzing. Thus the fuzzer can easily
adapt to a target program and rapidly increase coverage. We compare our fuzzer
to 11 other state-of-the-art grey-box fuzzers on 27 popular benchmarks. Our
evaluation shows that SIVO scores the highest both in terms of code coverage
and in terms of number of found vulnerabilities.
</p>
"
Event Occurrence Model for Power Distribution System Based on Data Eigenvalue Behavior Analytic. (arXiv:2102.02395v1 [eess.SY]),http://arxiv.org/abs/2102.02395,"<p>This paper describes a model for power distribution network, which is hten
utilized for eigenvalue behavior distribution analysis. We utilized the
compensation theory to model the event that occurs in the network and derived
the relation between the system nodal voltage eigenvalues and the occurrence of
events. The paper combines the power system model with the eigenvalue
distribution analysis.
</p>
"
Provably End-to-end Label-Noise Learning without Anchor Points. (arXiv:2102.02400v1 [cs.LG]),http://arxiv.org/abs/2102.02400,"<p>In label-noise learning, the transition matrix plays a key role in building
statistically consistent classifiers. Existing consistent estimators for the
transition matrix have been developed by exploiting anchor points. However, the
anchor-point assumption is not always satisfied in real scenarios. In this
paper, we propose an end-to-end framework for solving label-noise learning
without anchor points, in which we simultaneously minimize two objectives: the
discrepancy between the distribution learned by the neural network and the
noisy class-posterior distribution, and the volume of the simplex formed by the
columns of the transition matrix. Our proposed framework can identify the
transition matrix if the clean class-posterior probabilities are sufficiently
scattered. This is by far the mildest assumption under which the transition
matrix is provably identifiable and the learned classifier is statistically
consistent. Experimental results on benchmark datasets demonstrate the
effectiveness and robustness of the proposed method.
</p>
"
SAFELearning: Enable Backdoor Detectability In Federated Learning With Secure Aggregation. (arXiv:2102.02402v1 [cs.CR]),http://arxiv.org/abs/2102.02402,"<p>For model privacy, local model parameters in federated learning shall be
obfuscated before sent to the remote aggregator. This technique is referred to
as \emph{secure aggregation}. However, secure aggregation makes model poisoning
attacks, e.g., to insert backdoors, more convenient given existing anomaly
detection methods mostly require access to plaintext local models. This paper
proposes SAFELearning which supports backdoor detection for secure aggregation.
We achieve this through two new primitives - \emph{oblivious random grouping
(ORG)} and \emph{partial parameter disclosure (PPD)}. ORG partitions
participants into one-time random subgroups with group configurations oblivious
to participants; PPD allows secure partial disclosure of aggregated subgroup
models for anomaly detection without leaking individual model privacy.
SAFELearning is able to significantly reduce backdoor model accuracy without
jeopardizing the main task accuracy under common backdoor strategies. Extensive
experiments show SAFELearning reduces backdoor accuracy from $100\%$ to $8.2\%$
for ResNet-18 over CIFAR-10 when $10\%$ participants are malicious.
</p>
"
Optimal Co-Designs of Communication and Control in Bandwidth-Constrained Cyber-Physical Systems. (arXiv:2102.02403v1 [math.OC]),http://arxiv.org/abs/2102.02403,"<p>We address the problem of sparsity-promoting optimal control of
cyber-physical systems (CPSs) in the presence of communication delays. The
delays are categorized into two types - namely, an inter-layer delay for
passing state and control information between the physical layer and the cyber
layer, and an intra-layer delay that operates between the computing agents,
referred to here as control nodes (CNs), within the cyber-layer. Our objective
is to minimize the closed-loop H2-norm of the physical system by co-designing
an optimal combination of these two delays and a sparse state-feedback
controller while respecting a given bandwidth cost constraint. We propose a
two-loop optimization algorithm for this. Based on the alternating directions
method of multipliers (ADMM), the inner loop handles the conflicting directions
between the decreasing H2-norm and the increasing sparsity level of the
controller. The outer loop comprises a semidefinite program (SDP)-based
relaxation of non-convex inequalities necessary for closed-loop stability.
Moreover, for CPSs where the state and control information assigned to the CNs
are not private, we derive an additional algorithm that further sparsifies the
communication topology by modifying the row and column structures of the
obtained controller, resulting in reassigning the communication map between the
cyber and physical layers, and determining which physical agent should send its
state information to which CN. Proofs for closed-loop stability and optimality
are provided for both algorithms, followed by numerical simulations.
</p>
"
An efficient optimization based microstructure reconstruction approach with multiple loss functions. (arXiv:2102.02407v1 [cond-mat.mtrl-sci]),http://arxiv.org/abs/2102.02407,"<p>Stochastic microstructure reconstruction involves digital generation of
microstructures that match key statistics and characteristics of a (set of)
target microstructure(s). This process enables computational analyses on
ensembles of microstructures without having to perform exhaustive and costly
experimental characterizations. Statistical functions-based and deep
learning-based methods are among the stochastic microstructure reconstruction
approaches applicable to a wide range of material systems. In this paper, we
integrate statistical descriptors as well as feature maps from a pre-trained
deep neural network into an overall loss function for an optimization based
reconstruction procedure. This helps us to achieve significant computational
efficiency in reconstructing microstructures that retain the critically
important physical properties of the target microstructure. A numerical example
for the microstructure reconstruction of bi-phase random porous ceramic
material demonstrates the efficiency of the proposed methodology. We further
perform a detailed finite element analysis (FEA) of the reconstructed
microstructures to calculate effective elastic modulus, effective thermal
conductivity and effective hydraulic conductivity, in order to analyse the
algorithm's capacity to capture the variability of these material properties
with respect to those of the target microstructure. This method provides an
economic, efficient and easy-to-use approach for reconstructing random
multiphase materials in 2D which has the potential to be extended to 3D
structures.
</p>
"
Variational Inference for Deblending Crowded Starfields. (arXiv:2102.02409v1 [astro-ph.IM]),http://arxiv.org/abs/2102.02409,"<p>In the image data collected by astronomical surveys, stars and galaxies often
overlap. Deblending is the task of distinguishing and characterizing individual
light sources from survey images. We propose StarNet, a fully Bayesian method
to deblend sources in astronomical images of crowded star fields. StarNet
leverages recent advances in variational inference, including amortized
variational distributions and the wake-sleep algorithm. Wake-sleep, which
minimizes forward KL divergence, has significant benefits compared to
traditional variational inference, which minimizes a reverse KL divergence. In
our experiments with SDSS images of the M2 globular cluster, StarNet is
substantially more accurate than two competing methods: Probablistic Cataloging
(PCAT), a method that uses MCMC for inference, and a software pipeline employed
by SDSS for deblending (DAOPHOT). In addition, StarNet is as much as $100,000$
times faster than PCAT, exhibiting the scaling characteristics necessary to
perform fully Bayesian inference on modern astronomical surveys.
</p>
"
A Local Convergence Theory for Mildly Over-Parameterized Two-Layer Neural Network. (arXiv:2102.02410v1 [cs.LG]),http://arxiv.org/abs/2102.02410,"<p>While over-parameterization is widely believed to be crucial for the success
of optimization for the neural networks, most existing theories on
over-parameterization do not fully explain the reason -- they either work in
the Neural Tangent Kernel regime where neurons don't move much, or require an
enormous number of neurons. In practice, when the data is generated using a
teacher neural network, even mildly over-parameterized neural networks can
achieve 0 loss and recover the directions of teacher neurons. In this paper we
develop a local convergence theory for mildly over-parameterized two-layer
neural net. We show that as long as the loss is already lower than a threshold
(polynomial in relevant parameters), all student neurons in an
over-parameterized two-layer neural network will converge to one of teacher
neurons, and the loss will go to 0. Our result holds for any number of student
neurons as long as it is at least as large as the number of teacher neurons,
and our convergence rate is independent of the number of student neurons. A key
component of our analysis is the new characterization of local optimization
landscape -- we show the gradient satisfies a special case of Lojasiewicz
property which is different from local strong convexity or PL conditions used
in previous work.
</p>
"
On Single-User Interactive Beam Alignment in Millimeter Wave Systems: Impact of Feedback Delay. (arXiv:2102.02413v1 [cs.IT]),http://arxiv.org/abs/2102.02413,"<p>Narrow beams are key to wireless communications in millimeter wave frequency
bands. Beam alignment (BA) allows the base station (BS) to adjust the direction
and width of the beam used for communication. During BA, the BS transmits a
number of scanning beams covering different angular regions. The goal is to
minimize the expected width of the uncertainty region (UR) that includes the
angle of departure of the user. Conventionally, in interactive BA, it is
assumed that the feedback corresponding to each scanning packet is received
prior to transmission of the next one. However, in practice, the feedback delay
could be larger because of propagation or system constraints. This paper
investigates BA strategies that operate under arbitrary fixed feedback delays.
This problem is analyzed through a source coding prospective where the feedback
sequences are viewed as source codewords. It is shown that these codewords form
a codebook with a particular characteristic which is used to define a new class
of codes called d-unimodal codes. By analyzing the properties of these codes, a
lower bound on the minimum achievable expected beamwidth is provided. The
results reveal potential performance improvements in terms of the BA duration
it takes to achieve a fixed expected width of the UR over the state-of-the-art
BA methods which do not consider the effect of delay.
</p>
"
Learning Noise Transition Matrix from Only Noisy Labels via Total Variation Regularization. (arXiv:2102.02414v1 [stat.ML]),http://arxiv.org/abs/2102.02414,"<p>Many weakly supervised classification methods employ a noise transition
matrix to capture the class-conditional label corruption. To estimate the
transition matrix from noisy data, existing methods often need to estimate the
noisy class-posterior, which could be unreliable due to the overconfidence of
neural networks. In this work, we propose a theoretically grounded method that
can estimate the noise transition matrix and learn a classifier simultaneously,
without relying on the error-prone noisy class-posterior estimation.
Concretely, inspired by the characteristics of the stochastic label corruption
process, we propose total variation regularization, which encourages the
predicted probabilities to be more distinguishable from each other. Under mild
assumptions, the proposed method yields a consistent estimator of the
transition matrix. We show the effectiveness of the proposed method through
experiments on benchmark and real-world datasets.
</p>
"
New upper bounds for the forgotten index among bicyclic graphs. (arXiv:2102.02415v1 [math.CO]),http://arxiv.org/abs/2102.02415,"<p>The forgotten topological index of a graph $G$, denoted by $F(G)$, is defined
as the sum of weights $d(u)^{2}+d(v)^{2}$ over all edges $uv$ of $G$ , where
$d(u)$ denotes the degree of a vertex $u$. In this paper, we give sharp upper
bounds of the F-index (forgotten topological index) over bicyclic graphs, in
terms of the order and maximum degree.
</p>
"
An end-to-end trainable hybrid classical-quantum classifier. (arXiv:2102.02416v1 [quant-ph]),http://arxiv.org/abs/2102.02416,"<p>We introduce a hybrid model combining a quantum-inspired tensor network and a
variational quantum circuit to perform supervised learning tasks. This
architecture allows for the classical and quantum parts of the model to be
trained simultaneously, providing an end-to-end training framework. We show
that compared to the principal component analysis, a tensor network based on
the matrix product state with low bond dimensions performs better as a feature
extractor for the input data of the variational quantum circuit in the binary
and ternary classification of MNIST and Fashion-MNIST datasets. The
architecture is highly adaptable and the classical-quantum boundary can be
adjusted according the availability of the quantum resource by exploiting the
correspondence between tensor networks and quantum circuits.
</p>
"
Audio Adversarial Examples: Attacks Using Vocal Masks. (arXiv:2102.02417v1 [cs.SD]),http://arxiv.org/abs/2102.02417,"<p>We construct audio adversarial examples on automatic Speech-To-Text systems .
Given any audio waveform, we produce an another by overlaying an audio vocal
mask generated from the original audio. We apply our audio adversarial attack
to five SOTA STT systems: DeepSpeech, Julius, Kaldi, wav2letter@anywhere and
CMUSphinx. In addition, we engaged human annotators to transcribe the
adversarial audio. Our experiments show that these adversarial examples fool
State-Of-The-Art Speech-To-Text systems, yet humans are able to consistently
pick out the speech. The feasibility of this attack introduces a new domain to
study machine and human perception of speech.
</p>
"
First-Passage Time Statistics on Surfaces of General Shape: Surface PDE Solvers using Generalized Moving Least Squares (GMLS). (arXiv:2102.02421v1 [math.NA]),http://arxiv.org/abs/2102.02421,"<p>We develop numerical methods for computing statistics of stochastic processes
on surfaces of general shape with drift-diffusion dynamics $d{X}_t = a({X}_t)dt
+ {b}({X}_t)d{W}_t$. We consider on a surface domain $\Omega$ the statistics
$u(\mathbf{x}) = \mathbb{E}^{\mathbf{x}}\left[\int_0^\tau g(X_t)dt \right] +
\mathbb{E}^{\mathbf{x}}\left[f(X_\tau)\right]$ with the exit stopping time
$\tau = \inf_t \{t &gt; 0 \; |\; X_t \not\in \Omega\}$. Using Dynkin's formula, we
compute statistics by developing high-order Generalized Moving Least Squares
(GMLS) solvers for the associated surface PDE boundary-value problems. We focus
particularly on the mean First Passage Times (FPTs) given by the special case
$f = 0,\, g = 1$ with $u(\mathbf{x}) =
\mathbb{E}^{\mathbf{x}}\left[\tau\right]$. We perform studies for a variety of
shapes showing our methods converge with high-order accuracy both in capturing
the geometry and the surface PDE solutions. We then perform studies showing how
FPTs are influenced by the surface geometry, drift dynamics, and spatially
dependent diffusivities.
</p>
"
A Game-Theoretic Approach to Secure Estimation and Control for Cyber-Physical Systems with a Digital Twin. (arXiv:2102.02428v1 [eess.SY]),http://arxiv.org/abs/2102.02428,"<p>Cyber-Physical Systems (CPSs) play an increasingly significant role in many
critical applications. These valuable applications attract various
sophisticated attacks. This paper considers a stealthy estimation attack, which
aims to modify the state estimation of the CPSs. The intelligent attackers can
learn defense strategies and use clandestine attack strategies to avoid
detection. To address the issue, we design a Chi-square detector in a Digital
Twin (DT), which is an online digital model of the physical system. We use a
Signaling Game with Evidence (SGE) to find the optimal attack and defense
strategies. Our analytical results show that the proposed defense strategies
can mitigate the impact of the attack on the physical estimation and guarantee
the stability of the CPSs. Finally, we use an illustrative application to
evaluate the performance of the proposed framework.
</p>
"
Reconfigurable-intelligent-surface-assisted Downlink Transmission Design via Bayesian Optimization. (arXiv:2102.02430v1 [cs.IT]),http://arxiv.org/abs/2102.02430,"<p>This paper investigates the transmission design in the
reconfigurable-intelligent-surface (RIS)-assisted downlink system. The channel
state information (CSI) is usually difficult to be estimated at the base
station (BS) when the RIS is not equipped with radio frequency chains. In this
paper, we propose a downlink transmission framework with unknown CSI via
Bayesian optimization. Since the CSI is not available at the BS, we treat the
unknown objective function as the black-box function and take the beamformer,
the phase shift, and the receiving filter as the input. Then the objective
function is decomposed as the sum of low-dimension subfunctions to reduce the
complexity. By re-expressing the power constraint of the BS in spherical
coordinates, the original constraint problem is converted into an equivalent
unconstrained problem. The users estimate the sum MSE of the training symbols
as the objective value and feed it back to the BS. We assume a Gaussian prior
of the feedback samples and the next query point is updated by minimizing the
constructed acquisition function. Furthermore, this framework can also be
applied to the power transfer system and fairness problems. Simulation results
validate the effectiveness of the proposed transmission scheme in the downlink
data transmission and power transfer.
</p>
"
Graph Coding for Model Selection and Anomaly Detection in Gaussian Graphical Models. (arXiv:2102.02431v1 [cs.LG]),http://arxiv.org/abs/2102.02431,"<p>A classic application of description length is for model selection with the
minimum description length (MDL) principle. The focus of this paper is to
extend description length for data analysis beyond simple model selection and
sequences of scalars. More specifically, we extend the description length for
data analysis in Gaussian graphical models. These are powerful tools to model
interactions among variables in a sequence of i.i.d Gaussian data in the form
of a graph. Our method uses universal graph coding methods to accurately
account for model complexity, and therefore provide a more rigorous approach
for graph model selection. The developed method is tested with synthetic and
electrocardiogram (ECG) data to find the graph model and anomaly in Gaussian
graphical models. The experiments show that our method gives better performance
compared to commonly used methods.
</p>
"
The use of a time-fractional transport model for performing computational homogenisation of 2D heterogeneous media exhibiting memory effects. (arXiv:2102.02432v1 [math.NA]),http://arxiv.org/abs/2102.02432,"<p>In this work, a two-dimensional time-fractional subdiffusion model is
developed to investigate the underlying transport phenomena evolving in a
binary medium comprised of two sub-domains occupied by homogeneous material. We
utilise an unstructured mesh control volume method to validate the model
against a derived semi-analytical solution for a class of two-layered problems.
This generalised transport model is then used to perform computational
homogenisation on various two-dimensional heterogenous porous media. A key
contribution of our work is to extend the classical homogenisation theory to
accommodate the new framework and show that the effective diffusivity tensor
can be computed once the cell problems reach steady state at the microscopic
scale. We verify the theory for binary media via a series of well-known test
problems and then investigate media having inclusions that exhibit a molecular
relaxation (memory) effect. Finally, we apply the generalised transport model
to estimate the bound water diffusivity tensor on cellular structures obtained
from environmental scanning electron microscope (ESEM) images for Spruce wood
and Australian hardwood. A highlight of our work is that the computed
diffusivity for the heterogeneous media with molecular relaxation is quite
different from the classical diffusion cases, being dominated at steady-state
by the material with memory effects.
</p>
"
Assessing Individual and Community Vulnerability to Fake News in Social Networks. (arXiv:2102.02434v1 [cs.SI]),http://arxiv.org/abs/2102.02434,"<p>The plague of false information, popularly called fake news has affected
lives of news consumers ever since the prevalence of social media. Thus
understanding the spread of false information in social networks has gained a
lot of attention in the literature. While most proposed models do content
analysis of the information, no much work has been done by exploring the
community structures that also play an important role in determining how people
get exposed to it. In this paper we base our idea on Computational Trust in
social networks to propose a novel Community Health Assessment model against
fake news. Based on the concepts of neighbor, boundary and core nodes of a
community, we propose novel evaluation metrics to quantify the vulnerability of
nodes (individual-level) and communities (group-level) to spreading false
information. Our model hypothesizes that if the boundary nodes trust the
neighbor nodes of a community who are spreaders, the densely-connected core
nodes of the community are highly likely to become spreaders. We test our model
with communities generated using three popular community detection algorithms
based on two new datasets of information spreading networks collected from
Twitter. Our experimental results show that the proposed metrics perform
clearly better on the networks spreading false information than on those
spreading true ones, indicating our community health assessment model is
effective.
</p>
"
"Converse, Focus and Guess -- Towards Multi-Document Driven Dialogue. (arXiv:2102.02435v1 [cs.CL])",http://arxiv.org/abs/2102.02435,"<p>We propose a novel task, Multi-Document Driven Dialogue (MD3), in which an
agent can guess the target document that the user is interested in by leading a
dialogue. To benchmark progress, we introduce a new dataset of GuessMovie,
which contains 16,881 documents, each describing a movie, and associated 13,434
dialogues. Further, we propose the MD3 model. Keeping guessing the target
document in mind, it converses with the user conditioned on both document
engagement and user feedback. In order to incorporate large-scale external
documents into the dialogue, it pretrains a document representation which is
sensitive to attributes it talks about an object. Then it tracks dialogue state
by detecting evolvement of document belief and attribute belief, and finally
optimizes dialogue policy in principle of entropy decreasing and reward
increasing, which is expected to successfully guess the user's target in a
minimum number of turns. Experiments show that our method significantly
outperforms several strong baseline methods and is very close to human's
performance.
</p>
"
"Lower Bound on the Optimal Access Bandwidth of ($K+2,K,2$)-MDS Array Code with Degraded Read Friendly. (arXiv:2102.02436v1 [cs.IT])",http://arxiv.org/abs/2102.02436,"<p>Accessing the data in the failed disk (degraded read) with low latency is
crucial for an erasure-coded storage system. In this work, the maximum distance
separable (MDS) array code with the property of degraded-read friendly (DRF) is
discussed. For the DRF MDS array code with 2 redundant nodes and the
sub-packetization level of 2, the lower bound of its access bandwidth is
derived.
</p>
"
EUCA: A Practical Prototyping Framework towards End-User-Centered Explainable Artificial Intelligence. (arXiv:2102.02437v1 [cs.HC]),http://arxiv.org/abs/2102.02437,"<p>The ability to explain decisions to its end-users is a necessity to deploy AI
as critical decision support. Yet making AI explainable to end-users is a
relatively ignored and challenging problem. To bridge the gap, we first
identified twelve end-user-friendly explanatory forms that do not require
technical knowledge to comprehend, including feature-, example-, and rule-based
explanations. We then instantiated the explanatory forms as prototyping cards
in four AI-assisted critical decision-making tasks, and conducted a user study
to co-design low-fidelity prototypes with 32 layperson participants. The
results verified the relevance of using the explanatory forms as building
blocks of explanations, and identified their proprieties (pros, cons,
applicable explainability needs, and design implications). The explanatory
forms, their proprieties, and prototyping support constitute the
End-User-Centered explainable AI framework EUCA. It serves as a practical
prototyping toolkit for HCI/AI practitioners and researchers to build
end-user-centered explainable AI.
</p>
<p>The EUCA framework is available at <a href=""http://weina.me/end-user-xai"">this http URL</a>
</p>
"
Towards Decentralized Human-Swarm Interaction by Means of Sequential Hand Gesture Recognition. (arXiv:2102.02439v1 [cs.RO]),http://arxiv.org/abs/2102.02439,"<p>In this work, we present preliminary work on a novel method for Human-Swarm
Interaction (HSI) that can be used to change the macroscopic behavior of a
swarm of robots with decentralized sensing and control. By integrating a small
yet capable hand gesture recognition convolutional neural network (CNN) with
the next-generation Robot Operating System \emph{ros2}, which enables
decentralized implementation of robot software for multi-robot applications, we
demonstrate the feasibility of programming a swarm of robots to recognize and
respond to a sequence of hand gestures that capable of correspond to different
types of swarm behaviors. We test our approach using a sequence of gestures
that modifies the target inter-robot distance in a group of three Turtlebot3
Burger robots in order to prevent robot collisions with obstacles. The approach
is validated in three different Gazebo simulation environments and in a
physical testbed that reproduces one of the simulated environments.
</p>
"
Online Sketch-based Query Optimization. (arXiv:2102.02440v1 [cs.DB]),http://arxiv.org/abs/2102.02440,"<p>Cost-based query optimization remains a critical task in relational databases
even after decades of research and industrial development. Query optimizers
rely on a large range of statistical synopses -- including attribute-level
histograms and table-level samples -- for accurate cardinality estimation. As
the complexity of selection predicates and the number of join predicates
increase, two problems arise. First, statistics cannot be incrementally
composed to effectively estimate the cost of the sub-plans generated in plan
enumeration. Second, small errors are propagated exponentially through join
operators, which can lead to severely sub-optimal plans. In this paper, we
introduce COMPASS, a novel query optimization paradigm for in-memory databases
based on a single type of statistics -- Fast-AGMS sketches. In COMPASS, query
optimization and execution are intertwined. Selection predicates and sketch
updates are pushed-down and evaluated online during query optimization. This
allows Fast-AGMS sketches to be computed only over the relevant tuples -- which
enhances cardinality estimation accuracy. Plan enumeration is performed over
the query join graph by incrementally composing attribute-level sketches -- not
by building a separate sketch for every sub-plan. We prototype COMPASS in MapD
-- an open-source parallel database -- and perform extensive experiments over
the complete JOB benchmark. The results prove that COMPASS generates better
execution plans -- both in terms of cardinality and runtime -- compared to four
other database systems. Overall, COMPASS achieves a speedup ranging from 1.35X
to 11.28X in cumulative query execution time over the considered competitors.
</p>
"
Persistent Rule-based Interactive Reinforcement Learning. (arXiv:2102.02441v1 [cs.AI]),http://arxiv.org/abs/2102.02441,"<p>Interactive reinforcement learning has allowed speeding up the learning
process in autonomous agents by including a human trainer providing extra
information to the agent in real-time. Current interactive reinforcement
learning research has been limited to interactions that offer relevant advice
to the current state only. Additionally, the information provided by each
interaction is not retained and instead discarded by the agent after a
single-use. In this work, we propose a persistent rule-based interactive
reinforcement learning approach, i.e., a method for retaining and reusing
provided knowledge, allowing trainers to give general advice relevant to more
than just the current state. Our experimental results show persistent advice
substantially improves the performance of the agent while reducing the number
of interactions required for the trainer. Moreover, rule-based advice shows
similar performance impact as state-based advice, but with a substantially
reduced interaction count.
</p>
"
The Analysis from Nonlinear Distance Metric to Kernel-based Drug Prescription Prediction System. (arXiv:2102.02446v1 [cs.LG]),http://arxiv.org/abs/2102.02446,"<p>Distance metrics and their nonlinear variant play a crucial role in machine
learning based real-world problem solving. We demonstrated how Euclidean and
cosine distance measures differ not only theoretically but also in real-world
medical application, namely, outcome prediction of drug prescription. Euclidean
distance exhibits favorable properties in the local geometry problem. To this
regard, Euclidean distance can be applied under short-term disease with
low-variation outcome observation. Moreover, when presenting to highly variant
chronic disease, it is preferable to use cosine distance. These different
geometric properties lead to different submanifolds in the original embedded
space, and hence, to different optimizing nonlinear kernel embedding
frameworks. We first established the geometric properties that we needed in
these frameworks. From these properties interpreted their differences in
certain perspectives. Our evaluation on real-world, large-scale electronic
health records and embedding space visualization empirically validated our
approach.
</p>
"
Safety during Transient Response in Direct Current Microgrids using Control Barrier Functions. (arXiv:2102.02448v1 [eess.SY]),http://arxiv.org/abs/2102.02448,"<p>We consider the problem of guaranteeing that the transient voltages and
currents stay within prescribed bounds in Direct Current (DC) microgrids, when
the controller does not have access to accurate system dynamics due to the load
being unknown and/or time-varying. To achieve this, we propose an optimization
based controller design using control barrier functions. We show that the
proposed controller has a decentralized structure and is robust with respect to
the uncertainty in the precise values of the system parameters, such as the
load.
</p>
"
Hybrid Adversarial Inverse Reinforcement Learning. (arXiv:2102.02454v1 [cs.LG]),http://arxiv.org/abs/2102.02454,"<p>In this paper, we investigate the problem of the inverse reinforcement
learning (IRL), especially the beyond-demonstrator (BD) IRL. The BD-IRL aims to
not only imitate the expert policy but also extrapolate BD policy based on
finite demonstrations of the expert. Currently, most of the BD-IRL algorithms
are two-stage, which first infer a reward function then learn the policy via
reinforcement learning (RL). Because of the two separate procedures, the
two-stage algorithms have high computation complexity and lack robustness. To
overcome these flaw, we propose a BD-IRL framework entitled hybrid adversarial
inverse reinforcement learning (HAIRL), which successfully integrates the
imitation and exploration into one procedure. The simulation results show that
the HAIRL is more efficient and robust when compared with other similar
state-of-the-art (SOTA) algorithms.
</p>
"
Privacy Preserving and Resilient RPKI. (arXiv:2102.02456v1 [cs.CR]),http://arxiv.org/abs/2102.02456,"<p>Resource Public Key Infrastructure (RPKI) is vital to the security of
inter-domain routing. However, RPKI enables Regional Internet Registries (RIRs)
to unilaterally takedown IP prefixes - indeed, such attacks have been launched
by nation-state adversaries. The threat of IP prefix takedowns is one of the
factors hindering RPKI adoption.
</p>
<p>In this work, we propose the first distributed RPKI system, based on
threshold signatures, that requires the coordination of a number of RIRs to
make changes to RPKI objects; hence, preventing unilateral prefix takedown. We
perform extensive evaluations using our implementation demonstrating the
practicality of our solution. Furthermore, we show that our system is scalable
and remains efficient even when RPKI is widely deployed.
</p>
"
Deep Face Fuzzy Vault: Implementation and Performance. (arXiv:2102.02458v1 [cs.CV]),http://arxiv.org/abs/2102.02458,"<p>Deep convolutional neural networks have achieved remarkable improvements in
facial recognition performance. Similar kinds of developments, e.g.
deconvolutional neural networks, have shown impressive results for
reconstructing face images from their corresponding embeddings in the latent
space. This poses a severe security risk which necessitates the protection of
stored deep face embeddings in order to prevent from misuse, e.g. identity
fraud.
</p>
<p>In this work, an unlinkable improved deep face fuzzy vault-based template
protection scheme is presented. To this end, a feature transformation method is
introduced which maps fixed-length real-valued deep face embeddings to
integer-valued feature sets. As part of said feature transformation, a detailed
analysis of different feature quantisation and binarisation techniques is
conducted using features extracted with a state-of-the-art deep convolutional
neural network trained with the additive angular margin loss (ArcFace). At key
binding, obtained feature sets are locked in an unlinkable improved fuzzy
vault. For key retrieval, the efficiency of different polynomial reconstruction
techniques is investigated. The proposed feature transformation method and
template protection scheme are agnostic of the biometric characteristic and,
thus, can be applied to virtually any biometric features computed by a deep
neural network.
</p>
<p>For the best configuration, a false non-match rate below 1% at a false match
rate of 0.01%, is achieved in cross-database experiments on the FERET and
FRGCv2 face databases. On average, a security level of up to approximately 28
bits is obtained. This work presents the first effective face-based fuzzy vault
scheme providing privacy protection of facial reference data as well as digital
key derivation from face.
</p>
"
DIFFnet: Diffusion parameter mapping network generalized for input diffusion gradient schemes and bvalues. (arXiv:2102.02463v1 [eess.IV]),http://arxiv.org/abs/2102.02463,"<p>In MRI, deep neural networks have been proposed to reconstruct diffusion
model parameters. However, the inputs of the networks were designed for a
specific diffusion gradient scheme (i.e., diffusion gradient directions and
numbers) and a specific b-value that are the same as the training data. In this
study, a new deep neural network, referred to as DIFFnet, is developed to
function as a generalized reconstruction tool of the diffusion-weighted signals
for various gradient schemes and b-values. For generalization, diffusion
signals are normalized in a q-space and then projected and quantized, producing
a matrix (Qmatrix) as an input for the network. To demonstrate the validity of
this approach, DIFFnet is evaluated for diffusion tensor imaging (DIFFnetDTI)
and for neurite orientation dispersion and density imaging (DIFFnetNODDI). In
each model, two datasets with different gradient schemes and b-values are
tested. The results demonstrate accurate reconstruction of the diffusion
parameters at substantially reduced processing time (approximately 8.7 times
and 2240 times faster processing time than conventional methods in DTI and
NODDI, respectively; less than 4% mean normalized root-mean-square errors
(NRMSE) in DTI and less than 8% in NODDI). The generalization capability of the
networks was further validated using reduced numbers of diffusion signals from
the datasets. Different from previously proposed deep neural networks, DIFFnet
does not require any specific gradient scheme and b-value for its input. As a
result, it can be adopted as an online reconstruction tool for various complex
diffusion imaging.
</p>
"
App Developer Centric Trusted Execution Environment. (arXiv:2102.02465v1 [cs.CR]),http://arxiv.org/abs/2102.02465,"<p>ARM TrustZone is the de-facto hardware TEE implementation on mobile devices
like smartphones. As a vendor-centric TEE, TrustZone greatly overlooks the
strong protection demands and requirements from the App developers. Several
security solutions have been proposed to enable the TEE-assisted isolation in
the Normal World of ARM, attempting to balance the security and usability.
However, they are still not full-fledged in serving Apps' needs. In this paper,
we introduce LEAP, which is a lightweight App developer Centric TEE solution in
the Normal World. LEAP offers the auto DevOps tool to help developers to
prepare the codes running on it, enables isolated codes to execute in parallel
and access peripheral (e.g. mobile GPUs) with ease, and dynamically manage
system resources upon Apps' requests. We implement the LEAP prototype on the
off-the-shelf ARM platform without any hardware change. We perform the
comprehensive analyses and experiments to demonstrate that LEAP is efficient in
design, comprehensive in support, and convenient in adoption.
</p>
"
Cumulant Expansion of Mutual Information for Quantifying Leakage of a Protected Secret. (arXiv:2102.02468v1 [cs.IT]),http://arxiv.org/abs/2102.02468,"<p>The information leakage of a cryptographic implementation with a given degree
of protection is evaluated in a typical situation when the signal-to-noise
ratio is small. This is solved by expanding Kullback-Leibler divergence,
entropy, and mutual information in terms of moments/cumulants.
</p>
"
Machine Learning-Based Generalized Model for Finite Element Analysis of Roll Deflection During the Austenitic Stainless Steel 316L Strip Rolling. (arXiv:2102.02470v1 [cs.LG]),http://arxiv.org/abs/2102.02470,"<p>During the strip rolling process, a considerable amount of the forces of the
material pressure cause elastic deformation on the work-roll, i.e., the
deflection process. The uncontrollable amount of the work-roll deflection leads
to the high deviations in the permissible thickness of the plate along its
width. In the context of the Austenitic Stainless Steels (ASS), due to the
instability of the Austenite phase in a cold temperature, cold deformation
leads to the production of Strain-Induced Martensite (SIM), which improves the
mechanical properties. It leads to the hardening of the ASS 316L during the
cold deformation, which causes the Strain-Stress curve of the ASS 316L to
behave non-linearly, which distinguishes it from other categories of steels. To
account for this phenomenon, we propose to utilize a Machine Learning (ML)
method to predict more accurately the flow stress of the ASS 316L during the
cold rolling. Furthermore, we conduct various mechanical tensile tests in order
to obtain the required dataset, Stress316L, for training the neural network.
Moreover, instead of using a constant value of flow stress during the
multi-pass rolling process, we use a Finite Difference (FD) formulation of the
equilibrium equation in order to account for the dynamic behavior of the flow
stress, which leads to the estimation of the mean pressure, which the strip
enforces to the rolls during deformation. Finally, using the Finite Element
Analysis (FEA), the deflection of the work-roll tools will be calculated. As a
result, we end up with a generalized model for the calculation of the roll
deflection, specific to the ASS 316L. To the best of our knowledge, this is the
first model for ASS 316L which considers dynamic flow stress and SIM of the
rolled plate, using FEM and an ML approach, which could contribute to the
better design of the tolls.
</p>
"
Transfer Learning in Bandits with Latent Continuity. (arXiv:2102.02472v1 [cs.LG]),http://arxiv.org/abs/2102.02472,"<p>Structured stochastic multi-armed bandits provide accelerated regret rates
over the standard unstructured bandit problems. Most structured bandits,
however, assume the knowledge of the structural parameter such as Lipschitz
continuity, which is often not available. To cope with the latent structural
parameter, we consider a transfer learning setting in which an agent must learn
to transfer the structural information from the prior tasks to the next task,
which is inspired by practical problems such as rate adaptation in wireless
link. We propose a novel framework to provably and accurately estimate the
Lipschitz constant based on previous tasks and fully exploit it for the new
task at hand. We analyze the efficiency of the proposed framework in two folds:
(i) the sample complexity of our estimator matches with the
information-theoretic fundamental limit; and (ii) our regret bound on the new
task is close to that of the oracle algorithm with the full knowledge of the
Lipschitz constant under mild assumptions. Our analysis reveals a set of useful
insights on transfer learning for latent Lipschitzconstants such as the
fundamental challenge a learner faces. Our numerical evaluations confirm our
theoretical findings and show the superiority of the proposed framework
compared to baselines.
</p>
"
Error analysis of some nonlocal diffusion discretization schemes. (arXiv:2102.02476v1 [math.NA]),http://arxiv.org/abs/2102.02476,"<p>We study two numerical approximations of solutions of nonlocal diffusion
evolution problems which are inspired in algorithms for computing the bilateral
denoising filtering of an image, and which are based on functional
rearrangements and on the Fourier transform. Apart from the usual time-space
discretization, these algorithms also use the discretization of the range of
the solution (quantization). We show that the discrete approximations converge
to the continuous solution in suitable functional spaces, and provide error
estimates. Finally, we present some numerical experiments illustrating the
performance of the algorithms, specially focusing in the execution time.
</p>
"
Bangla Text Dataset and Exploratory Analysis for Online Harassment Detection. (arXiv:2102.02478v1 [cs.CL]),http://arxiv.org/abs/2102.02478,"<p>Being the seventh most spoken language in the world, the use of the Bangla
language online has increased in recent times. Hence, it has become very
important to analyze Bangla text data to maintain a safe and harassment-free
online place. The data that has been made accessible in this article has been
gathered and marked from the comments of people in public posts by celebrities,
government officials, athletes on Facebook. The total amount of collected
comments is 44001. The dataset is compiled with the aim of developing the
ability of machines to differentiate whether a comment is a bully expression or
not with the help of Natural Language Processing and to what extent it is
improper if it is an inappropriate comment. The comments are labeled with
different categories of harassment. Exploratory analysis from different
perspectives is also included in this paper to have a detailed overview. Due to
the scarcity of data collection of categorized Bengali language comments, this
dataset can have a significant role for research in detecting bully words,
identifying inappropriate comments, detecting different categories of Bengali
bullies, etc. The dataset is publicly available at
https://data.mendeley.com/datasets/9xjx8twk8p.
</p>
"
Kernelization of Maximum Minimal Vertex Cover. (arXiv:2102.02484v1 [cs.DS]),http://arxiv.org/abs/2102.02484,"<p>In the Maximum Minimal Vertex Cover (MMVC) problem, we are given a graph $G$
and a positive integer $k$, and the objective is to decide whether $G$ contains
a minimal vertex cover of size at least $k$. This problem has been considered
in several articles in the last years. We focus on its kernelization, which had
been almost unexplored so far. We prove that MMVC does not admit polynomial
kernels parameterized by the size of a minimum vertex cover, even on bipartite
graphs, unless ${\sf NP} \subseteq {\sf coNP} / {\sf poly}$. Motivated by a
question of Boria et al. [Discret. Appl. Math. 2015] about the existence of
subquadratic kernels for MMVC parameterized by $k$, we rule out their existence
unless $P=NP$, if we restrict the kernelization algorithms to apply only a type
of natural reduction rules that we call ""large optimal preserving rules"". In
particular, these rules contain the typical reduction rules to obtain linear
kernels for Vertex Cover. On the positive side, we provide subquadratic kernels
on $H$-free graphs for several graphs $H$, such as the bull, the paw, or the
complete graphs, by making use of the Erd\H{o}s-Hajnal property in order to
find an appropriate decomposition.
</p>
"
Image Restoration by Deep Projected GSURE. (arXiv:2102.02485v1 [cs.CV]),http://arxiv.org/abs/2102.02485,"<p>Ill-posed inverse problems appear in many image processing applications, such
as deblurring and super-resolution. In recent years, solutions that are based
on deep Convolutional Neural Networks (CNNs) have shown great promise. Yet,
most of these techniques, which train CNNs using external data, are restricted
to the observation models that have been used in the training phase. A recent
alternative that does not have this drawback relies on learning the target
image using internal learning. One such prominent example is the Deep Image
Prior (DIP) technique that trains a network directly on the input image with a
least-squares loss. In this paper, we propose a new image restoration framework
that is based on minimizing a loss function that includes a ""projected-version""
of the Generalized SteinUnbiased Risk Estimator (GSURE) and parameterization of
the latent image by a CNN. We demonstrate two ways to use our framework. In the
first one, where no explicit prior is used, we show that the proposed approach
outperforms other internal learning methods, such as DIP. In the second one, we
show that our GSURE-based loss leads to improved performance when used within a
plug-and-play priors scheme.
</p>
"
From a Point Cloud to a Simulation Model: Bayesian Segmentation and Entropy based Uncertainty Estimation for 3D Modelling. (arXiv:2102.02488v1 [stat.ML]),http://arxiv.org/abs/2102.02488,"<p>The 3D modelling of indoor environments and the generation of process
simulations play an important role in factory and assembly planning. In
brownfield planning cases existing data are often outdated and incomplete
especially for older plants, which were mostly planned in 2D. Thus, current
environment models cannot be generated directly on the basis of existing data
and a holistic approach on how to build such a factory model in a highly
automated fashion is mostly non-existent. Major steps in generating an
environment model in a production plant include data collection and
pre-processing, object identification as well as pose estimation. In this work,
we elaborate a methodical workflow, which starts with the digitalization of
large-scale indoor environments and ends with the generation of a static
environment or simulation model. The object identification step is realized
using a Bayesian neural network capable of point cloud segmentation. We
elaborate how the information on network uncertainty generated by a Bayesian
segmentation framework can be used in order to build up a more accurate
environment model. The steps of data collection and point cloud segmentation as
well as the resulting model accuracy are evaluated on a real-world data set
collected at the assembly line of a large-scale automotive production plant.
The segmentation network is further evaluated on the publicly available
Stanford Large-Scale 3D Indoor Spaces data set. The Bayesian segmentation
network clearly surpasses the performance of the frequentist baseline and
allows us to increase the accuracy of the model placement in a simulation scene
considerably.
</p>
"
Boundary Stabilization and Observation of an Unstable Heat Equation in a General Multi-dimensional Domain. (arXiv:2102.02492v1 [eess.SY]),http://arxiv.org/abs/2102.02492,"<p>In this paper, we consider the exponential stabilization and observation of
an unstable heat equation in a general multi-dimensional domain by combining
the finite-dimensional spectral truncation technique and the recently developed
dynamics compensation approach. In contrast to the unstable one-dimensional
partial differential equation (PDE), such as the transport equation, wave
equation and the heat equation, that can be treated by the well-known PDE
backstepping method, stabilization of unstable PDE in a general
multi-dimensional domain is still a challenging problem. We treat the
stabilization and observation problems separately. A dynamical state feedback
law is proposed firstly to stabilize the unstable heat equation exponentially
and then a state observer is designed via a boundary measurement. Both the
stability of the closed-loop system and the well-posedness of the observer are
proved. Some of the theoretical results are validated by the numerical
simulations.
</p>
"
Keep it Simple: Data-efficient Learning for Controlling Complex Systems with Simple Models. (arXiv:2102.02493v1 [cs.RO]),http://arxiv.org/abs/2102.02493,"<p>When manipulating a novel object with complex dynamics, a state
representation is not always available, for example for deformable objects.
Learning both a representation and dynamics from observations requires large
amounts of data. We propose Learned Visual Similarity Predictive Control
(LVSPC), a novel method for data-efficient learning to control systems with
complex dynamics and high-dimensional state spaces from images. LVSPC leverages
a given simple model approximation from which image observations can be
generated. We use these images to train a perception model that estimates the
simple model state from observations of the complex system online. We then use
data from the complex system to fit the parameters of the simple model and
learn where this model is inaccurate, also online. Finally, we use Model
Predictive Control and bias the controller away from regions where the simple
model is inaccurate and thus where the controller is less reliable. We evaluate
LVSPC on two tasks; manipulating a tethered mass and a rope. We find that our
method performs comparably to state-of-the-art reinforcement learning methods
with an order of magnitude less data. LVSPC also completes the rope
manipulation task on a real robot with 80% success rate after only 10 trials,
despite using a perception system trained only on images from simulation.
</p>
"
Permutation-invariant quantum coding for quantum deletion channels. (arXiv:2102.02494v1 [quant-ph]),http://arxiv.org/abs/2102.02494,"<p>Quantum deletions, which are harder to correct than erasure errors, occur in
many realistic settings. It is therefore pertinent to develop quantum coding
schemes for quantum deletion channels. To date, not much is known about which
explicit quantum error correction codes can combat quantum deletions. We note
that {\em any} permutation-invariant quantum code that has a distance of $t+1$
can correct $t$ quantum deletions for any positive integer $t$ in both the
qubit and the qudit setting. Leveraging on coding properties of
permutation-invariant quantum codes under erasure errors, we derive
corresponding coding bounds for permutation-invariant quantum codes under
quantum deletions. We focus our attention on a specific family of $N$-qubit
permutation-invariant quantum codes, which we call shifted gnu codes, and show
that their encoding and decoding algorithms can be performed in $O(N)$ and
$O(N^2)$.
</p>
"
3D Surface Reconstruction From Multi-Date Satellite Images. (arXiv:2102.02502v1 [cs.CV]),http://arxiv.org/abs/2102.02502,"<p>The reconstruction of accurate three-dimensional environment models is one of
the most fundamental goals in the field of photogrammetry. Since satellite
images provide suitable properties for obtaining large-scale environment
reconstructions, there exist a variety of Stereo Matching based methods to
reconstruct point clouds for satellite image pairs. Recently, the first
Structure from Motion (SfM) based approach has been proposed, which allows to
reconstruct point clouds from multiple satellite images. In this work, we
propose an extension of this SfM based pipeline that allows us to reconstruct
not only point clouds but watertight meshes including texture information. We
provide a detailed description of several steps that are mandatory to exploit
state-of-the-art mesh reconstruction algorithms in the context of satellite
imagery. This includes a decomposition of finite projective camera calibration
matrices, a skew correction of corresponding depth maps and input images as
well as the recovery of real-world depth maps from reparameterized depth
values. The paper presents an extensive quantitative evaluation on multi-date
satellite images demonstrating that the proposed pipeline combined with current
meshing algorithms outperforms state-of-the-art point cloud reconstruction
algorithms in terms of completeness and median error. We make the source code
of our pipeline publicly available.
</p>
"
"Understanding the Capabilities, Limitations, and Societal Impact of Large Language Models. (arXiv:2102.02503v1 [cs.CL])",http://arxiv.org/abs/2102.02503,"<p>On October 14th, 2020, researchers from OpenAI, the Stanford Institute for
Human-Centered Artificial Intelligence, and other universities convened to
discuss open research questions surrounding GPT-3, the largest
publicly-disclosed dense language model at the time. The meeting took place
under Chatham House Rules. Discussants came from a variety of research
backgrounds including computer science, linguistics, philosophy, political
science, communications, cyber policy, and more. Broadly, the discussion
centered around two main questions: 1) What are the technical capabilities and
limitations of large language models? 2) What are the societal effects of
widespread use of large language models? Here, we provide a detailed summary of
the discussion organized by the two themes above.
</p>
"
Meta-strategy for Learning Tuning Parameters with Guarantees. (arXiv:2102.02504v1 [stat.ML]),http://arxiv.org/abs/2102.02504,"<p>Online gradient methods, like the online gradient algorithm (OGA), often
depend on tuning parameters that are difficult to set in practice. We consider
an online meta-learning scenario, and we propose a meta-strategy to learn these
parameters from past tasks. Our strategy is based on the minimization of a
regret bound. It allows to learn the initialization and the step size in OGA
with guarantees. We provide a regret analysis of the strategy in the case of
convex losses. It suggests that, when there are parameters
$\theta_1,\dots,\theta_T$ solving well tasks $1,\dots,T$ respectively and that
are close enough one to each other, our strategy indeed improves on learning
each task in isolation.
</p>
"
Gapped Indexing for Consecutive Occurrences. (arXiv:2102.02505v1 [cs.DS]),http://arxiv.org/abs/2102.02505,"<p>The classic string indexing problem is to preprocess a string S into a
compact data structure that supports efficient pattern matching queries.
Typical queries include existential queries (decide if the pattern occurs in
S), reporting queries (return all positions where the pattern occurs), and
counting queries (return the number of occurrences of the pattern). In this
paper we consider a variant of string indexing, where the goal is to compactly
represent the string such that given two patterns P1 and P2 and a gap range
[\alpha,\beta] we can quickly find the consecutive occurrences of P1 and P2
with distance in [\alpha,\beta], i.e., pairs of occurrences immediately
following each other and with distance within the range. We present data
structures that use \~O(n) space and query time \~O(|P1|+|P2|+n^(2/3)) for
existence and counting and \~O(|P1|+|P2|+n^(2/3)*occ^(1/3)) for reporting. We
complement this with a conditional lower bound based on the set intersection
problem showing that any solution using \~O(n) space must use
\tilde{\Omega}}(|P1|+|P2|+\sqrt{n}) query time. To obtain our results we
develop new techniques and ideas of independent interest including a new suffix
tree decomposition and hardness of a variant of the set intersection problem.
</p>
"
Aitken-Schwarz heterogeneous Domain Decomposition for EMT-TS Simulation. (arXiv:2102.02507v1 [math.NA]),http://arxiv.org/abs/2102.02507,"<p>In this paper, a Schwarz heterogeneous domain decomposition method (DDM) is
used to co-simulate an RLC electrical circuit where a part of the domain is
modeled with Electro-Magnetic Transients (EMT) modeling and the other part with
dynamic phasor (TS) modeling. Domain partitioning is not based on cutting at
transmission lines which introduces a physical delay on the dynamics of the
solution, as is usually done, but only on connectivity considerations. We show
the convergence property of the homogeneous DDM EMT-EMT and TS-TS and of the
heterogeneous DDM TS-EMT, with and without overlap and we use the pure linear
divergence/convergence of the method to accelerate it toward the searched
solution with the Aitken's acceleration of the convergence technique.
</p>
"
An Analysis of International Use of Robots for COVID-19. (arXiv:2102.02509v1 [cs.RO]),http://arxiv.org/abs/2102.02509,"<p>This article analyses data collected on 338 instances of robots used
explicitly in response to COVID-19 from 24 Jan, 2020, to 23 Jan, 2021, in 48
countries. The analysis was guided by four overarching questions: 1) What were
robots used for in the COVID-19 response? 2) When were they used? 3) How did
different countries innovate? and 4) Did having a national policy on robotics
influence a country's innovation and insertion of robotics for COVID-19? The
findings indicate that robots were used for six different sociotechnical work
domains and 29 discrete use cases. When robots were used varied greatly on the
country; although many countries did report an increase at the beginning of
their first surge. To understand the findings of how innovation occurred, the
data was examined through the lens of the technology's maturity according to
NASA's Technical Readiness Assessment metrics. Through this lens, findings note
that existing robots were used for more than 78% of the instances; slightly
modified robots made up 10%; and truly novel robots or novel use cases
constituted 12% of the instances. The findings clearly indicate that countries
with a national robotics initiative were more likely to use robotics more often
and for broader purposes. Finally, the dataset and analysis produces a broad
set of implications that warrant further study and investigation. The results
from this analysis are expected to be of value to the robotics and robotics
policy community in preparing robots for rapid insertion into future disasters.
</p>
"
User Interface Factors of Mobile UX: A Study with an Incident Reporting Application. (arXiv:2102.02510v1 [cs.HC]),http://arxiv.org/abs/2102.02510,"<p>Smartphones are now ubiquitous, yet our understanding of user interface
factors that maximize mobile user experience (UX), is still limited. This work
presents a controlled experiment, which investigated factors that affect the
usability and UX of a mobile incident reporting app. The results indicate that
sequence of user interface elements matters while striving to increase UX, and
that there is no difference between tab and scrolling as navigation modalities
in short forms. These findings can serve as building blocks for
empirically-derived guidelines for mobile incident reporting.
</p>
"
High-Rate Quantum Private Information Retrieval with Weakly Self-Dual Star Product Codes. (arXiv:2102.02511v1 [cs.IT]),http://arxiv.org/abs/2102.02511,"<p>In the classical private information retrieval (PIR) setup, a user wants to
retrieve a file from a database or a distributed storage system (DSS) without
revealing the file identity to the servers holding the data. In the quantum PIR
(QPIR) setting, a user privately retrieves a classical file by receiving
quantum information from the servers. The QPIR problem has been treated by Song
et al. in the case of replicated servers, both with and without collusion. QPIR
over $[n,k]$ maximum distance separable (MDS) coded servers was recently
considered by Allaix et al., but the collusion was essentially restricted to
$t=n-k$ servers. In this paper, the QPIR setting is extended to account for
more flexible collusion of servers satisfying $t &lt; n-k+1$. Similarly to the
previous cases, the rates achieved are better than those known or conjectured
in the classical counterparts, as well as those of the previously proposed
coded and colluding QPIR schemes. This is enabled by considering the stabilizer
formalism and weakly self-dual generalized Reed--Solomon (GRS) star product
codes.
</p>
"
FedAUX: Leveraging Unlabeled Auxiliary Data in Federated Learning. (arXiv:2102.02514v1 [cs.LG]),http://arxiv.org/abs/2102.02514,"<p>Federated Distillation (FD) is a popular novel algorithmic paradigm for
Federated Learning, which achieves training performance competitive to prior
parameter averaging based methods, while additionally allowing the clients to
train different model architectures, by distilling the client predictions on an
unlabeled auxiliary set of data into a student model. In this work we propose
FedAUX, an extension to FD, which, under the same set of assumptions,
drastically improves performance by deriving maximum utility from the unlabeled
auxiliary data. FedAUX modifies the FD training procedure in two ways: First,
unsupervised pre-training on the auxiliary data is performed to find a model
initialization for the distributed training. Second, $(\varepsilon,
\delta)$-differentially private certainty scoring is used to weight the
ensemble predictions on the auxiliary data according to the certainty of each
client model. Experiments on large-scale convolutional neural networks and
transformer models demonstrate, that the training performance of FedAUX exceeds
SOTA FL baseline methods by a substantial margin in both the iid and non-iid
regime, further closing the gap to centralized training performance. Code is
available at github.com/fedl-repo/fedaux.
</p>
"
HYDRA: Hypergradient Data Relevance Analysis for Interpreting Deep Neural Networks. (arXiv:2102.02515v1 [cs.LG]),http://arxiv.org/abs/2102.02515,"<p>The behaviors of deep neural networks (DNNs) are notoriously resistant to
human interpretations. In this paper, we propose Hypergradient Data Relevance
Analysis, or HYDRA, which interprets the predictions made by DNNs as effects of
their training data. Existing approaches generally estimate data contributions
around the final model parameters and ignore how the training data shape the
optimization trajectory. By unrolling the hypergradient of test loss w.r.t. the
weights of training data, HYDRA assesses the contribution of training data
toward test data points throughout the training trajectory. In order to
accelerate computation, we remove the Hessian from the calculation and prove
that, under moderate conditions, the approximation error is bounded.
Corroborating this theoretical claim, empirical results indicate the error is
indeed small. In addition, we quantitatively demonstrate that HYDRA outperforms
influence functions in accurately estimating data contribution and detecting
noisy data labels. The source code is available at
https://github.com/cyyever/aaai_hydra_8686.
</p>
"
Non-Symmetric Coded Caching for Location-Dependent Content Delivery. (arXiv:2102.02518v1 [cs.IT]),http://arxiv.org/abs/2102.02518,"<p>Immersive viewing is emerging as the next interface evolution for
human-computer interaction. A truly wireless immersive application necessitates
immense data delivery with ultra-low latency, raising stringent requirements
for next-generation wireless networks. A potential solution for addressing
these requirements is through the efficient usage of in-device storage and
computation capabilities. This paper proposes a novel location-based coded
cache placement and delivery scheme, which leverages the nested code modulation
(NCM) to enable multi-rate multicasting transmission. To provide a uniform
quality of experience in different network locations, we formulate a linear
programming cache allocation problem. Next, based on the users' spatial
realizations, we adopt an NCM based coded delivery algorithm to efficiently
serve a distinct group of users during each transmission. Numerical results
demonstrate that the proposed location-based delivery method significantly
increases transmission efficiency compared to state of the art.
</p>
"
"Koopman Operator Dynamical Models: Learning, Analysis and Control. (arXiv:2102.02522v1 [eess.SY])",http://arxiv.org/abs/2102.02522,"<p>The Koopman operator allows for handling nonlinear systems through a
(globally) linear representation. In general, the operator is
infinite-dimensional - necessitating finite approximations - for which there is
no overarching framework. Although there are principled ways of learning such
finite approximations, they are in many instances overlooked in favor of, often
ill-posed and unstructured methods. Also, Koopman operator theory has
long-standing connections to known system-theoretic and dynamical system
notions that are not universally recognized. Given the former and latter
realities, this work aims to bridge the gap between various concepts regarding
both theory and tractable realizations. Firstly, we review data-driven
representations (both unstructured and structured) for Koopman operator
dynamical models, categorizing various existing methodologies and highlighting
their differences. Furthermore, we provide concise insight into the paradigm's
relation to system-theoretic notions and analyze the prospect of using the
paradigm for modeling control systems. Additionally, we outline the current
challenges and comment on future perspectives.
</p>
"
Efficient adaptive step size control for exponential integrators. (arXiv:2102.02524v1 [math.NA]),http://arxiv.org/abs/2102.02524,"<p>Traditional step size controllers make the tacit assumption that the cost of
a time step is independent of the step size. This is reasonable for explicit
and implicit integrators that use direct solvers. In the context of exponential
integrators, however, an iterative approach, such as Krylov methods or
polynomial interpolation, to compute the action of the required matrix
functions is usually employed. In this case, the assumption of constant cost is
not valid. This is, in particular, a problem for higher-order exponential
integrators, which are able to take relatively large time steps based on
accuracy considerations. In this paper, we consider an adaptive step size
controller for exponential Rosenbrock methods that determines the step size
based on the premise of minimizing computational cost. The largest allowed step
size, given by accuracy considerations, merely acts as a constraint. We test
this approach on a range of nonlinear partial differential equations. Our
results show significant improvements (up to a factor of 4 reduction in the
computational cost) over the traditional step size controller for a wide range
of tolerances.
</p>
"
Improved Communication Efficiency for Distributed Mean Estimation with Side Information. (arXiv:2102.02525v1 [cs.IT]),http://arxiv.org/abs/2102.02525,"<p>In this paper, we consider the distributed mean estimation problem where the
server has access to some side information, e.g., its local computed mean
estimation or the received information sent by the distributed clients at the
previous iterations. We propose a practical and efficient estimator based on an
r-bit Wynzer-Ziv estimator proposed by Mayekar et al., which requires no
probabilistic assumption on the data. Unlike Mayekar's work which only utilizes
side information at the server, our scheme jointly exploits the correlation
between clients' data and server' s side information, and also between data of
different clients. We derive an upper bound of the estimation error of the
proposed estimator. Based on this upper bound, we provide two algorithms on how
to choose input parameters for the estimator. Finally, parameter regions in
which our estimator is better than the previous one are characterized.
</p>
"
Deep Learning for Short-Term Voltage Stability Assessment of Power Systems. (arXiv:2102.02526v1 [eess.SP]),http://arxiv.org/abs/2102.02526,"<p>To fully learn the latent temporal dependencies from post-disturbance system
dynamic trajectories, deep learning is utilized for short-term voltage
stability (STVS) assessment of power systems in this paper. First of all, a
semi-supervised cluster algorithm is performed to obtain class labels of STVS
instances due to the unavailability of reliable quantitative criteria.
Secondly, a long short-term memory (LSTM) based assessment model is built
through learning the time dependencies from the post-disturbance system
dynamics. Finally, the trained assessment model is employed to determine the
systems stability status in real time. The test results on the IEEE 39-bus
system suggest that the proposed approach manages to assess the stability
status of the system accurately and timely. Furthermore, the superiority of the
proposed method over traditional shallow learning-based assessment methods has
also been proved.
</p>
"
FuzzSplore: Visualizing Feedback-Driven Fuzzing Techniques. (arXiv:2102.02527v1 [cs.CR]),http://arxiv.org/abs/2102.02527,"<p>Fuzz Testing techniques are the state of the art in software testing for
security issues nowadays. Their great effectiveness attracted the attention of
researchers and hackers and involved them in developing a lot of new techniques
to improve Fuzz Testing. The evaluation and the cross-comparison of these
techniques is an almost open problem. In this paper, we propose a human-driven
approach to this problem based on information visualization. We developed a
prototype upon the AFL++ fuzzing framework, FuzzSplore, that an analyst can use
to get useful insights about different fuzzing configurations applied to a
specific target in order to choose or tune the best technique during a fuzzing
campaign.
</p>
"
On the Global Optimality of Whittle's index policy for minimizing the age of information. (arXiv:2102.02528v1 [cs.IT]),http://arxiv.org/abs/2102.02528,"<p>This paper examines the average age minimization problem where only a
fraction of the network users can transmit simultaneously over unreliable
channels. Finding the optimal scheduling scheme, in this case, is known to be
challenging. Accordingly, the Whittle's index policy was proposed in the
literature as a low-complexity heuristic to the problem. Although simple to
implement, characterizing this policy's performance is recognized to be a
notoriously tricky task. In the sequel, we provide a new mathematical approach
to establish its optimality in the many-users regime for specific network
settings. Our novel approach is based on intricate techniques, and unlike
previous works in the literature, it is free of any mathematical assumptions.
These findings showcase that the Whittle's index policy has analytically
provable asymptotic optimality for the AoI minimization problem. Finally, we
lay out numerical results that corroborate our theoretical findings and
demonstrate the policy's notable performance in the many-users regime.
</p>
"
ABCNet: Attentive Bilateral Contextual Network for Efficient Semantic Segmentation of Fine-Resolution Remote Sensing Images. (arXiv:2102.02531v1 [cs.CV]),http://arxiv.org/abs/2102.02531,"<p>Semantic segmentation of remotely sensed images plays a crucial role in
precision agriculture, environmental protection, and economic assessment. In
recent years, substantial fine-resolution remote sensing images are available
for semantic segmentation. However, due to the complicated information caused
by the increased spatial resolution, state-of-the-art deep learning algorithms
normally utilize complex network architectures for segmentation, which usually
incurs high computational complexity. Specifically, the high-caliber
performance of the convolutional neural network (CNN) heavily relies on
fine-grained spatial details (fine resolution) and sufficient contextual
information (large receptive fields), both of which trigger high computational
costs. This crucially impedes their practicability and availability in
real-world scenarios that require real-time processing. In this paper, we
propose an Attentive Bilateral Contextual Network (ABCNet), a convolutional
neural network (CNN) with double branches, with prominently lower computational
consumptions compared to the cutting-edge algorithms, while maintaining a
competitive accuracy. Code is available at https://github.com/lironui/ABCNet.
</p>
"
Deep Learning Based Model Identification System Exploits the Modular Structure of a Bio-Inspired Posture Control Model for Humans and Humanoids. (arXiv:2102.02536v1 [cs.LG]),http://arxiv.org/abs/2102.02536,"<p>This work presents a system identification procedure based on Convolutional
Neural Networks (CNN) for human posture control using the DEC (Disturbance
Estimation and Compensation) parametric model. The modular structure of the
proposed control model inspired the design of a modular identification
procedure, in the sense that the same neural network is used to identify the
parameters of the modules controlling different degrees of freedom. In this way
the presented examples of body sway induced by external stimuli provide several
training samples at once
</p>
"
Accurate numerical simulation of electrodiffusion and water movement in brain tissue. (arXiv:2102.02539v1 [math.NA]),http://arxiv.org/abs/2102.02539,"<p>Mathematical modelling of ionic electrodiffusion and water movement is
emerging as a powerful avenue of investigation to provide new physiological
insight into brain homeostasis. However, in order to provide solid answers and
resolve controversies, the accuracy of the predictions is essential. Ionic
electrodiffusion models typically comprise non-trivial systems of non-linear
and highly coupled partial and ordinary differential equations that govern
phenomena on disparate time scales. Here, we study numerical challenges related
to approximating these systems. We consider a homogenized model for
electrodiffusion and osmosis in brain tissue and present and evaluate different
associated finite element-based splitting schemes in terms of their numerical
properties, including accuracy, convergence, and computational efficiency for
both idealized scenarios and for the physiologically relevant setting of
cortical spreading depression (CSD). We find that the schemes display optimal
convergence rates in space for problems with smooth manufactured solutions.
However, the physiological CSD setting is challenging: we find that the
accurate computation of CSD wave characteristics (wave speed and wave width)
requires a very fine spatial and fine temporal resolution.
</p>
"
On Fading Channel Dependency Structures with a Positive Zero-Outage Capacity. (arXiv:2102.02541v1 [cs.IT]),http://arxiv.org/abs/2102.02541,"<p>With emerging technologies like 6G, many new applications like autonomous
systems evolve which have strict demands on the reliability of data
communications. In this work, we consider a system with multiple slowly fading
channels that constitute a diversity system. We show that the joint
distribution of the fading coefficients has a tremendous impact on the outage
performance of a communication system. In particular, we investigate the
zero-outage capacity (ZOC) and characterize the joint fading distributions with
a strictly positive ZOC. Interestingly, the set of joint distributions, that
lead to positive ZOCs, is larger than a singleton in general. We also derive
expressions for the maximum ZOC with respect to all possible joint
distributions for selection combining (SC) as diversity combining technique at
the receiver. For maximum ratio combining (MRC), we characterize the maximum
ZOC within a finite number of bits. Finally, the results are evaluated
explicitly for the special cases of Rayleigh fading and Nakagami-$m$ fading in
order to quantify the ZOCs for common fading models.
</p>
"
The Importance of Models in Data Analysis with Small Human Movement Datasets -- Inspirations from Neurorobotics Applied to Posture Control of Humanoids and Humans. (arXiv:2102.02543v1 [cs.RO]),http://arxiv.org/abs/2102.02543,"<p>This work presents a system identification procedure based on Convolutional
Neural Networks (CNN) for human posture control using the DEC (Disturbance
Estimation and Compensation) parametric model. The modular structure of the
proposed control model inspired the design of a modular identification
procedure, in the sense that the same neural network is used to identify the
parameters of the modules controlling different degrees of freedom. In this way
the presented examples of body sway induced by external stimuli provide several
training samples at once.
</p>
"
"Decoding Reed-Solomon codes by solving a bilinear system with a Gr\""obner basis approach. (arXiv:2102.02544v1 [cs.IT])",http://arxiv.org/abs/2102.02544,"<p>Decoding a Reed-Solomon code can be modeled by a bilinear system which can be
solved by Gr\""obner basis techniques. We will show that in this particular
case, these techniques are much more efficient than for generic bilinear
systems with the same number of unknowns and equations (where these techniques
have exponential complexity). Here we show that they are able to solve the
problem in polynomial time up to the Sudan radius. Moreover, beyond this radius
these techniques recover automatically polynomial identities that are at the
heart of improvements of the power decoding approach for reaching the Johnson
decoding radius. They also allow to derive new polynomial identities that can
be used to derive new algebraic decoding algorithms for Reed-Solomon codes. We
provide numerical evidence that this sometimes allows to correct efficiently
slightly more errors than the Johnson radius.
</p>
"
CHEF: Cross-modal Hierarchical Embeddings for Food Domain Retrieval. (arXiv:2102.02547v1 [cs.CV]),http://arxiv.org/abs/2102.02547,"<p>Despite the abundance of multi-modal data, such as image-text pairs, there
has been little effort in understanding the individual entities and their
different roles in the construction of these data instances. In this work, we
endeavour to discover the entities and their corresponding importance in
cooking recipes automaticall} as a visual-linguistic association problem. More
specifically, we introduce a novel cross-modal learning framework to jointly
model the latent representations of images and text in the food image-recipe
association and retrieval tasks. This model allows one to discover complex
functional and hierarchical relationships between images and text, and among
textual parts of a recipe including title, ingredients and cooking
instructions. Our experiments show that by making use of efficient
tree-structured Long Short-Term Memory as the text encoder in our computational
cross-modal retrieval framework, we are not only able to identify the main
ingredients and cooking actions in the recipe descriptions without explicit
supervision, but we can also learn more meaningful feature representations of
food recipes, appropriate for challenging cross-modal retrieval and recipe
adaption tasks.
</p>
"
Dual-embedding based Neural Collaborative Filtering for Recommender Systems. (arXiv:2102.02549v1 [cs.IR]),http://arxiv.org/abs/2102.02549,"<p>Among various recommender techniques, collaborative filtering (CF) is the
most successful one. And a key problem in CF is how to represent users and
items. Previous works usually represent a user (an item) as a vector of latent
factors (aka. \textit{embedding}) and then model the interactions between users
and items based on the representations. Despite its effectiveness, we argue
that it's insufficient to yield satisfactory embeddings for collaborative
filtering. Inspired by the idea of SVD++ that represents users based on
themselves and their interacted items, we propose a general collaborative
filtering framework named DNCF, short for Dual-embedding based Neural
Collaborative Filtering, to utilize historical interactions to enhance the
representation. In addition to learning the primitive embedding for a user (an
item), we introduce an additional embedding from the perspective of the
interacted items (users) to augment the user (item) representation. Extensive
experiments on four publicly datasets demonstrated the effectiveness of our
proposed DNCF framework by comparing its performance with several traditional
matrix factorization models and other state-of-the-art deep learning based
recommender models.
</p>
"
ML-Doctor: Holistic Risk Assessment of Inference Attacks Against Machine Learning Models. (arXiv:2102.02551v1 [cs.CR]),http://arxiv.org/abs/2102.02551,"<p>Inference attacks against Machine Learning (ML) models allow adversaries to
learn information about training data, model parameters, etc. While researchers
have studied these attacks thoroughly, they have done so in isolation. We lack
a comprehensive picture of the risks caused by the attacks, such as the
different scenarios they can be applied to, the common factors that influence
their performance, the relationship among them, or the effectiveness of defense
techniques. In this paper, we fill this gap by presenting a first-of-its-kind
holistic risk assessment of different inference attacks against machine
learning models. We concentrate on four attacks - namely, membership inference,
model inversion, attribute inference, and model stealing - and establish a
threat model taxonomy. Our extensive experimental evaluation conducted over
five model architectures and four datasets shows that the complexity of the
training dataset plays an important role with respect to the attack's
performance, while the effectiveness of model stealing and membership inference
attacks are negatively correlated. We also show that defenses like DP-SGD and
Knowledge Distillation can only hope to mitigate some of the inference attacks.
Our analysis relies on a modular re-usable software, ML-Doctor, which enables
ML model owners to assess the risks of deploying their models, and equally
serves as a benchmark tool for researchers and practitioners.
</p>
"
Decoding of Space-Symmetric Rank Errors. (arXiv:2102.02554v1 [cs.IT]),http://arxiv.org/abs/2102.02554,"<p>This paper investigates the decoding of certain Gabidulin codes that were
transmitted over a channel with space-symmetric errors. Space-symmetric errors
are additive error matrices that have the property that their column and row
spaces are equal. We show that for channels restricted to space-symmetric
errors, with high probability errors of rank up to 2(n-k)/3 can be decoded with
a Gabidulin code of length n and dimension k, using a weak-self orthogonal
basis as code locators.
</p>
"
Adaptive Semiparametric Language Models. (arXiv:2102.02557v1 [cs.CL]),http://arxiv.org/abs/2102.02557,"<p>We present a language model that combines a large parametric neural network
(i.e., a transformer) with a non-parametric episodic memory component in an
integrated architecture. Our model uses extended short-term context by caching
local hidden states -- similar to transformer-XL -- and global long-term memory
by retrieving a set of nearest neighbor tokens at each timestep. We design a
gating function to adaptively combine multiple information sources to make a
prediction. This mechanism allows the model to use either local context,
short-term memory, or long-term memory (or any combination of them) on an ad
hoc basis depending on the context. Experiments on word-based and
character-based language modeling datasets demonstrate the efficacy of our
proposed method compared to strong baselines.
</p>
"
"Evolutionary Multitask Optimization: a Methodological Overview, Challenges and Future Research Directions. (arXiv:2102.02558v1 [cs.NE])",http://arxiv.org/abs/2102.02558,"<p>In this work we consider multitasking in the context of solving multiple
optimization problems simultaneously by conducting a single search process. The
principal goal when dealing with this scenario is to dynamically exploit the
existing complementarities among the problems (tasks) being optimized, helping
each other through the exchange of valuable knowledge. Additionally, the
emerging paradigm of Evolutionary Multitasking tackles multitask optimization
scenarios by using as inspiration concepts drawn from Evolutionary Computation.
The main purpose of this survey is to collect, organize and critically examine
the abundant literature published so far in Evolutionary Multitasking, with an
emphasis on the methodological patterns followed when designing new algorithmic
proposals in this area (namely, multifactorial optimization and
multipopulation-based multitasking). We complement our critical analysis with
an identification of challenges that remain open to date, along with promising
research directions that can stimulate future efforts in this topic. Our
discussions held throughout this manuscript are offered to the audience as a
reference of the general trajectory followed by the community working in this
field in recent times, as well as a self-contained entry point for newcomers
and researchers interested to join this exciting research avenue.
</p>
"
An efficient linear programming rounding-and-refinement algorithm for large-scale network slicing problem. (arXiv:2102.02563v1 [cs.NI]),http://arxiv.org/abs/2102.02563,"<p>In this paper, we consider the network slicing problem which attempts to map
multiple customized virtual network requests (also called services) to a common
shared network infrastructure and allocate network resources to meet diverse
service requirements, and propose an efficient two-stage algorithm for solving
this NP-hard problem. In the first stage, the proposed algorithm uses an
iterative linear programming (LP) rounding procedure to place the virtual
network functions of all services into cloud nodes while taking traffic routing
of all services into consideration; in the second stage, the proposed algorithm
uses an iterative LP refinement procedure to obtain a solution for traffic
routing of all services with their end-to-end delay constraints being
satisfied. Compared with the existing algorithms which either have an
exponential complexity or return a low-quality solution, our proposed algorithm
achieves a better trade-off between solution quality and computational
complexity. In particular, the worst-case complexity of our proposed algorithm
is polynomial, which makes it suitable for solving large-scale problems.
Numerical results demonstrate the effectiveness and efficiency of our proposed
algorithm.
</p>
"
Incremental Beam Manipulation for Natural Language Generation. (arXiv:2102.02574v1 [cs.CL]),http://arxiv.org/abs/2102.02574,"<p>The performance of natural language generation systems has improved
substantially with modern neural networks. At test time they typically employ
beam search to avoid locally optimal but globally suboptimal predictions.
However, due to model errors, a larger beam size can lead to deteriorating
performance according to the evaluation metric. For this reason, it is common
to rerank the output of beam search, but this relies on beam search to produce
a good set of hypotheses, which limits the potential gains. Other alternatives
to beam search require changes to the training of the model, which restricts
their applicability compared to beam search. This paper proposes incremental
beam manipulation, i.e. reranking the hypotheses in the beam during decoding
instead of only at the end. This way, hypotheses that are unlikely to lead to a
good final output are discarded, and in their place hypotheses that would have
been ignored will be considered instead. Applying incremental beam manipulation
leads to an improvement of 1.93 and 5.82 BLEU points over vanilla beam search
for the test sets of the E2E and WebNLG challenges respectively. The proposed
method also outperformed a strong reranker by 1.04 BLEU points on the E2E
challenge, while being on par with it on the WebNLG dataset.
</p>
"
Exploring Scale-Measures of Data Sets. (arXiv:2102.02576v1 [cs.AI]),http://arxiv.org/abs/2102.02576,"<p>Measurement is a fundamental building block of numerous scientific models and
their creation. This is in particular true for data driven science. Due to the
high complexity and size of modern data sets, the necessity for the development
of understandable and efficient scaling methods is at hand. A profound theory
for scaling data is scale-measures, as developed in the field of formal concept
analysis. Recent developments indicate that the set of all scale-measures for a
given data set constitutes a lattice and does hence allow efficient exploring
algorithms. In this work we study the properties of said lattice and propose a
novel scale-measure exploration algorithm that is based on the well-known and
proven attribute exploration approach. Our results motivate multiple
applications in scale recommendation, most prominently (semi-)automatic
scaling.
</p>
"
Regenerating Soft Robots through Neural Cellular Automata. (arXiv:2102.02579v1 [cs.NE]),http://arxiv.org/abs/2102.02579,"<p>Morphological regeneration is an important feature that highlights the
environmental adaptive capacity of biological systems. Lack of this
regenerative capacity significantly limits the resilience of machines and the
environments they can operate in. To aid in addressing this gap, we develop an
approach for simulated soft robots to regrow parts of their morphology when
being damaged. Although numerical simulations using soft robots have played an
important role in their design, evolving soft robots with regenerative
capabilities have so far received comparable little attention. Here we propose
a model for soft robots that regenerate through a neural cellular automata.
Importantly, this approach only relies on local cell information to regrow
damaged components, opening interesting possibilities for physical regenerable
soft robots in the future. Our approach allows simulated soft robots that are
damaged to partially regenerate their original morphology through local cell
interactions alone and regain some of their ability to locomote. These results
take a step towards equipping artificial systems with regenerative capacities
and could potentially allow for more robust operations in a variety of
situations and environments. The code for the experiments in this paper is
available at: \url{github.com/KazuyaHoribe/RegeneratingSoftRobots}.
</p>
"
Parallelware Tools: An Experimental Evaluation on POWER Systems. (arXiv:2102.02582v1 [cs.DC]),http://arxiv.org/abs/2102.02582,"<p>Static code analysis tools are designed to aid software developers to build
better quality software in less time, by detecting defects early in the
software development life cycle. Even the most experienced developer regularly
introduces coding defects. Identifying, mitigating and resolving defects is an
essential part of the software development process, but frequently defects can
go undetected. One defect can lead to a minor malfunction or cause serious
security and safety issues. This is magnified in the development of the complex
parallel software required to exploit modern heterogeneous multicore hardware.
Thus, there is an urgent need for new static code analysis tools to help in
building better concurrent and parallel software. The paper reports preliminary
results about the use of Appentra's Parallelware technology to address this
problem from the following three perspectives: finding concurrency issues in
the code, discovering new opportunities for parallelization in the code, and
generating parallel-equivalent codes that enable tasks to run faster. The paper
also presents experimental results using well-known scientific codes and POWER
systems.
</p>
"
Optimizing Blockchain Based Smart Grid Auctions: A Green Revolution. (arXiv:2102.02583v1 [cs.CR]),http://arxiv.org/abs/2102.02583,"<p>Integrating blockchain with energy trading is a new paradigm for researchers
working in the field of smart grid. In energy trading, auction theory plays an
important role to ensure truthfulness, rationality, and to balance utility of
participants. However, traditional energy auctions cannot directly be
integrated in blockchain based auctions due to the decentralized nature.
Therefore, researches are being carried out to propose more efficient
decentralized auctions for energy trading. Despite of all these advances, a
greater standpoint that is not well-highlighted or discussed in majority of
proposed mechanisms is the integration of green aspect in these auctions.
Since, blockchain is a novel paradigm to ensure trust but it also comes up with
a curse of high computation and communication complexity which eventually
causes resource scarcity. Therefore, there is a need to develop and encourage
development of more green auctions to carry out decentralised energy trading.
In this paper, we first provide a thorough motivation of decentralized auctions
over traditional auctions. Afterwards, we provide in-depth design requirements
that can be taken into consideration while developing such auctions. After
that, we analyse technical works that have developed blockchain based energy
auctions from green perspective. Finally, we summarize the article by providing
challenges and possible future research directions of blockchain based energy
auction from green viewpoint.
</p>
"
Human Values in Software Release Planning. (arXiv:2102.02584v1 [cs.SE]),http://arxiv.org/abs/2102.02584,"<p>Software products have become an integral part of human lives, and therefore
need to account for human values such as privacy, fairness, and equality.
Ignoring human values in software development leads to biases and violations of
human values: racial biases in recidivism assessment and facial recognition
software are well-known examples of such issues. One of the most critical steps
in software development is Software Release Planning (SRP), where decisions are
made about the presence or absence of the requirements (features) in the
software. Such decisions are primarily guided by the economic value of the
requirements, ignoring their impacts on a broader range of human values. That
may result in ignoring (selecting) requirements that positively (negatively)
impact human values, increasing the risk of value breaches in the software. To
address this, we have proposed an Integer Programming approach to considering
human values in software release planning. In this regard, an Integer Linear
Programming (ILP) model has been proposed, that explicitly accounts for human
values in finding an ""optimal"" subset of the requirements. The ILP model
exploits the algebraic structure of fuzzy graphs to capture dependencies and
conflicts among the values of the requirements.
</p>
"
One Size Does Not Fit All: Finding the Optimal N-gram Sizes for FastText Models across Languages. (arXiv:2102.02585v1 [cs.CL]),http://arxiv.org/abs/2102.02585,"<p>Unsupervised word representation learning from large corpora is badly needed
for downstream tasks such as text classification, information retrieval, and
machine translation. The representation precision of the fastText language
models is mostly due to their use of subword information. In previous work, the
optimization of fastText subword sizes has been largely neglected, and
non-English fastText language models were trained using subword sizes optimized
for English and German.
</p>
<p>In our work, we train English, German, Czech, and Italian fastText language
models on Wikipedia, and we optimize the subword sizes on the English, German,
Czech, and Italian word analogy tasks. We show that the optimization of subword
sizes results in a 5% improvement on the Czech word analogy task. We also show
that computationally expensive hyperparameter optimization can be replaced with
cheap $n$-gram frequency analysis: subword sizes that are the closest to
covering 3.76% of all unique subwords in a language are shown to be the optimal
fastText hyperparameters on the English, German, Czech, and Italian word
analogy tasks.
</p>
"
Temporal Cascade and Structural Modelling of EHRs for Granular Readmission Prediction. (arXiv:2102.02586v1 [cs.LG]),http://arxiv.org/abs/2102.02586,"<p>Predicting (1) when the next hospital admission occurs and (2) what will
happen in the next admission about a patient by mining electronic health record
(EHR) data can provide granular readmission predictions to assist clinical
decision making. Recurrent neural network (RNN) and point process models are
usually employed in modelling temporal sequential data. Simple RNN models
assume that sequences of hospital visits follow strict causal dependencies
between consecutive visits. However, in the real-world, a patient may have
multiple co-existing chronic medical conditions, i.e., multimorbidity, which
results in a cascade of visits where a non-immediate historical visit can be
most influential to the next visit. Although a point process (e.g., Hawkes
process) is able to model a cascade temporal relationship, it strongly relies
on a prior generative process assumption. We propose a novel model, MEDCAS, to
address these challenges. MEDCAS combines the strengths of RNN-based models and
point processes by integrating point processes in modelling visit types and
time gaps into an attention-based sequence-to-sequence learning model, which is
able to capture the temporal cascade relationships. To supplement the patients
with short visit sequences, a structural modelling technique with graph-based
methods is used to construct the markers of the point process in MEDCAS.
Extensive experiments on three real-world EHR datasets have been performed and
the results demonstrate that \texttt{MEDCAS} outperforms state-of-the-art
models in both tasks.
</p>
"
Lookup subnet based Spatial Graph Convolutional neural Network. (arXiv:2102.02588v1 [cs.LG]),http://arxiv.org/abs/2102.02588,"<p>Convolutional Neural Networks(CNNs) has achieved remarkable performance
breakthrough in Euclidean structure data. Recently, aggregation-transformation
based Graph Neural networks(GNNs) gradually produce a powerful performance on
non-Euclidean data. In this paper, we propose a cross-correlation based graph
convolution method allowing to naturally generalize CNNs to non-Euclidean
domains and inherit the excellent natures of CNNs, such as local filters,
parameter sharing, flexible receptive field, etc. Meanwhile, it leverages
dynamically generated convolution kernel and cross-correlation operators to
address the shortcomings of prior methods based on aggregation-transformation
or their approximations. Our method has achieved or matched popular
state-of-the-art results across three established graph benchmarks: the Cora,
Citeseer, and Pubmed citation network datasets.
</p>
"
Mean-field control variate methods for kinetic equations with uncertainties and applications to socio-economic sciences. (arXiv:2102.02589v1 [math.NA]),http://arxiv.org/abs/2102.02589,"<p>In this paper, we extend a recently introduced multi-fidelity control variate
for the uncertainty quantification of the Boltzmann equation to the case of
kinetic models arising in the study of multiagent systems. For these phenomena,
where the effect of uncertainties is particularly evident, several models have
been developed whose equilibrium states are typically unknown. In particular,
we aim to develop efficient numerical methods based on solving the kinetic
equations in the phase space by Direct Simulation Monte Carlo (DSMC) coupled to
a Monte Carlo sampling in the random space. To this end, exploiting the
knowledge of the corresponding mean-field approximation we develop novel
mean-field Control Variate (MFCV) methods that are able to strongly reduce the
variance of the standard Monte Carlo sampling method in the random space. We
verify these observations with several numerical examples based on classical
models , including wealth exchanges and opinion formation model for collective
phenomena.
</p>
"
A Faster Algorithm for Finding Closest Pairs in Hamming Metric. (arXiv:2102.02597v1 [cs.DS]),http://arxiv.org/abs/2102.02597,"<p>We study the Closest Pair Problem in Hamming metric, which asks to find the
pair with the smallest Hamming distance in a collection of binary vectors. We
give a new randomized algorithm for the problem on uniformly random input
outperforming previous approaches whenever the dimension of input points is
small compared to the dataset size. For moderate to large dimensions, our
algorithm matches the time complexity of the previously best-known locality
sensitive hashing based algorithms. Technically our algorithm follows similar
design principles as Dubiner (IEEE Trans. Inf. Theory 2010) and May-Ozerov
(Eurocrypt 2015). Besides improving the time complexity in the aforementioned
areas, we significantly simplify the analysis of these previous works. We give
a modular analysis, which allows us to investigate the performance of the
algorithm also on non-uniform input distributions. Furthermore, we give a proof
of concept implementation of our algorithm which performs well in comparison to
a quadratic search baseline. This is the first step towards answering an open
question raised by May and Ozerov regarding the practicability of algorithms
following these design principles.
</p>
"
VSEGAN: Visual Speech Enhancement Generative Adversarial Network. (arXiv:2102.02599v1 [eess.AS]),http://arxiv.org/abs/2102.02599,"<p>Speech enhancement is an essential task of improving speech quality in noise
scenario. Several state-of-the-art approaches have introduced visual
information for speech enhancement,since the visual aspect of speech is
essentially unaffected by acoustic environment. This paper proposes a novel
frameworkthat involves visual information for speech enhancement, by
in-corporating a Generative Adversarial Network (GAN). In par-ticular, the
proposed visual speech enhancement GAN consistof two networks trained in
adversarial manner, i) a generator that adopts multi-layer feature fusion
convolution network to enhance input noisy speech, and ii) a discriminator that
attemptsto minimize the discrepancy between the distributions of the clean
speech signal and enhanced speech signal. Experiment re-sults demonstrated
superior performance of the proposed modelagainst several state-of-the-art
</p>
"
A formalization of Dedekind domains and class groups of global fields. (arXiv:2102.02600v1 [cs.LO]),http://arxiv.org/abs/2102.02600,"<p>Dedekind domains and their class groups are notions in commutative algebra
that are essential in algebraic number theory. We formalized these structures
and several fundamental properties, including number theoretic finiteness
results for class groups, in the Lean prover as part of the mathlib
mathematical library. This paper describes the formalization process, noting
the idioms we found useful in our development and mathlib's decentralized
collaboration processes involved in this project.
</p>
"
An Empirical Analysis of Implementing Enterprise Blockchain Protocols in Supply Chain Anti-Counterfeiting and Traceability. (arXiv:2102.02601v1 [cs.CR]),http://arxiv.org/abs/2102.02601,"<p>A variety of innovative software solutions, addressing product
anti-counterfeiting and record provenance of the wider supply chain industry,
have been implemented. However, these solutions have been developed with
centralized system architecture which could be susceptible to malicious
modifications on states of product records and various potential security
attacks leading to system failure and downtime. Blockchain technology has been
enabling decentralized trust with a network of distributed peer nodes to
maintain consistent shared states via a decentralized consensus reached, with
which an idea of developing decentralized and reliable solutions has been
basing on. A Decentralized NFC-Enabled Anti-Counterfeiting System (dNAS) was
therefore proposed and developed, decentralizing a legacy anti-counterfeiting
system of supply chain industry utilizing enterprise blockchain protocols and
enterprise consortium, to facilitate trustworthy data provenance retrieval,
verification and management, as well as strengthening capability of product
anti-counterfeiting and traceability in supply chain industry. The adoption of
enterprise blockchain protocols and implementations has been surging in supply
chain industry given its advantages in scalability, governance and
compatibility with existing supply chain systems and networks, but development
and adoption of decentralized solutions could also impose additional
implications to supply chain integrity, in terms of security, privacy and
confidentiality. In this research, an empirical analysis performed against
decentralized solutions, including dNAS, summarizes the effectiveness,
limitations and future opportunities of developing decentralized solutions
built around existing enterprise blockchain protocols and implementations for
supply chain anti-counterfeiting and traceability.
</p>
"
A Learning-based Stochastic Driving Model for Autonomous Vehicle Testing. (arXiv:2102.02602v1 [eess.SY]),http://arxiv.org/abs/2102.02602,"<p>In the simulation-based testing and evaluation of autonomous vehicles (AVs),
how background vehicles (BVs) drive directly influences the AV's driving
behavior and further impacts the testing result. Existing simulation platforms
use either pre-determined trajectories or deterministic driving models to model
the BVs' behaviors. However, pre-determined BV trajectories can not react to
the AV's maneuvers, and deterministic models are different from real human
drivers due to the lack of stochastic components and errors. Both methods lead
to unrealistic traffic scenarios. This paper presents a learning-based
stochastic driving model that meets the unique needs of AV testing, i.e.
interactive and human-like. The model is built based on the
long-short-term-memory (LSTM) architecture. By incorporating the concept of
quantile-regression to the loss function of the model, the stochastic behaviors
are reproduced without any prior assumption of human drivers. The model is
trained with the large-scale naturalistic driving data (NDD) from the Safety
Pilot Model Deployment(SPMD) project and then compared with a stochastic
intelligent driving model (IDM). Analysis of individual trajectories shows that
the proposed model can reproduce more similar trajectories to human drivers
than IDM. To validate the ability of the proposed model in generating a
naturalistic driving environment, traffic simulation experiments are
implemented. The results show that the traffic flow parameters such as speed,
range, and headway distribution match closely with the NDD, which is of
significant importance for AV testing and evaluation.
</p>
"
Linear complexity of some sequences derived from hyperelliptic curves of genus 2. (arXiv:2102.02605v1 [math.NT]),http://arxiv.org/abs/2102.02605,"<p>For a given hyperelliptic curve $C$ over a finite field with Jacobian $J_C$,
we consider the hyperelliptic analogue of the congruential generator defined by
$W_n=W_{n-1}+D$ for $n\geq 1$ and $D,W_0\in J_C$. We show that curves of genus
2 produce sequences with large linear complexity.
</p>
"
Minimizing the alphabet size in codes with restricted error sets. (arXiv:2102.02608v1 [cs.IT]),http://arxiv.org/abs/2102.02608,"<p>This paper focuses on error-correcting codes that can handle a predefined set
of specific error patterns. The need for such codes arises in many settings of
practical interest, including wireless communication and flash memory systems.
In many such settings, a smaller field size is achievable than that offered by
MDS and other standard codes. We establish a connection between the minimum
alphabet size for this generalized setting and the combinatorial properties of
a hypergraph that represents the prespecified collection of error patterns. We
also show a connection between error and erasure correcting codes in this
specialized setting. This allows us to establish bounds on the minimum alphabet
size and show an advantage of non-linear codes over linear codes in a
generalized setting. We also consider a variation of the problem which allows a
small probability of decoding error and relate it to an approximate version of
hypergraph coloring.
</p>
"
Barrier Function-based Collaborative Control of Multiple Robots under Signal Temporal Logic Tasks. (arXiv:2102.02609v1 [eess.SY]),http://arxiv.org/abs/2102.02609,"<p>Motivated by the recent interest in cyber-physical and autonomous robotic
systems, we study the problem of dynamically coupled multi-agent systems under
a set of signal temporal logic tasks. In particular, the satisfaction of each
of these signal temporal logic tasks depends on the behavior of a distinct set
of agents. Instead of abstracting the agent dynamics and the temporal logic
tasks into a discrete domain and solving the problem therein or using
optimization-based methods, we derive collaborative feedback control laws.
These control laws are based on a decentralized control barrier function
condition that results in discontinuous control laws, as opposed to a
centralized condition resembling the single-agent case. The benefits of our
approach are inherent robustness properties typically present in feedback
control as well as satisfaction guarantees for continuous-time multi-agent
systems. More specifically, time-varying control barrier functions are used
that account for the semantics of the signal temporal logic tasks at hand. For
a certain fragment of signal temporal logic tasks, we further propose a
systematic way to construct such control barrier functions. Finally, we show
the efficacy and robustness of our framework in an experiment including a group
of three omnidirectional robots.
</p>
"
Strategyproof Facility Location Mechanisms on Discrete Trees. (arXiv:2102.02610v1 [cs.GT]),http://arxiv.org/abs/2102.02610,"<p>We address the problem of strategyproof (SP) facility location mechanisms on
discrete trees. Our main result is a full characterization of onto and SP
mechanisms. In particular, we prove that when a single agent significantly
affects the outcome, the trajectory of the facility is almost contained in the
trajectory of the agent, and both move in the same direction along the common
edges. We show tight relations of our characterization to previous results on
discrete lines and on continuous trees. We then derive further implications of
the main result for infinite discrete lines.
</p>
"
CKConv: Continuous Kernel Convolution For Sequential Data. (arXiv:2102.02611v1 [cs.LG]),http://arxiv.org/abs/2102.02611,"<p>Conventional neural architectures for sequential data present important
limitations. Recurrent networks suffer from exploding and vanishing gradients,
small effective memory horizons, and must be trained sequentially.
Convolutional networks are unable to handle sequences of unknown size and their
memory horizon must be defined a priori. In this work, we show that all these
problems can be solved by formulating convolutional kernels in CNNs as
continuous functions. The resulting Continuous Kernel Convolution (CKConv)
allows us to model arbitrarily long sequences in a parallel manner, within a
single operation, and without relying on any form of recurrence. We show that
Continuous Kernel Convolutional Networks (CKCNNs) obtain state-of-the-art
results in multiple datasets, e.g., permuted MNIST, and, thanks to their
continuous nature, are able to handle non-uniformly sampled datasets and
irregularly-sampled data natively. CKCNNs match or perform better than neural
ODEs designed for these purposes in a much faster and simpler manner.
</p>
"
Parabolic optimal control with strongly monotone quasilinearity and its time discretization. (arXiv:2102.02616v1 [math.OC]),http://arxiv.org/abs/2102.02616,"<p>In this paper we discuss the optimal control of a quasilinear parabolic state
equation. Its form is leaned on the kind of problems arising for example when
controlling the anisotropic Allen-Cahn equation as a model for crystal growth.
Motivated by this application we consider the state equation as a result of a
gradient flow of an energy functional. The quasilinear term is strongly
monotone and obeys a certain growth condition. The state equation is
discretized implicitly in time with piecewise constant functions. The existence
of the control-to-state operator and its Lipschitz-continuity is shown for the
time discretized as well as for the time continuous problem. Latter is based on
the convergence proof of the discretized solutions. Finally we present for both
the existence of global minimizers. When the target function is given over the
whole time horizon also convergence of a subsequence of time discrete optimal
controls to a global minimizer of the time continuous problem can be shown. Our
results hold in arbitrary space dimensions.
</p>
"
A Deep Collocation Method for the Bending Analysis of Kirchhoff Plate. (arXiv:2102.02617v1 [math.NA]),http://arxiv.org/abs/2102.02617,"<p>In this paper, a deep collocation method (DCM) for thin plate bending
problems is proposed. This method takes advantage of computational graphs and
backpropagation algorithms involved in deep learning. Besides, the proposed DCM
is based on a feedforward deep neural network (DNN) and differs from most
previous applications of deep learning for mechanical problems. First, batches
of randomly distributed collocation points are initially generated inside the
domain and along the boundaries. A loss function is built with the aim that the
governing partial differential equations (PDEs) of Kirchhoff plate bending
problems, and the boundary/initial conditions are minimised at those
collocation points. A combination of optimizers is adopted in the
backpropagation process to minimize the loss function so as to obtain the
optimal hyperparameters. In Kirchhoff plate bending problems, the C1 continuity
requirement poses significant difficulties in traditional mesh-based methods.
This can be solved by the proposed DCM, which uses a deep neural network to
approximate the continuous transversal deflection, and is proved to be suitable
to the bending analysis of Kirchhoff plate of various geometries.
</p>
"
Optimised one-class classification performance. (arXiv:2102.02618v1 [cs.LG]),http://arxiv.org/abs/2102.02618,"<p>We provide a thorough treatment of hyperparameter optimisation for three data
descriptors with a good track-record in the literature: Support Vector Machine
(SVM), Nearest Neighbour Distance (NND) and Average Localised Proximity (ALP).
The hyperparameters of SVM have to be optimised through cross-validation, while
NND and ALP allow the reuse of a single nearest-neighbour query and an
efficient form of leave-one-out validation. We experimentally evaluate the
effect of hyperparameter optimisation with 246 classification problems drawn
from 50 datasets. From a selection of optimisation algorithms, the recent
Malherbe-Powell proposal optimises the hyperparameters of all three data
descriptors most efficiently. We calculate the increase in test AUROC and the
amount of overfitting as a function of the number of hyperparameter
evaluations. After 50 evaluations, ALP and SVM both significantly outperform
NND. The performance of ALP and SVM is comparable, but ALP can be optimised
more efficiently, while a choice between ALP and SVM based on validation AUROC
gives the best overall result. This distils the many variables of one-class
classification with hyperparameter optimisation down to a clear choice with a
known trade-off, allowing practitioners to make informed decisions.
</p>
"
Electricity-gas integrated energy system optimal operation in typical scenario of coal district considering hydrogen heavy trucks. (arXiv:2102.02620v1 [eess.SY]),http://arxiv.org/abs/2102.02620,"<p>The coal industry contributes significantly to the social economy, but the
emission of greenhouse gases puts huge pressure on the environment in the
process of mining, transportation, and power generation. In the integrated
energy system (IES), the current research about the power-to-gas (P2G)
technology mainly focuses on the injection of hydrogen generated from renewable
energy electrolyzed water into natural gas pipelines, which may cause hydrogen
embrittlement of the pipeline and cannot be repaired. In this paper,
considering the scenario of coal districts is rich in coal and renewable
energy, and sufficient hydrogen energy can be produced through P2G technology
and coal-to-hydrogen (C2H) of coal gasification. In order to transport the
mined coal to the destination, hydrogen heavy trucks have a broad space for
development, which can absorb hydrogen energy in time and avoid potentially
dangerous hydrogen injection into pipelines and relatively expensive hydrogen
storage. An optimized scheduling model of electric-gas IES is proposed based on
second-order cone programming (SOCP). In the model proposed above, the closed
industrial loop between coal mining, hydrogen production, truck transportation
of coal, and integrated energy systems has been innovatively studied, to
consume renewable energy and coordinate multi-energy. Finally, an electric-gas
IES study case constructed by IEEE 30-node power system and Belgium 24-node
natural gas network was used to analyze and verify the economy, low carbon, and
effectiveness of the proposed mechanism.
</p>
"
Cryptocurrency Solutions to Enable Micro-payments in Consumer IoT. (arXiv:2102.02623v1 [cs.CR]),http://arxiv.org/abs/2102.02623,"<p>The successful amalgamation of cryptocurrency and consumer Internet of Things
(IoT) devices can pave the way for novel applications in machine-to-machine
economy. However, the lack of scalability and heavy resource requirements of
initial blockchain designs hinders the integration as they prioritized
decentralization and security. Numerous solutions have been proposed since the
emergence of Bitcoin to achieve this goal. However, none of them seem to
dominate and thus it is unclear how consumer devices will be adopting these
approaches. Therefore, in this paper, we critically review the existing
integration approaches and cryptocurrency designs that strive to enable
micro-payments among consumer devices. We identify and discuss solutions under
three main categories; direct integration, payment channel network and new
cryptocurrency design. The first approach utilizes a full node to interact with
the payment system. Offline channel payment is suggested as a second layer
solution to solve the scalability issue and enable instant payment with low
fee. New designs converge to semi-centralized scheme and focuson lightweight
consensus protocol that does not require highcomputation power which might mean
loosening the initial designchoices in favor of scalability. We evaluate the
pros and cons ofeach of these approaches and then point out future
researchchallenges. Our goal is to help researchers and practitioners tobetter
focus their efforts to facilitate micro-payment adoptions.
</p>
"
"The #ETH is False, #k-SAT is in Sub-Exponential Time. (arXiv:2102.02624v1 [cs.CC])",http://arxiv.org/abs/2102.02624,"<p>We orchestrate a randomized algorithm for #$k$-SAT which counts the exact
number of satisfying assignments in $2^{o(n)}$ time. The existence of such
algorithm signifies that the #ETH is hereby refuted, and so are $\oplus$ETH,
ETH, #SETH, $\oplus$SETH and SETH.
</p>
"
Safety Case Templates for Autonomous Systems. (arXiv:2102.02625v1 [cs.SE]),http://arxiv.org/abs/2102.02625,"<p>This report documents safety assurance argument templates to support the
deployment and operation of autonomous systems that include machine learning
(ML) components. The document presents example safety argument templates
covering: the development of safety requirements, hazard analysis, a safety
monitor architecture for an autonomous system including at least one ML
element, a component with ML and the adaptation and change of the system over
time. The report also presents generic templates for argument defeaters and
evidence confidence that can be used to strengthen, review, and adapt the
templates as necessary. This Interim Report is made available to get feedback
on the approach and on the templates. This work is being sponsored by the UK
Dstl under the R-cloud framework.
</p>
"
Formalising a Turing-Complete Choreographic Language in Coq. (arXiv:2102.02627v1 [cs.LO]),http://arxiv.org/abs/2102.02627,"<p>Theory of choreographic languages typically includes a number of complex
results that are proved by structural induction. The high number of cases and
the subtle details in some of them lead to long reviewing processes, and
occasionally to errors being found in published proofs. In this work, we take a
published proof of Turing completeness of a choreographic language and
formalise it in Coq. Our development includes formalising the choreographic
language and its basic properties, Kleene's theory of partial recursive
functions, the encoding of these functions as choreographies, and proving this
encoding correct.
</p>
<p>With this effort, we show that theorem proving can be a very useful tool in
the field of choreographic languages: besides the added degree of confidence
that we get from a mechanised proof, the formalisation process led us to a
significant simplification of the underlying theory. Our results offer a
foundation for the future formal development of choreographic languages.
</p>
"
Learning Monocular Depth in Dynamic Scenes via Instance-Aware Projection Consistency. (arXiv:2102.02629v1 [cs.CV]),http://arxiv.org/abs/2102.02629,"<p>We present an end-to-end joint training framework that explicitly models
6-DoF motion of multiple dynamic objects, ego-motion and depth in a monocular
camera setup without supervision. Our technical contributions are three-fold.
First, we highlight the fundamental difference between inverse and forward
projection while modeling the individual motion of each rigid object, and
propose a geometrically correct projection pipeline using a neural forward
projection module. Second, we design a unified instance-aware photometric and
geometric consistency loss that holistically imposes self-supervisory signals
for every background and object region. Lastly, we introduce a general-purpose
auto-annotation scheme using any off-the-shelf instance segmentation and
optical flow models to produce video instance segmentation maps that will be
utilized as input to our training pipeline. These proposed elements are
validated in a detailed ablation study. Through extensive experiments conducted
on the KITTI and Cityscapes dataset, our framework is shown to outperform the
state-of-the-art depth and motion estimation methods. Our code, dataset, and
models are available at https://github.com/SeokjuLee/Insta-DM .
</p>
"
Universal Approximation Theorems of Fully Connected Binarized Neural Networks. (arXiv:2102.02631v1 [cs.LG]),http://arxiv.org/abs/2102.02631,"<p>Neural networks (NNs) are known for their high predictive accuracy in complex
learning problems. Beside practical advantages, NNs also indicate favourable
theoretical properties such as universal approximation (UA) theorems. Binarized
Neural Networks (BNNs) significantly reduce time and memory demands by
restricting the weight and activation domains to two values. Despite the
practical advantages, theoretical guarantees based on UA theorems of BNNs are
rather sparse in the literature. We close this gap by providing UA theorems for
fully connected BNNs under the following scenarios: (1) for binarized inputs,
UA can be constructively achieved under one hidden layer; (2) for inputs with
real numbers, UA can not be achieved under one hidden layer but can be
constructively achieved under two hidden layers for Lipschitz-continuous
functions. Our results indicate that fully connected BNNs can approximate
functions universally, under certain conditions.
</p>
"
Optimal Trajectories of a UAV Base Station Using Hamilton-Jacobi Equations. (arXiv:2102.02632v1 [math.OC]),http://arxiv.org/abs/2102.02632,"<p>We consider the problem of optimizing the trajectory of an Unmanned Aerial
Vehicle (UAV). Assuming a traffic intensity map of users to be served, the UAV
must travel from a given initial location to a final position within a given
duration and serves the traffic on its way. The problem consists in finding the
optimal trajectory that minimizes a certain cost depending on the velocity and
on the amount of served traffic. We formulate the problem using the framework
of Lagrangian mechanics. We derive closed-form formulas for the optimal
trajectory when the traffic intensity is quadratic (single-phase) using
Hamilton-Jacobi equations. When the traffic intensity is bi-phase, i.e. made of
two quadratics, we provide necessary conditions of optimality that allow us to
propose a gradient-based algorithm and a new algorithm based on the linear
control properties of the quadratic model. These two solutions are of very low
complexity because they rely on fast convergence numerical schemes and closed
form formulas. These two approaches return a trajectory satisfying the
necessary conditions of optimality. At last, we propose a data processing
procedure based on a modified K-means algorithm to derive a bi-phase model and
an optimal trajectory simulation from real traffic data.
</p>
"
Deep Autoencoder-based Fuzzy C-Means for Topic Detection. (arXiv:2102.02636v1 [cs.IR]),http://arxiv.org/abs/2102.02636,"<p>Topic detection is a process for determining topics from a collection of
textual data. One of the topic detection methods is a clustering-based method,
which assumes that the centroids are topics. The clustering method has the
advantage that it can process data with negative representations. Therefore,
the clustering method allows a combination with a broader representation
learning method. In this paper, we adopt deep learning for topic detection by
using a deep autoencoder and fuzzy c-means called deep autoencoder-based fuzzy
c-means (DFCM). The encoder of the autoencoder performs a lower-dimensional
representation learning. Fuzzy c-means groups the lower-dimensional
representation to identify the centroids. The autoencoder's decoder transforms
back the centroids into the original representation to be interpreted as the
topics. Our simulation shows that DFCM improves the coherence score of
eigenspace-based fuzzy c-means (EFCM) and is comparable to the leading standard
methods, i.e., nonnegative matrix factorization (NMF) or latent Dirichlet
allocation (LDA).
</p>
"
Big Data Analytics Applying the Fusion Approach of Multicriteria Decision Making with Deep Learning Algorithms. (arXiv:2102.02637v1 [cs.LG]),http://arxiv.org/abs/2102.02637,"<p>Data is evolving with the rapid progress of population and communication for
various types of devices such as networks, cloud computing, Internet of Things
(IoT), actuators, and sensors. The increment of data and communication content
goes with the equivalence of velocity, speed, size, and value to provide the
useful and meaningful knowledge that helps to solve the future challenging
tasks and latest issues. Besides, multicriteria based decision making is one of
the key issues to solve for various issues related to the alternative effects
in big data analysis. It tends to find a solution based on the latest machine
learning techniques that include algorithms like decision making and deep
learning mechanism based on multicriteria in providing insights to big data. On
the other hand, the derivations are made for it to go with the approximations
to increase the duality of runtime and improve the entire system's potentiality
and efficacy. In essence, several fields, including business, agriculture,
information technology, and computer science, use deep learning and
multicriteria-based decision-making problems. This paper aims to provide
various applications that involve the concepts of deep learning techniques and
exploiting the multicriteria approaches for issues that are facing in big data
analytics by proposing new studies with the fusion approaches of data-driven
techniques.
</p>
"
Autodidactic Neurosurgeon: Collaborative Deep Inference for Mobile Edge Intelligence via Online Learning. (arXiv:2102.02638v1 [cs.LG]),http://arxiv.org/abs/2102.02638,"<p>Recent breakthroughs in deep learning (DL) have led to the emergence of many
intelligent mobile applications and services, but in the meanwhile also pose
unprecedented computing challenges on resource-constrained mobile devices. This
paper builds a collaborative deep inference system between a
resource-constrained mobile device and a powerful edge server, aiming at
joining the power of both on-device processing and computation offloading. The
basic idea of this system is to partition a deep neural network (DNN) into a
front-end part running on the mobile device and a back-end part running on the
edge server, with the key challenge being how to locate the optimal partition
point to minimize the end-to-end inference delay. Unlike existing efforts on
DNN partitioning that rely heavily on a dedicated offline profiling stage to
search for the optimal partition point, our system has a built-in online
learning module, called Autodidactic Neurosurgeon (ANS), to automatically learn
the optimal partition point on-the-fly. Therefore, ANS is able to closely
follow the changes of the system environment by generating new knowledge for
adaptive decision making. The core of ANS is a novel contextual bandit learning
algorithm, called $\mu$LinUCB, which not only has provable theoretical learning
performance guarantee but also is ultra-lightweight for easy real-world
implementation. We implement our system on a video stream object detection
testbed to validate the design of ANS and evaluate its performance. The
experiments show that ANS significantly outperforms state-of-the-art benchmarks
in terms of tracking system changes and reducing the end-to-end inference
delay.
</p>
"
Improving Reinforcement Learning with Human Assistance: An Argument for Human Subject Studies with HIPPO Gym. (arXiv:2102.02639v1 [cs.LG]),http://arxiv.org/abs/2102.02639,"<p>Reinforcement learning (RL) is a popular machine learning paradigm for game
playing, robotics control, and other sequential decision tasks. However, RL
agents often have long learning times with high data requirements because they
begin by acting randomly. In order to better learn in complex tasks, this
article argues that an external teacher can often significantly help the RL
agent learn.
</p>
<p>OpenAI Gym is a common framework for RL research, including a large number of
standard environments and agents, making RL research significantly more
accessible. This article introduces our new open-source RL framework, the Human
Input Parsing Platform for Openai Gym (HIPPO Gym), and the design decisions
that went into its creation. The goal of this platform is to facilitate
human-RL research, again lowering the bar so that more researchers can quickly
investigate different ways that human teachers could assist RL agents,
including learning from demonstrations, learning from feedback, or curriculum
learning.
</p>
"
Low Bit-Rate Wideband Speech Coding: A Deep Generative Model based Approach. (arXiv:2102.02640v1 [cs.SD]),http://arxiv.org/abs/2102.02640,"<p>Traditional low bit-rate speech coding approach only handles narrowband
speech at 8kHz, which limits further improvements in speech quality. Motivated
by recent successful exploration of deep learning methods for image and speech
compression, this paper presents a new approach through vector quantization
(VQ) of mel-frequency cepstral coefficients (MFCCs) and using a deep generative
model called WaveGlow to provide efficient and high-quality speech coding. The
coding feature is sorely an 80-dimension MFCCs vector for 16kHz wideband
speech, then speech coding at the bit-rate throughout 1000-2000 bit/s could be
scalably implemented by applying different VQ schemes for MFCCs vector. This
new deep generative network based codec works fast as the WaveGlow model
abandons the sample-by-sample autoregressive mechanism. We evaluated this new
approach over the multi-speaker TIMIT corpus, and experimental results
demonstrate that it provides better speech quality compared with the
state-of-the-art classic MELPe codec at lower bit-rate.
</p>
"
Asymptotically Exact and Fast Gaussian Copula Models for Imputation of Mixed Data Types. (arXiv:2102.02642v1 [stat.ML]),http://arxiv.org/abs/2102.02642,"<p>Missing values with mixed data types is a common problem in a large number of
machine learning applications such as processing of surveys and in different
medical applications. Recently, Gaussian copula models have been suggested as a
means of performing imputation of missing values using a probabilistic
framework. While the present Gaussian copula models have shown to yield state
of the art performance, they have two limitations: they are based on an
approximation that is fast but may be imprecise and they do not support
unordered multinomial variables. We address the first limitation using direct
and arbitrarily precise approximations both for model estimation and imputation
by using randomized quasi-Monte Carlo procedures. The method we provide has
lower errors for the estimated model parameters and the imputed values,
compared to previously proposed methods. We also extend the previous Gaussian
copula models to include unordered multinomial variables in addition to the
present support of ordinal, binary, and continuous variables.
</p>
"
Pick the Right Edge Device: Towards Power and Performance Estimation of CUDA-based CNNs on GPGPUs. (arXiv:2102.02645v1 [cs.LG]),http://arxiv.org/abs/2102.02645,"<p>The emergence of Machine Learning (ML) as a powerful technique has been
helping nearly all fields of business to increase operational efficiency or to
develop new value propositions. Besides the challenges of deploying and
maintaining ML models, picking the right edge device (e.g., GPGPUs) to run
these models (e.g., CNN with the massive computational process) is one of the
most pressing challenges faced by organizations today. As the cost of renting
(on Cloud) or purchasing an edge device is directly connected to the cost of
final products or services, choosing the most efficient device is essential.
However, this decision making requires deep knowledge about performance and
power consumption of the ML models running on edge devices that must be
identified at the early stage of ML workflow.
</p>
<p>In this paper, we present a novel ML-based approach that provides ML
engineers with the early estimation of both power consumption and performance
of CUDA-based CNNs on GPGPUs. The proposed approach empowers ML engineers to
pick the most efficient GPGPU for a given CNN model at the early stage of
development.
</p>
"
Towards a reinforcement learning de novo genome assembler. (arXiv:2102.02649v1 [q-bio.GN]),http://arxiv.org/abs/2102.02649,"<p>The use of reinforcement learning has proven to be very promising for solving
complex activities without human supervision during their learning process.
However, their successful applications are predominantly focused on fictional
and entertainment problems - such as games. Based on the above, this work aims
to shed light on the application of reinforcement learning to solve this
relevant real-world problem, the genome assembly. By expanding the only
approach found in the literature that addresses this problem, we carefully
explored the aspects of intelligent agent learning, performed by the Q-learning
algorithm, to understand its suitability to be applied in scenarios whose
characteristics are more similar to those faced by real genome projects. The
improvements proposed here include changing the previously proposed reward
system and including state space exploration optimization strategies based on
dynamic pruning and mutual collaboration with evolutionary computing. These
investigations were tried on 23 new environments with larger inputs than those
used previously. All these environments are freely available on the internet
for the evolution of this research by the scientific community. The results
suggest consistent performance progress using the proposed improvements,
however, they also demonstrate the limitations of them, especially related to
the high dimensionality of state and action spaces. We also present, later, the
paths that can be traced to tackle genome assembly efficiently in real
scenarios considering recent, successfully reinforcement learning applications
- including deep reinforcement learning - from other domains dealing with
high-dimensional inputs.
</p>
"
Triadic Exploration and Exploration with Multiple Experts. (arXiv:2102.02654v1 [cs.AI]),http://arxiv.org/abs/2102.02654,"<p>Formal Concept Analysis (FCA) provides a method called attribute exploration
which helps a domain expert discover structural dependencies in knowledge
domains that can be represented by a formal context (a cross table of objects
and attributes). Triadic Concept Analysis is an extension of FCA that
incorporates the notion of conditions. Many extensions and variants of
attribute exploration have been studied but only few attempts at incorporating
multiple experts have been made. In this paper we present triadic exploration
based on Triadic Concept Analysis to explore conditional attribute implications
in a triadic domain. We then adapt this approach to formulate attribute
exploration with multiple experts that have different views on a domain.
</p>
"
An Evaluation of Cryptocurrency Payment Channel Networks and Their Privacy Implications. (arXiv:2102.02659v1 [cs.CR]),http://arxiv.org/abs/2102.02659,"<p>Cryptocurrencies redefined how money can be stored and transferred among
users. However, independent of the amount being sent, public blockchain-based
cryptocurrencies suffer from high transaction waiting times and fees. These
drawbacks hinder the wide use of cryptocurrencies by masses. To address these
challenges, payment channel network concept is touted as the most viable
solution to be used for micro-payments. The idea is exchanging the ownership of
money by keeping the state of the accounts locally. The users inform the
blockchain rarely, which decreases the load on the blockchain. Specifically,
payment channel networks can provide transaction approvals in seconds by
charging a nominal fee proportional to the payment amount. Such attraction on
payment channel networks inspired many recent studies which focus on how to
design them and allocate channels such that the transactions will be secure and
efficient. However, as payment channel networks are emerging and reaching large
number of users, privacy issues are becoming more relevant that raise concerns
about exposing not only individual habits but also businesses' revenues. In
this paper, we first propose a categorization of the existing payment networks
formed on top of blockchain-backed cryptocurrencies. After discussing several
emerging attacks on user/business privacy in these payment channel networks, we
qualitatively evaluate them based on a number of privacy metrics that relate to
our case. Based on the discussions on the strengths and weaknesses of the
approaches, we offer possible directions for research for the future of privacy
based payment channel networks.
</p>
"
No-reference denoising of low-dose CT projections. (arXiv:2102.02662v1 [eess.IV]),http://arxiv.org/abs/2102.02662,"<p>Low-dose computed tomography (LDCT) became a clear trend in radiology with an
aspiration to refrain from delivering excessive X-ray radiation to the
patients. The reduction of the radiation dose decreases the risks to the
patients but raises the noise level, affecting the quality of the images and
their ultimate diagnostic value. One mitigation option is to consider pairs of
low-dose and high-dose CT projections to train a denoising model using deep
learning algorithms; however, such pairs are rarely available in practice. In
this paper, we present a new self-supervised method for CT denoising. Unlike
existing self-supervised approaches, the proposed method requires only noisy CT
projections and exploits the connections between adjacent images. The
experiments carried out on an LDCT dataset demonstrate that our method is
almost as accurate as the supervised approach, while also outperforming the
considered self-supervised denoising methods.
</p>
"
Digital twins based on bidirectional LSTM and GAN for modelling COVID-19. (arXiv:2102.02664v1 [cs.LG]),http://arxiv.org/abs/2102.02664,"<p>The outbreak of the coronavirus disease 2019 (COVID-19) has now spread
throughout the globe infecting over 100 million people and causing the death of
over 2.2 million people. Thus, there is an urgent need to study the dynamics of
epidemiological models to gain a better understanding of how such diseases
spread. While epidemiological models can be computationally expensive, recent
advances in machine learning techniques have given rise to neural networks with
the ability to learn and predict complex dynamics at reduced computational
costs. Here we introduce two digital twins of a SEIRS model applied to an
idealised town. The SEIRS model has been modified to take account of spatial
variation and, where possible, the model parameters are based on official virus
spreading data from the UK. We compare predictions from a data-corrected
Bidirectional Long Short-Term Memory network and a predictive Generative
Adversarial Network. The predictions given by these two frameworks are accurate
when compared to the original SEIRS model data. Additionally, these frameworks
are data-agnostic and could be applied to towns, idealised or real, in the UK
or in other countries. Also, more compartments could be included in the SEIRS
model, in order to study more realistic epidemiological behaviour.
</p>
"
Hybrid consistency and plausibility verification of product data according to FIC. (arXiv:2102.02665v1 [cs.LG]),http://arxiv.org/abs/2102.02665,"<p>The labelling of food products in the EU is regulated by the Food Information
of Customers (FIC). Companies are required to provide the corresponding
information regarding nutrients and allergens among others. With the rise of
e-commerce more and more food products are sold online. There are often errors
in the online product descriptions regarding the FIC-relevant information due
to low data quality in the vendors' product data base. In this paper we propose
a hybrid approach of both rule-based and machine learning to verify nutrient
declaration and allergen labelling according to FIC requirements. Special focus
is given to the problem of false negatives in allergen prediction since this
poses a significant health risk to customers. Results show that a neural net
trained on a subset of the ingredients of a product is capable of predicting
the allergens contained with a high reliability.
</p>
"
The Wisdom of the Crowd and Higher-Order Beliefs. (arXiv:2102.02666v1 [econ.TH]),http://arxiv.org/abs/2102.02666,"<p>The classic wisdom-of-the-crowd problem asks how a principal can ""aggregate""
information about the unknown state of the world from agents without
understanding the information structure among them. We propose a new simple
procedure ""\emph{population mean based aggregation}"" to achieve this goal. It
only requires eliciting agents' beliefs about the state, and also eliciting
some agents' expectations of the average belief in the population. We show that
this procedure fully aggregates information: in an infinite population, it
always infers the true state of the world. The procedure can accommodate
correlation in agents' information, misspecified beliefs, any finite number of
possible states of the world, and only requires very weak assumptions on the
information structure.
</p>
"
Disease Prediction with a Maximum Entropy Method. (arXiv:2102.02668v1 [cs.LG]),http://arxiv.org/abs/2102.02668,"<p>In this paper, we propose a maximum entropy method for predicting disease
risks. It is based on a patient's medical history with diseases coded in ICD-10
which can be used in various cases. The complete algorithm with strict
mathematical derivation is given. We also present experimental results on a
medical dataset, demonstrating that our method performs well in predicting
future disease risks and achieves an accuracy rate twice that of the
traditional method. We also perform a comorbidity analysis to reveal the
intrinsic relation of diseases.
</p>
"
OmiEmbed: reconstruct comprehensive phenotypic information from multi-omics data using multi-task deep learning. (arXiv:2102.02669v1 [q-bio.GN]),http://arxiv.org/abs/2102.02669,"<p>High-dimensional omics data contains intrinsic biomedical information that is
crucial for personalised medicine. Nevertheless, it is challenging to capture
them from the genome-wide data due to the large number of molecular features
and small number of available samples, which is also called ""the curse of
dimensionality"" in machine learning. To tackle this problem and pave the way
for machine learning aided precision medicine, we proposed a unified multi-task
deep learning framework called OmiEmbed to capture a holistic and relatively
precise profile of phenotype from high-dimensional omics data. The deep
embedding module of OmiEmbed learnt an omics embedding that mapped multiple
omics data types into a latent space with lower dimensionality. Based on the
new representation of multi-omics data, different downstream networks of
OmiEmbed were trained together with the multi-task strategy to predict the
comprehensive phenotype profile of each sample. We trained the model on two
publicly available omics datasets to evaluate the performance of OmiEmbed. The
OmiEmbed model achieved promising results for multiple downstream tasks
including dimensionality reduction, tumour type classification, multi-omics
integration, demographic and clinical feature reconstruction, and survival
prediction. Instead of training and applying different downstream networks
separately, the multi-task strategy combined them together and conducted
multiple tasks simultaneously and efficiently. The model achieved better
performance with the multi-task strategy comparing to training them
individually. OmiEmbed is a powerful tool to accurately capture comprehensive
phenotypic information from high-dimensional omics data and has a great
potential to facilitate more accurate and personalised clinical decision
making.
</p>
"
Multimodal-Aware Weakly Supervised Metric Learning with Self-weighting Triplet Loss. (arXiv:2102.02670v1 [cs.LG]),http://arxiv.org/abs/2102.02670,"<p>In recent years, we have witnessed a surge of interests in learning a
suitable distance metric from weakly supervised data. Most existing methods aim
to pull all the similar samples closer while push the dissimilar ones as far as
possible. However, when some classes of the dataset exhibit multimodal
distribution, these goals conflict and thus can hardly be concurrently
satisfied. Additionally, to ensure a valid metric, many methods require a
repeated eigenvalue decomposition process, which is expensive and numerically
unstable. Therefore, how to learn an appropriate distance metric from weakly
supervised data remains an open but challenging problem. To address this issue,
in this paper, we propose a novel weakly supervised metric learning algorithm,
named MultimoDal Aware weakly supervised Metric Learning (MDaML). MDaML
partitions the data space into several clusters and allocates the local cluster
centers and weight for each sample. Then, combining it with the weighted
triplet loss can further enhance the local separability, which encourages the
local dissimilar samples to keep a large distance from the local similar
samples. Meanwhile, MDaML casts the metric learning problem into an
unconstrained optimization on the SPD manifold, which can be efficiently solved
by Riemannian Conjugate Gradient Descent (RCGD). Extensive experiments
conducted on 13 datasets validate the superiority of the proposed MDaML.
</p>
"
Directive Explanations for Actionable Explainability in Machine Learning Applications. (arXiv:2102.02671v1 [cs.LG]),http://arxiv.org/abs/2102.02671,"<p>This paper investigates the prospects of using directive explanations to
assist people in achieving recourse of machine learning decisions. Directive
explanations list which specific actions an individual needs to take to achieve
their desired outcome. If a machine learning model makes a decision that is
detrimental to an individual (e.g. denying a loan application), then it needs
to both explain why it made that decision and also explain how the individual
could obtain their desired outcome (if possible). At present, this is often
done using counterfactual explanations, but such explanations generally do not
tell individuals how to act. We assert that counterfactual explanations can be
improved by explicitly providing people with actions they could use to achieve
their desired goal. This paper makes two contributions. First, we present the
results of an online study investigating people's perception of directive
explanations. Second, we propose a conceptual model to generate such
explanations. Our online study showed a significant preference for directive
explanations ($p&lt;0.001$). However, the participants' preferred explanation type
was affected by multiple factors, such as individual preferences, social
factors, and the feasibility of the directives. Our findings highlight the need
for a human-centred and context-specific approach for creating directive
explanations.
</p>
"
DNN Based Beam Selection in mmW Heterogeneous Networks. (arXiv:2102.02672v1 [cs.NI]),http://arxiv.org/abs/2102.02672,"<p>We consider a heterogeneous cellular network wherein multiple small cell
millimeter wave (mmW) base stations (BSs) coexist with legacy sub-6GHz macro
BSs. In the mmW band, small cells use multiple narrow beams to ensure
sufficient coverage and User Equipments (UEs) have to select the best small
cell and the best beam in order to access the network. This process usually
based on exhaustive search may introduce unacceptable latency. In order to
address this issue, we rely on the sub-6GHz macro BS support and propose a deep
neural network (DNN) architecture that utilizes basic components from the
Channel State Information (CSI) of sub-6GHz network as input features. The
output of the DNN is the mmW BS and beam selection that can provide the best
communication performance. In the set of features, we avoid using the UE
location, which may not be readily available for every device. We formulate a
mmW BS selection and beam selection problem as a classification and regression
problem respectively and propose a joint solution using a branched neural
network. The numerical comparison with the conventional exhaustive search
results shows that the proposed design demonstrate better performance than
exhaustive search in terms of latency with at least 85\% accuracy.
</p>
"
Continuous Random Variable Estimation is not Optimal for the Witsenhausen Counterexample. (arXiv:2102.02673v1 [cs.IT]),http://arxiv.org/abs/2102.02673,"<p>Optimal design of distributed decision policies can be a difficult task,
illustrated by the famous Witsenhausen counterexample. In this paper we
characterize the optimal control designs for the vector-valued setting assuming
that it results in an internal state that can be described by a continuous
random variable which has a probability density function. More specifically, we
provide a genie-aided outer bound that relies on our previous results for
empirical coordination problems. This solution turns out to be not optimal in
general, since it consists of a time-sharing strategy between two linear
schemes of specific power. It follows that the optimal decision strategy for
the original scalar Witsenhausen problem must lead to an internal state that
cannot be described by a continuous random variable which has a probability
density function.
</p>
"
Observability of the relative motion from inertial data in kinematic chains. (arXiv:2102.02675v1 [eess.SY]),http://arxiv.org/abs/2102.02675,"<p>In recent years, it has been shown that the motion of kinematic chains can be
estimated using measurements from inertial sensors placed on segments connected
by rotational joints. These methods specifically avoid using magnetometer
measurements, which are known to cause issues since the magnetic field at the
different sensor locations is typically different. They rely on the assumption
that the motion of the kinematic chain is sufficiently rich to assure / yield
observability of the relative pose. However, a formal investigation of this
crucial requirement has not yet been presented and no specific conditions for
observability have so far been given. In this work, we present an observability
analysis and show that the relative pose of the body segments is indeed
observable under a very mild condition on the motion. We support these results
by a simulation study, in which we also show the effect of stationary periods
in the data and of the amount of excitation on the accuracy of the estimates.
We use experimental data from a human gait experiment to show that the
excitation level is sufficient for obtaining accurate estimates even when the
subject remains stationary for a period of 47 seconds halfway during the
experiment.
</p>
"
Certifying Differential Equation Solutions from Computer Algebra Systems in Isabelle/HOL. (arXiv:2102.02679v1 [cs.LO]),http://arxiv.org/abs/2102.02679,"<p>The Isabelle/HOL proof assistant has a powerful library for continuous
analysis, which provides the foundation for verification of hybrid systems.
However, Isabelle lacks automated proof support for continuous artifacts, which
means that verification is often manual. In contrast, Computer Algebra Systems
(CAS), such as Mathematica and SageMath, contain a wealth of efficient
algorithms for matrices, differential equations, and other related artifacts.
Nevertheless, these algorithms are not verified, and thus their outputs cannot,
of themselves, be trusted for use in a safety critical system. In this paper we
integrate two CAS systems into Isabelle, with the aim of certifying symbolic
solutions to ordinary differential equations. This supports a verification
technique that is both automated and trustworthy.
</p>
"
Hierarchical Multi-head Attentive Network for Evidence-aware Fake News Detection. (arXiv:2102.02680v1 [cs.AI]),http://arxiv.org/abs/2102.02680,"<p>The widespread of fake news and misinformation in various domains ranging
from politics, economics to public health has posed an urgent need to
automatically fact-check information. A recent trend in fake news detection is
to utilize evidence from external sources. However, existing evidence-aware
fake news detection methods focused on either only word-level attention or
evidence-level attention, which may result in suboptimal performance. In this
paper, we propose a Hierarchical Multi-head Attentive Network to fact-check
textual claims. Our model jointly combines multi-head word-level attention and
multi-head document-level attention, which aid explanation in both word-level
and evidence-level. Experiments on two real-word datasets show that our model
outperforms seven state-of-the-art baselines. Improvements over baselines are
from 6\% to 18\%. Our source code and datasets are released at
\texttt{\url{https://github.com/nguyenvo09/EACL2021}}.
</p>
"
Force-Directed Layout of Order Diagrams using Dimensional Reduction. (arXiv:2102.02684v1 [cs.CG]),http://arxiv.org/abs/2102.02684,"<p>Order diagrams allow human analysts to understand and analyze structural
properties of ordered data. While an experienced expert can create easily
readable order diagrams, the automatic generation of those remains a hard task.
In this work, we adapt force-directed approaches, which are known to generate
aesthetically-pleasing drawings of graphs, to the realm of order diagrams. Our
algorithm ReDraw thereby embeds the order in a high dimension and then
iteratively reduces the dimension until a two-dimensional drawing is achieved.
To improve aesthetics, this reduction is equipped with two force-directed steps
where one optimizes on distances of nodes and the other on distances of lines
in order to satisfy a set of a priori fixed conditions. By respecting an
invariant about the vertical position of the elements in each step of our
algorithm we ensure that the resulting drawings satisfy all necessary
properties of order diagrams. Finally, we present the results of a user study
to demonstrate that our algorithm outperforms comparable approaches on drawings
of lattices with a high degree of distributivity.
</p>
"
Impossibility of Partial Recovery in the Graph Alignment Problem. (arXiv:2102.02685v1 [stat.ML]),http://arxiv.org/abs/2102.02685,"<p>Random graph alignment refers to recovering the underlying vertex
correspondence between two random graphs with correlated edges. This can be
viewed as an average-case and noisy version of the well-known NP-hard graph
isomorphism problem. For the correlated Erd\""os-R\'enyi model, we prove an
impossibility result for partial recovery in the sparse regime, with constant
average degree and correlation, as well as a general bound on the maximal
reachable overlap. Our bound is tight in the noiseless case (the graph
isomorphism problem) and we conjecture that it is still tight with noise. Our
proof technique relies on a careful application of the probabilistic method to
build automorphisms between tree components of a subcritical Erd\""os-R\'enyi
graph.
</p>
"
TricycleGAN: Unsupervised Image Synthesis and Segmentation Based on Shape Priors. (arXiv:2102.02690v1 [eess.IV]),http://arxiv.org/abs/2102.02690,"<p>Medical image segmentation is routinely performed to isolate regions of
interest, such as organs and lesions. Currently, deep learning is the state of
the art for automatic segmentation, but is usually limited by the need for
supervised training with large datasets that have been manually segmented by
trained clinicians. The goal of semi-superised and unsupervised image
segmentation is to greatly reduce, or even eliminate, the need for training
data and therefore to minimze the burden on clinicians when training
segmentation models. To this end we introduce a novel network architecture for
capable of unsupervised and semi-supervised image segmentation called
TricycleGAN. This approach uses three generative models to learn translations
between medical images and segmentation maps using edge maps as an intermediate
step. Distinct from other approaches based on generative networks, TricycleGAN
relies on shape priors rather than colour and texture priors. As such, it is
particularly well-suited for several domains of medical imaging, such as
ultrasound imaging, where commonly used visual cues may be absent. We present
experiments with TricycleGAN on a clinical dataset of kidney ultrasound images
and the benchmark ISIC 2018 skin lesion dataset.
</p>
"
"HMC, an Algorithms in Data Mining, the Functional Analysis approach. (arXiv:2102.02691v1 [stat.CO])",http://arxiv.org/abs/2102.02691,"<p>The main purpose of this paper is to facilitate the communication between the
Analytic, Probabilistic and Algorithmic communities.
</p>
<p>We present a proof of convergence of the Hamiltonian (Hybrid) Monte Carlo
algorithm from the point of view of the
</p>
<p>Dynamical Systems, where the evolving objects are densities of probability
distributions and the tool are derived from the Functional Analysis.
</p>
"
Invertible DenseNets with Concatenated LipSwish. (arXiv:2102.02694v1 [stat.ML]),http://arxiv.org/abs/2102.02694,"<p>We introduce Invertible Dense Networks (i-DenseNets), a more parameter
efficient alternative to Residual Flows. The method relies on an analysis of
the Lipschitz continuity of the concatenation in DenseNets, where we enforce
invertibility of the network by satisfying the Lipschitz constant. We extend
this method by proposing a learnable concatenation, which not only improves the
model performance but also indicates the importance of the concatenated
representation. Additionally, we introduce the Concatenated LipSwish as
activation function, for which we show how to enforce the Lipschitz condition
and which boosts performance. The new architecture, i-DenseNet, out-performs
Residual Flow and other flow-based models on density estimation evaluated in
bits per dimension, where we utilize an equal parameter budget. Moreover, we
show that the proposed model out-performs Residual Flows when trained as a
hybrid model where the model is both a generative and a discriminative model.
</p>
"
Active Boundary Loss for Semantic Segmentation. (arXiv:2102.02696v1 [cs.CV]),http://arxiv.org/abs/2102.02696,"<p>This paper proposes a novel active boundary loss for semantic segmentation.
It can progressively encourage the alignment between predicted boundaries and
ground-truth boundaries during end-to-end training, which is not explicitly
enforced in commonly used cross-entropy loss. Based on the predicted boundaries
detected from the segmentation results using current network parameters, we
formulate the boundary alignment problem as a differentiable direction vector
prediction problem to guide the movement of predicted boundaries in each
iteration. Our loss is model-agnostic and can be plugged into the training of
segmentation networks to improve the boundary details. Experimental results
show that training with the active boundary loss can effectively improve the
boundary F-score and mean Intersection-over-Union on challenging image and
video object segmentation datasets.
</p>
"
Additive Average Schwarz Method for Elliptic Mortar Finite Element Problems with Highly Heterogeneous Coefficients. (arXiv:2102.02700v1 [math.NA]),http://arxiv.org/abs/2102.02700,"<p>In this paper, we extend the additive average Schwarz method to solve second
order elliptic boundary value problems with heterogeneous coefficients inside
the subdomains and across subdomain interfaces by the mortar technique, where
the mortar finite element discretization is on nonmatching meshes. In this
two-level method, we enrich the coarse space in two different ways, i.e., by
adding eigenfunctions of two variants of the generalized eigenvalue problems.
We prove that the condition number for the system of algebraic equations
resulting from the extended additive average Schwarz method, corresponding to
both coarse spaces, is of the order O(H/h) and independent of jumps of the
coefficients, where H and h are the mesh parameters.
</p>
"
EFloat: Entropy-coded Floating Point Format for Deep Learning. (arXiv:2102.02705v1 [cs.LG]),http://arxiv.org/abs/2102.02705,"<p>We describe the EFloat floating-point number format with 4 to 6 additional
bits of precision and a wider exponent range than the existing floating point
(FP) formats of any width including FP32, BFloat16, IEEE-Half precision,
DLFloat, TensorFloat, and 8-bit floats. In a large class of deep learning
models we observe that FP exponent values tend to cluster around few unique
values which presents entropy encoding opportunities. The EFloat format encodes
frequent exponent values and signs with Huffman codes to minimize the average
exponent field width. Saved bits then become available to the mantissa
increasing the EFloat numeric precision on average by 4 to 6 bits compared to
other FP formats of equal width. The proposed encoding concept may be
beneficial to low-precision formats including 8-bit floats. Training deep
learning models with low precision arithmetic is challenging. EFloat, with its
increased precision may provide an opportunity for those tasks as well. We
currently use the EFloat format for compressing and saving memory used in large
NLP deep learning models. A potential hardware implementation for improving
PCIe and memory bandwidth limitations of AI accelerators is also discussed.
</p>
"
ProxyFAUG: Proximity-based Fingerprint Augmentation. (arXiv:2102.02706v1 [cs.CV]),http://arxiv.org/abs/2102.02706,"<p>The proliferation of data-demanding machine learning methods has brought to
light the necessity for methodologies which can enlarge the size of training
datasets, with simple, rule-based methods. In-line with this concept, the
fingerprint augmentation scheme proposed in this work aims to augment
fingerprint datasets which are used to train positioning models. The proposed
method utilizes fingerprints which are recorded in spacial proximity, in order
to perform fingerprint augmentation, creating new fingerprints which combine
the features of the original ones. The proposed method of composing the new,
augmented fingerprints is inspired by the crossover and mutation operators of
genetic algorithms. The ProxyFAUG method aims to improve the achievable
positioning accuracy of fingerprint datasets, by introducing a rule-based,
stochastic, proximity-based method of fingerprint augmentation. The performance
of ProxyFAUG is evaluated in an outdoor Sigfox setting using a public dataset.
The best performing published positioning method on this dataset is improved by
40% in terms of median error and 6% in terms of mean error, with the use of the
augmented dataset. The analysis of the results indicate a systematic and
significant performance improvement at the lower error quartiles, as indicated
by the impressive improvement of the median error.
</p>
"
Fractionally Log-Concave and Sector-Stable Polynomials: Counting Planar Matchings and More. (arXiv:2102.02708v1 [cs.DS]),http://arxiv.org/abs/2102.02708,"<p>We show fully polynomial time randomized approximation schemes (FPRAS) for
counting matchings of a given size, or more generally sampling/counting
monomer-dimer systems in planar, not-necessarily-bipartite, graphs. While
perfect matchings on planar graphs can be counted exactly in polynomial time,
counting non-perfect matchings was shown by [Jer87] to be #P-hard, who also
raised the question of whether efficient approximate counting is possible. We
answer this affirmatively by showing that the multi-site Glauber dynamics on
the set of monomers in a monomer-dimer system always mixes rapidly, and that
this dynamics can be implemented efficiently on downward-closed families of
graphs where counting perfect matchings is tractable. As further applications
of our results, we show how to sample efficiently using multi-site Glauber
dynamics from partition-constrained strongly Rayleigh distributions, and
nonsymmetric determinantal point processes.
</p>
<p>In order to analyze mixing properties of the multi-site Glauber dynamics, we
establish two notions for generating polynomials of discrete set-valued
distributions: sector-stability and fractional log-concavity. These notions
generalize well-studied properties like real-stability and log-concavity, but
unlike them robustly degrade under useful transformations applied to the
distribution. We relate these notions to pairwise correlations in the
underlying distribution and the notion of spectral independence introduced by
[ALO20], providing a new tool for establishing spectral independence based on
geometry of polynomials. As a byproduct of our techniques, we show that
polynomials avoiding roots in a sector of the complex plane must satisfy what
we call fractional log-concavity; this generalizes a classic result established
by [Gar59] who showed homogeneous polynomials that have no roots in a
half-plane must be log-concave over the positive orthant.
</p>
"
Matching Impatient and Heterogeneous Demand and Supply. (arXiv:2102.02710v1 [math.OC]),http://arxiv.org/abs/2102.02710,"<p>Service platforms must determine rules for matching heterogeneous demand
(customers) and supply (workers) that arrive randomly over time and may be lost
if forced to wait too long for a match. We show how to balance the trade-off
between making a less good match quickly and waiting for a better match, at the
risk of losing impatient customers and/or workers. When the objective is to
maximize the cumulative value of matches over a finite-time horizon, we propose
discrete-review matching policies, both for the case in which the platform has
access to arrival rate parameter information and the case in which the platform
does not. We show that both the blind and nonblind policies are asymptotically
optimal in a high-volume setting. However, the blind policy requires frequent
re-solving of a linear program. For that reason, we also investigate a blind
policy that makes decisions in a greedy manner, and we are able to establish an
asymptotic lower bound for the greedy, blind policy that depends on the
matching values and is always higher than half of the value of an optimal
policy. Next, we develop a fluid model that approximates the evolution of the
stochastic model and captures explicitly the nonlinear dependence between the
amount of demand and supply waiting and the distribution of their patience
times. We use the fluid model to propose a policy for a more general objective
that additionally penalizes queue build-up. We run numerous simulations to
investigate the performance of the aforementioned proposed matching policies.
</p>
"
Fine-tuning deep learning model parameters for improved super-resolution of dynamic MRI with prior-knowledge. (arXiv:2102.02711v1 [eess.IV]),http://arxiv.org/abs/2102.02711,"<p>Dynamic imaging is a beneficial tool for interventions to assess
physiological changes. Nonetheless during dynamic MRI, while achieving a high
temporal resolution, the spatial resolution is compromised. To overcome this
spatio-temporal trade-off, this research presents a super-resolution (SR) MRI
reconstruction with prior knowledge based fine-tuning to maximise spatial
information while preserving high temporal resolution of dynamic MRI. An U-Net
based network with perceptual loss is trained on a benchmark dataset and
fine-tuned using one subject-specific static high resolution MRI as prior
knowledge to obtain high resolution dynamic images during the inference stage.
3D dynamic data for three subjects were acquired with different parameters to
test the generalisation capabilities of the network. The method was tested for
different levels of in-plane undersampling for dynamic MRI. The reconstructed
dynamic SR results showed higher similarity with the high resolution
ground-truth after fine-tuning. The average SSIM of the lowest resolution
experimented during this research (6.25~\% of the k-space) before and after
fine-tuning were 0.939 $\pm$ 0.008 and 0.957 $\pm$ 0.006 respectively. This
could theoretically result in an acceleration factor of 16, which can
potentially be acquired in less than half a second. The proposed approach shows
that the super-resolution MRI reconstruction with prior-information can
alleviate the spatio-temporal trade-off in dynamic MRI, even for high
acceleration factors.
</p>
"
RoI Tanh-polar Transformer Network for Face Parsing in the Wild. (arXiv:2102.02717v1 [cs.CV]),http://arxiv.org/abs/2102.02717,"<p>Face parsing aims to predict pixel-wise labels for facial components of a
target face in an image. Existing approaches usually crop the target face from
the input image with respect to a bounding box calculated during
pre-processing, and thus can only parse inner facial Regions of Interest
(RoIs). Peripheral regions like hair are ignored and nearby faces that are
partially included in the bounding box can cause distractions. Moreover, these
methods are only trained and evaluated on near-frontal portrait images and thus
their performance for in-the-wild cases were unexplored. To address these
issues, this paper makes three contributions. First, we introduce iBugMask
dataset for face parsing in the wild containing 1,000 manually annotated images
with large variations in sizes, poses, expressions and background, and
Helen-LP, a large-pose training set containing 21,866 images generated using
head pose augmentation. Second, we propose RoI Tanh-polar transform that warps
the whole image to a Tanh-polar representation with a fixed ratio between the
face area and the context, guided by the target bounding box. The new
representation contains all information in the original image, and allows for
rotation equivariance in the convolutional neural networks (CNNs). Third, we
propose a hybrid residual representation learning block, coined HybridBlock,
that contains convolutional layers in both the Tanh-polar space and the
Tanh-Cartesian space, allowing for receptive fields of different shapes in
CNNs. Through extensive experiments, we show that the proposed method
significantly improves the state-of-the-art for face parsing in the wild.
</p>
"
Edge Computing: A Systematic Mapping Study. (arXiv:2102.02720v1 [cs.NI]),http://arxiv.org/abs/2102.02720,"<p>Edge computing is a novel computing paradigm which extends cloud computing
storage and computation resources at the edge of network and closer to the
end-users in order to tackle the problem of communication latency in
latency-sensitive applications. For the last decades, there have been many
research efforts dedicated to this field. However, there are still many
operational challenges. The dramatic growth in researches, large volume of
published studies, and the attention of researchers in this field in recent
years have made it necessary to conduct a Systematic Mapping Study in this
field. we need a comprehensive guide to enable researchers to do more effective
searches on each scope of edge computing. An important part of the methodology
is to use the appropriate search method using a three-tier strategy. In this
method, we defined some quality criteria to extract search spaces and studies
with the highest quality for reading and analysis. In a separate phase, we
evaluated the extraction process of related studies in terms of accuracy. using
this comprehensive methodology, we select the number of 112 search spaces out
of all 805 ones and by search in these search spaces we select 1440
high-quality studies out of 8725. In our Systematic Mapping Study, 8 research
questions have been designed to achieve goals such as identifying the main
topics, architectures, techniques, and so on in the field of edge computing. We
aim this paper can serve as a guideline for researchers interested in this
field.
</p>
"
Data-to-text Generation with Macro Planning. (arXiv:2102.02723v1 [cs.CL]),http://arxiv.org/abs/2102.02723,"<p>Recent approaches to data-to-text generation have adopted the very successful
encoder-decoder architecture or variants thereof. These models generate text
which is fluent (but often imprecise) and perform quite poorly at selecting
appropriate content and ordering it coherently. To overcome some of these
issues, we propose a neural model with a macro planning stage followed by a
generation stage reminiscent of traditional methods which embrace separate
modules for planning and surface realization. Macro plans represent high level
organization of important content such as entities, events and their
interactions; they are learnt from data and given as input to the generator.
Extensive experiments on two data-to-text benchmarks (RotoWire and MLB) show
that our approach outperforms competitive baselines in terms of automatic and
human evaluation.
</p>
"
Multiple Criss-Cross Deletion-Correcting Codes. (arXiv:2102.02727v1 [cs.IT]),http://arxiv.org/abs/2102.02727,"<p>This paper investigates the problem of correcting multiple criss-cross
deletions in arrays. More precisely, we study the unique recovery of $n \times
n$ arrays affected by any combination of $t_\mathrm{r}$ row and $t_\mathrm{c}$
column deletions such that $t_\mathrm{r} + t_\mathrm{c} = t$ for a given $t$.
We refer to these type of deletions as $t$-criss-cross deletions. We show that
a code capable of correcting $t$-criss-cross deletions has redundancy at least
$tn + t \log n - \log(t!)$. Then, we present an existential construction of a
code capable of correcting $t$-criss-cross deletions where its redundancy is
bounded from above by $tn + \mathcal{O}(t^2 \log^2 n)$. The main ingredients of
the presented code are systematic binary $t$-deletion correcting codes and
Gabidulin codes. The first ingredient helps locating the indices of the deleted
rows and columns, thus transforming the deletion-correction problem into an
erasure-correction problem which is then solved using the second ingredient.
</p>
"
Minimum-Complexity Failure Correction in Linear Arrays via Compressive Processing. (arXiv:2102.02728v1 [eess.SY]),http://arxiv.org/abs/2102.02728,"<p>Given an array with defective elements, failure correction (FC) aims at
finding a new set of weights for the working elements so that the properties of
the original pattern can be recovered. Unlike several FC techniques available
in the literature, which update all the working excitations, the
Minimum-Complexity Failure Correction (MCFC) problem is addressed in this
paper. By properly reformulating the FC problem, the minimum number of
corrections of the whole excitations of the array is determined by means of an
innovative Compressive Processing (CP) technique in order to afford a pattern
as close as possible to the original one (i.e., the array without failures).
Selected examples, from a wide set of numerical test cases, are discussed to
assess the effectiveness of the proposed approach as well as to compare its
performance with other competitive state-of-the-art techniques in terms of both
pattern features and number of corrections.
</p>
"
Adversarial Attacks and Defenses in Physiological Computing: A Systematic Review. (arXiv:2102.02729v1 [cs.LG]),http://arxiv.org/abs/2102.02729,"<p>Physiological computing uses human physiological data as system inputs in
real time. It includes, or significantly overlaps with, brain-computer
interfaces, affective computing, adaptive automation, health informatics, and
physiological signal based biometrics. Physiological computing increases the
communication bandwidth from the user to the computer, but is also subject to
various types of adversarial attacks, in which the attacker deliberately
manipulates the training and/or test examples to hijack the machine learning
algorithm output, leading to possibly user confusion, frustration, injury, or
even death. However, the vulnerability of physiological computing systems has
not been paid enough attention to, and there does not exist a comprehensive
review on adversarial attacks to it. This paper fills this gap, by providing a
systematic review on the main research areas of physiological computing,
different types of adversarial attacks and their applications to physiological
computing, and the corresponding defense strategies. We hope this review will
attract more research interests on the vulnerability of physiological computing
systems, and more importantly, defense strategies to make them more secure.
</p>
"
Feedback Capacity of Parallel ACGN Channels and Kalman Filter: Power Allocation with Feedback. (arXiv:2102.02730v1 [cs.IT]),http://arxiv.org/abs/2102.02730,"<p>In this paper, we relate the feedback capacity of parallel additive colored
Gaussian noise (ACGN) channels to a variant of the Kalman filter. By doing so,
we obtain lower bounds on the feedback capacity of such channels, as well as
the corresponding feedback (recursive) coding schemes, which are essentially
power allocation policies with feedback, to achieve the bounds. The results are
seen to reduce to existing lower bounds in the case of a single ACGN feedback
channel, whereas when it comes to parallel additive white Gaussian noise (AWGN)
channels with feedback, the recursive coding scheme reduces to a ""feedback
water-filling"" power allocation policy.
</p>
"
Computational identification of significant actors in paintings through symbols and attributes. (arXiv:2102.02732v1 [cs.CV]),http://arxiv.org/abs/2102.02732,"<p>The automatic analysis of fine art paintings presents a number of novel
technical challenges to artificial intelligence, computer vision, machine
learning, and knowledge representation quite distinct from those arising in the
analysis of traditional photographs. The most important difference is that many
realist paintings depict stories or episodes in order to convey a lesson,
moral, or meaning. One early step in automatic interpretation and extraction of
meaning in artworks is the identifications of figures (actors). In Christian
art, specifically, one must identify the actors in order to identify the
Biblical episode or story depicted, an important step in understanding the
artwork. We designed an automatic system based on deep convolutional neural
networks and simple knowledge database to identify saints throughout six
centuries of Christian art based in large part upon saints symbols or
attributes. Our work represents initial steps in the broad task of automatic
semantic interpretation of messages and meaning in fine art.
</p>
"
Deep learning-based synthetic-CT generation in radiotherapy and PET: a review. (arXiv:2102.02734v1 [physics.med-ph]),http://arxiv.org/abs/2102.02734,"<p>Recently, deep learning (DL)-based methods for the generation of synthetic
computed tomography (sCT) have received significant research attention as an
alternative to classical ones. We present here a systematic review of these
methods by grouping them into three categories, according to their clinical
applications: I) to replace CT in magnetic resonance (MR)-based treatment
planning, II) facilitate cone-beam computed tomography (CBCT)-based
image-guided adaptive radiotherapy, and III) derive attenuation maps for the
correction of Positron Emission Tomography (PET). Appropriate database
searching was performed on journal articles published between January 2014 and
December 2020. The DL methods' key characteristics were extracted from each
eligible study, and a comprehensive comparison among network architectures and
metrics was reported. A detailed review of each category was given,
highlighting essential contributions, identifying specific challenges, and
summarising the achievements. Lastly, the statistics of all the cited works
from various aspects were analysed, revealing the popularity and future trends,
and the potential of DL-based sCT generation. The current status of DL-based
sCT generation was evaluated, assessing the clinical readiness of the presented
methods.
</p>
"
Hawkes Processes on Graphons. (arXiv:2102.02741v1 [cs.LG]),http://arxiv.org/abs/2102.02741,"<p>We propose a novel framework for modeling multiple multivariate point
processes, each with heterogeneous event types that share an underlying space
and obey the same generative mechanism. Focusing on Hawkes processes and their
variants that are associated with Granger causality graphs, our model leverages
an uncountable event type space and samples the graphs with different sizes
from a nonparametric model called {\it graphon}. Given those graphs, we can
generate the corresponding Hawkes processes and simulate event sequences.
Learning this graphon-based Hawkes process model helps to 1) infer the
underlying relations shared by different Hawkes processes; and 2) simulate
event sequences with different event types but similar dynamics. We learn the
proposed model by minimizing the hierarchical optimal transport distance
between the generated event sequences and the observed ones, leading to a novel
reward-augmented maximum likelihood estimation method. We analyze the
properties of our model in-depth and demonstrate its rationality and
effectiveness in both theory and experiments.
</p>
"
Sovereign Smartphone: To Enjoy Freedom We Have to Control Our Phones. (arXiv:2102.02743v1 [cs.CR]),http://arxiv.org/abs/2102.02743,"<p>The majority of smartphones either run iOS or Android operating systems. This
has created two distinct ecosystems largely controlled by Apple and Google -
they dictate which applications can run, how they run, and what kind of phone
resources they can access. Barring some exceptions in Android where different
phone manufacturers may have influence, users, developers, and governments are
left with little to no choice. Specifically, users need to entrust their
security and privacy to OS vendors and accept the functionality constraints
they impose. Given the wide use of Android and iOS, immediately leaving these
ecosystems is not practical, except in niche application areas. In this work,
we draw attention to the magnitude of this problem and why it is an undesirable
situation. As an alternative, we advocate the development of a new smartphone
architecture that securely transfers the control back to the users while
maintaining compatibility with the rich existing smartphone ecosystems. We
propose and analyze one such design based on advances in trusted execution
environments for ARM and RISC-V.
</p>
"
Semi-Supervised Action Recognition with Temporal Contrastive Learning. (arXiv:2102.02751v1 [cs.CV]),http://arxiv.org/abs/2102.02751,"<p>Learning to recognize actions from only a handful of labeled videos is a
challenging problem due to the scarcity of tediously collected activity labels.
We approach this problem by learning a two-pathway temporal contrastive model
using unlabeled videos at two different speeds leveraging the fact that
changing video speed does not change an action. Specifically, we propose to
maximize the similarity between encoded representations of the same video at
two different speeds as well as minimize the similarity between different
videos played at different speeds. This way we use the rich supervisory
information in terms of 'time' that is present in otherwise unsupervised pool
of videos. With this simple yet effective strategy of manipulating video
playback rates, we considerably outperform video extensions of sophisticated
state-of-the-art semi-supervised image recognition methods across multiple
diverse benchmark datasets and network architectures. Interestingly, our
proposed approach benefits from out-of-domain unlabeled videos showing
generalization and robustness. We also perform rigorous ablations and analysis
to validate our approach.
</p>
"
Materializing Knowledge Bases via Trigger Graphs. (arXiv:2102.02753v1 [cs.DB]),http://arxiv.org/abs/2102.02753,"<p>The chase is a well-established family of algorithms used to materialize
Knowledge Bases (KBs), like Knowledge Graphs (KGs), to tackle important tasks
like query answering under dependencies or data cleaning. A general problem of
chase algorithms is that they might perform redundant computations. To counter
this problem, we introduce the notion of Trigger Graphs (TGs), which guide the
execution of the rules avoiding redundant computations. We present the results
of an extensive theoretical and empirical study that seeks to answer when and
how TGs can be computed and what are the benefits of TGs when applied over
real-world KBs. Our results include introducing algorithms that compute
(minimal) TGs. We implemented our approach in a new engine, and our experiments
show that it can be significantly more efficient than the chase enabling us to
materialize KBs with 17B facts in less than 40 min on commodity machines.
</p>
"
Only a Matter of Style: Age Transformation Using a Style-Based Regression Model. (arXiv:2102.02754v1 [cs.CV]),http://arxiv.org/abs/2102.02754,"<p>The task of age transformation illustrates the change of an individual's
appearance over time. Accurately modeling this complex transformation over an
input facial image is extremely challenging as it requires making convincing
and possibly large changes to facial features and head shape, while still
preserving the input identity. In this work, we present an image-to-image
translation method that learns to directly encode real facial images into the
latent space of a pre-trained unconditional GAN (e.g., StyleGAN) subject to a
given aging shift. We employ a pre-trained age regression network used to
explicitly guide the encoder in generating the latent codes corresponding to
the desired age. In this formulation, our method approaches the continuous
aging process as a regression task between the input age and desired target
age, providing fine-grained control over the generated image. Moreover, unlike
other approaches that operate solely in the latent space using a prior on the
path controlling age, our method learns a more disentangled, non-linear path.
Finally, we demonstrate that the end-to-end nature of our approach, coupled
with the rich semantic latent space of StyleGAN, allows for further editing of
the generated images. Qualitative and quantitative evaluations show the
advantages of our method compared to state-of-the-art approaches.
</p>
"
Instance-based learning using the Half-Space Proximal Graph. (arXiv:2102.02755v1 [cs.LG]),http://arxiv.org/abs/2102.02755,"<p>The primary example of instance-based learning is the $k$-nearest neighbor
rule (kNN), praised for its simplicity and the capacity to adapt to new unseen
data and toss away old data. The main disadvantages often mentioned are the
classification complexity, which is $O(n)$, and the estimation of the parameter
$k$, the number of nearest neighbors to be used. The use of indexes at
classification time lifts the former disadvantage, while there is no conclusive
method for the latter.
</p>
<p>This paper presents a parameter-free instance-based learning algorithm using
the {\em Half-Space Proximal} (HSP) graph. The HSP neighbors simultaneously
possess proximity and variety concerning the center node. To classify a given
query, we compute its HSP neighbors and apply a simple majority rule over them.
In our experiments, the resulting classifier bettered $KNN$ for any $k$ in a
battery of datasets. This improvement sticks even when applying weighted
majority rules to both kNN and HSP classifiers.
</p>
<p>Surprisingly, when using a probabilistic index to approximate the HSP graph
and consequently speeding-up the classification task, our method could {\em
improve} its accuracy in stark contrast with the kNN classifier, which worsens
with a probabilistic index.
</p>
"
On the computational and statistical complexity of over-parameterized matrix sensing. (arXiv:2102.02756v1 [cs.LG]),http://arxiv.org/abs/2102.02756,"<p>We consider solving the low rank matrix sensing problem with Factorized
Gradient Descend (FGD) method when the true rank is unknown and over-specified,
which we refer to as over-parameterized matrix sensing. If the ground truth
signal $\mathbf{X}^* \in \mathbb{R}^{d*d}$ is of rank $r$, but we try to
recover it using $\mathbf{F} \mathbf{F}^\top$ where $\mathbf{F} \in
\mathbb{R}^{d*k}$ and $k&gt;r$, the existing statistical analysis falls short, due
to a flat local curvature of the loss function around the global maxima. By
decomposing the factorized matrix $\mathbf{F}$ into separate column spaces to
capture the effect of extra ranks, we show that $\|\mathbf{F}_t \mathbf{F}_t -
\mathbf{X}^*\|_{F}^2$ converges to a statistical error of $\tilde{\mathcal{O}}
({k d \sigma^2/n})$ after
$\tilde{\mathcal{O}}(\frac{\sigma_{r}}{\sigma}\sqrt{\frac{n}{d}})$ number of
iterations where $\mathbf{F}_t$ is the output of FGD after $t$ iterations,
$\sigma^2$ is the variance of the observation noise, $\sigma_{r}$ is the $r$-th
largest eigenvalue of $\mathbf{X}^*$, and $n$ is the number of sample. Our
results, therefore, offer a comprehensive picture of the statistical and
computational complexity of FGD for the over-parameterized matrix sensing
problem.
</p>
"
A 5 \mu W Standard Cell Memory-based Configurable Hyperdimensional Computing Accelerator for Always-on Smart Sensing. (arXiv:2102.02758v1 [eess.SP]),http://arxiv.org/abs/2102.02758,"<p>Hyperdimensional computing (HDC) is a brain-inspired computing paradigm based
on high-dimensional holistic representations of vectors. It recently gained
attention for embedded smart sensing due to its inherent error-resiliency and
suitability to highly parallel hardware implementations. In this work, we
propose a programmable all-digital CMOS implementation of a fully autonomous
HDC accelerator for always-on classification in energy-constrained sensor
nodes. By using energy-efficient standard cell memory (SCM), the design is
easily cross-technology mappable. It achieves extremely low power, 5 $\mu W$ in
typical applications, and an energy-efficiency improvement over the
state-of-the-art (SoA) digital architectures of up to 3$\times$ in post-layout
simulations for always-on wearable tasks such as EMG gesture recognition. As
part of the accelerator's architecture, we introduce novel hardware-friendly
embodiments of common HDC-algorithmic primitives, which results in 3.3$\times$
technology scaled area reduction over the SoA, achieving the same accuracy
levels in all examined targets. The proposed architecture also has a fully
configurable datapath using microcode optimized for HDC stored on an integrated
SCM based configuration memory, making the design ""general-purpose"" in terms of
HDC algorithm flexibility. This flexibility allows usage of the accelerator
across novel HDC tasks, for instance, a newly designed HDC applied to the task
of ball bearing fault detection.
</p>
"
Online Discrepancy Minimization via Persistent Self-Balancing Walks. (arXiv:2102.02765v1 [cs.DS]),http://arxiv.org/abs/2102.02765,"<p>We study the online discrepancy minimization problem for vectors in
$\mathbb{R}^d$ in the oblivious setting where an adversary is allowed fix the
vectors $x_1, x_2, \ldots, x_n$ in arbitrary order ahead of time. We give an
algorithm that maintains $O(\sqrt{\log(nd/\delta)})$ discrepancy with
probability $1-\delta$, matching the lower bound given in [Bansal et al. 2020]
up to an $O(\sqrt{\log \log n})$ factor in the high-probability regime. We also
provide results for the weighted and multi-color versions of the problem.
</p>
"
Designing an Encoder for StyleGAN Image Manipulation. (arXiv:2102.02766v1 [cs.CV]),http://arxiv.org/abs/2102.02766,"<p>Recently, there has been a surge of diverse methods for performing image
editing by employing pre-trained unconditional generators. Applying these
methods on real images, however, remains a challenge, as it necessarily
requires the inversion of the images into their latent space. To successfully
invert a real image, one needs to find a latent code that reconstructs the
input image accurately, and more importantly, allows for its meaningful
manipulation. In this paper, we carefully study the latent space of StyleGAN,
the state-of-the-art unconditional generator. We identify and analyze the
existence of a distortion-editability tradeoff and a distortion-perception
tradeoff within the StyleGAN latent space. We then suggest two principles for
designing encoders in a manner that allows one to control the proximity of the
inversions to regions that StyleGAN was originally trained on. We present an
encoder based on our two principles that is specifically designed for
facilitating editing on real images by balancing these tradeoffs. By evaluating
its performance qualitatively and quantitatively on numerous challenging
domains, including cars and horses, we show that our inversion method, followed
by common editing techniques, achieves superior real-image editing quality,
with only a small reconstruction accuracy drop.
</p>
"
PHASER: a Robust and Correspondence-free Global Pointcloud Registration. (arXiv:2102.02767v1 [cs.RO]),http://arxiv.org/abs/2102.02767,"<p>We propose PHASER, a correspondence-free global registration of
sensor-centric pointclouds that is robust to noise, sparsity, and partial
overlaps. Our method can seamlessly handle multimodal information and does not
rely on keypoint nor descriptor preprocessing modules. By exploiting properties
of Fourier analysis, PHASER operates directly on the sensor's signal, fusing
the spectra of multiple channels and computing the 6-DoF transformation based
on correlation. Our registration pipeline starts by finding the most likely
rotation followed by computing the most likely translation. Both estimates are
distributed according to a probability distribution that takes the underlying
manifold into account, i.e., a Bingham and Gaussian distribution, respectively.
This further allows our approach to consider the periodic-nature of rotations
and naturally represent its uncertainty. We extensively compare PHASER against
several well-known registration algorithms on both simulated datasets, and
real-world data acquired using different sensor configurations. Our results
show that PHASER can globally align pointclouds in less than 100ms with an
average accuracy of 2cm and 0.5deg, is resilient against noise, and can handle
partial overlap.
</p>
"
A survey on modelling of infectious disease spread and control on social contact networks. (arXiv:2102.02768v1 [physics.soc-ph]),http://arxiv.org/abs/2102.02768,"<p>Infectious diseases are a significant threat to human society which was over
sighted before the incidence of COVID-19, although according to the report of
the World Health Organisation (WHO) about 4.2 million people die annually due
to infectious disease. Due to recent COVID-19 pandemic, more than 2 million
people died during 2020 and 96.2 million people got affected by this
devastating disease. Recent research shows that applying individual
interactions and movements data could help managing the pandemic though
modelling the spread of infectious diseases on social contact networks.
Infectious disease spreading can be explained with the theories and methods of
diffusion processes where a dynamic phenomena evolves on networked systems. In
the modelling of diffusion process, it is assumed that contagious items spread
out in the networked system through the inter-node interactions. This resembles
spreading of infectious virus, e.g. spread of COVID-19, within a population
through individual social interactions. The evolution behaviours of the
diffusion process are strongly influenced by the characteristics of the
underlying system and the mechanism of the diffusion process itself. Thus,
spreading of infectious disease can be explained how people interact with each
other and by the characteristics of the disease itself. This paper presenters
the relevant theories and methodologies of diffusion process that can be used
to model the spread of infectious diseases.
</p>
"
A Living Review of Machine Learning for Particle Physics. (arXiv:2102.02770v1 [hep-ph]),http://arxiv.org/abs/2102.02770,"<p>Modern machine learning techniques, including deep learning, are rapidly
being applied, adapted, and developed for high energy physics. Given the fast
pace of this research, we have created a living review with the goal of
providing a nearly comprehensive list of citations for those developing and
applying these approaches to experimental, phenomenological, or theoretical
analyses. As a living document, it will be updated as often as possible to
incorporate the latest developments. A list of proper (unchanging) reviews can
be found within. Papers are grouped into a small set of topics to be as useful
as possible. Suggestions and contributions are most welcome, and we provide
instructions for participating.
</p>
"
Mask guided attention for fine-grained patchy image classification. (arXiv:2102.02771v1 [cs.CV]),http://arxiv.org/abs/2102.02771,"<p>In this work, we present a novel mask guided attention (MGA) method for
fine-grained patchy image classification. The key challenge of fine-grained
patchy image classification lies in two folds, ultra-fine-grained
inter-category variances among objects and very few data available for
training. This motivates us to consider employing more useful supervision
signal to train a discriminative model within limited training samples.
Specifically, the proposed MGA integrates a pre-trained semantic segmentation
model that produces auxiliary supervision signal, i.e., patchy attention mask,
enabling a discriminative representation learning. The patchy attention mask
drives the classifier to filter out the insignificant parts of images (e.g.,
common features between different categories), which enhances the robustness of
MGA for the fine-grained patchy image classification. We verify the
effectiveness of our method on three publicly available patchy image datasets.
Experimental results demonstrate that our MGA method achieves superior
performance on three datasets compared with the state-of-the-art methods. In
addition, our ablation study shows that MGA improves the accuracy by 2.25% and
2% on the SoyCultivarVein and BtfPIS datasets, indicating its practicality
towards solving the fine-grained patchy image classification.
</p>
"
DLpN: Single-Shell NODDI Using Deep Learner Estimated Isotropic Volume Fraction. (arXiv:2102.02772v1 [physics.med-ph]),http://arxiv.org/abs/2102.02772,"<p>Neurite orientation dispersion and density imaging (NODDI) enables assessment
of intracellular, extracellular and free water signals from multi-shell
diffusion MRI data. It is an insightful approach to characterize the brain
tissue microstructure. Single-shell reconstruction for NODDI parameters has
been discouraged in previous literature based on failure when fitting
especially for the neurite density index (NDI). Here, we investigated the
possibility to create robust NODDI parameter maps with single-shell data, using
isotropic volume fraction (f_{ISO}) as prior. We made the prior estimation
independent of NODDI model constraint using a dictionary based deep learning
approach. First, we proposed a stochastic sparse dictionary-based network,
DictNet in predicting f_{ISO} . In single-shell cases, fractional anisotropy
(FA) and T2 signal without diffusion weighting ( S_0 ) were incorporated in the
dictionary for f_{ISO} estimation. Then, NODDI framework was used in a prior
setting to estimate the NDI and orientation dispersion index (ODI). Using both
synthetic data simulation and human data collected on a 3T scanner, we compared
the performance of our dictionary based deep learning prior NODDI (DLpN) with
original NODDI method for both single-shell and multi-shell data. Our results
suggest that DLpN derived NDI and ODI parameters for single-shell protocols are
comparable with original multi-shell NODDI, and protocol with b=2000 s/mm 2
performs the best (error ~2% in white matter and ~4% in grey matter). This may
allow NODDI evaluation of retrospective studies on single-shell data by
additional scanning of two subjects for DictNet f_{ISO} training.
</p>
"
Recursive Prime Factorizations: Dyck Words as Numbers. (arXiv:2102.02777v1 [cs.FL]),http://arxiv.org/abs/2102.02777,"<p>I propose a class of numeral systems where numbers are represented by Dyck
words, with the systems arising from a generalization of prime factorization.
After describing two proper subsets of the Dyck language capable of uniquely
representing all natural and rational numbers respectively, I consider
""Dyck-complete"" languages, in which every member of the Dyck language
represents a number. I conclude by suggesting possible research directions.
</p>
"
Unifying Vision-and-Language Tasks via Text Generation. (arXiv:2102.02779v1 [cs.CL]),http://arxiv.org/abs/2102.02779,"<p>Existing methods for vision-and-language learning typically require designing
task-specific architectures and objectives for each task. For example, a
multi-label answer classifier for visual question answering, a region scorer
for referring expression comprehension, and a language decoder for image
captioning, etc. To alleviate these hassles, in this work, we propose a unified
framework that learns different tasks in a single architecture with the same
language modeling objective, i.e., multimodal conditional text generation,
where our models learn to generate labels in text based on the visual and
textual inputs. On 7 popular vision-and-language benchmarks, including visual
question answering, referring expression comprehension, visual commonsense
reasoning, most of which have been previously modeled as discriminative tasks,
our generative approach (with a single unified architecture) reaches comparable
performance to recent task-specific state-of-the-art vision-and-language
models. Moreover, our generative approach shows better generalization ability
on answering questions that have rare answers. In addition, we show that our
framework allows multi-task learning in a single architecture with a single set
of parameters, which achieves similar performance to separately optimized
single-task models. Our code will be publicly available at:
https://github.com/j-min/VL-T5
</p>
"
Comparing State-of-the-Art and Emerging Augmented Reality Interfaces for Autonomous Vehicle-to-Pedestrian Communication. (arXiv:2102.02783v1 [cs.HC]),http://arxiv.org/abs/2102.02783,"<p>Providing pedestrians and other vulnerable road users with a clear indication
about a fully autonomous vehicle status and intentions is crucial to make them
coexist. In the last few years, a variety of external interfaces have been
proposed, leveraging different paradigms and technologies including
vehicle-mounted devices (like LED panels), short-range on-road projections, and
road infrastructure interfaces (e.g., special asphalts with embedded displays).
These designs were experimented in different settings, using mockups, specially
prepared vehicles, or virtual environments, with heterogeneous evaluation
metrics. Promising interfaces based on Augmented Reality (AR) have been
proposed too, but their usability and effectiveness have not been tested yet.
This paper aims to complement such body of literature by presenting a
comparison of state-of-the-art interfaces and new designs under common
conditions. To this aim, an immersive Virtual Reality-based simulation was
developed, recreating a well-known scenario represented by pedestrians crossing
in urban environments under non-regulated conditions. A user study was then
performed to investigate the various dimensions of vehicle-to-pedestrian
interaction leveraging objective and subjective metrics. Even though no
interface clearly stood out over all the considered dimensions, one of the AR
designs achieved state-of-the-art results in terms of safety and trust, at the
cost of higher cognitive effort and lower intuitiveness compared to LED panels
showing anthropomorphic features. Together with rankings on the various
dimensions, indications about advantages and drawbacks of the various
alternatives that emerged from this study could provide important information
for next developments in the field.
</p>
"
Egalitarian Judgment Aggregation. (arXiv:2102.02785v1 [cs.AI]),http://arxiv.org/abs/2102.02785,"<p>Egalitarian considerations play a central role in many areas of social choice
theory. Applications of egalitarian principles range from ensuring everyone
gets an equal share of a cake when deciding how to divide it, to guaranteeing
balance with respect to gender or ethnicity in committee elections. Yet, the
egalitarian approach has received little attention in judgment aggregation -- a
powerful framework for aggregating logically interconnected issues. We make the
first steps towards filling that gap. We introduce axioms capturing two
classical interpretations of egalitarianism in judgment aggregation and situate
these within the context of existing axioms in the pertinent framework of
belief merging. We then explore the relationship between these axioms and
several notions of strategyproofness from social choice theory at large.
Finally, a novel egalitarian judgment aggregation rule stems from our analysis;
we present complexity results concerning both outcome determination and
strategic manipulation for that rule.
</p>
"
Disambiguation of weak supervision with exponential convergence rates. (arXiv:2102.02789v1 [cs.LG]),http://arxiv.org/abs/2102.02789,"<p>Machine learning approached through supervised learning requires expensive
annotation of data. This motivates weakly supervised learning, where data are
annotated with incomplete yet discriminative information. In this paper, we
focus on partial labelling, an instance of weak supervision where, from a given
input, we are given a set of potential targets. We review a disambiguation
principle to recover full supervision from weak supervision, and propose an
empirical disambiguation algorithm. We prove exponential convergence rates of
our algorithm under classical learnability assumptions, and we illustrate the
usefulness of our method on practical examples.
</p>
"
RECol: Reconstruction Error Columns for Outlier Detection. (arXiv:2102.02791v1 [cs.LG]),http://arxiv.org/abs/2102.02791,"<p>Detecting outliers or anomalies is a common data analysis task. As a
sub-field of unsupervised machine learning, a large variety of approaches
exist, but the vast majority treats the input features as independent and often
fails to recognize even simple (linear) relationships in the input feature
space. Hence, we introduce RECol, a generic data pre-processing approach to
generate additional columns in a leave-one-out-fashion: For each column, we try
to predict its values based on the other columns, generating reconstruction
error columns. We run experiments across a large variety of common baseline
approaches and benchmark datasets with and without our RECol pre-processing
method and show that the generated reconstruction error feature space generally
seems to support common outlier detection methods and often considerably
improves their ROC-AUC and PR-AUC values.
</p>
"
Im2Vec: Synthesizing Vector Graphics without Vector Supervision. (arXiv:2102.02798v1 [cs.CV]),http://arxiv.org/abs/2102.02798,"<p>Vector graphics are widely used to represent fonts, logos, digital artworks,
and graphic designs. But, while a vast body of work has focused on generative
algorithms for raster images, only a handful of options exists for vector
graphics. One can always rasterize the input graphic and resort to image-based
generative approaches, but this negates the advantages of the vector
representation. The current alternative is to use specialized models that
require explicit supervision on the vector graphics representation at training
time. This is not ideal because large-scale high quality vector-graphics
datasets are difficult to obtain. Furthermore, the vector representation for a
given design is not unique, so models that supervise on the vector
representation are unnecessarily constrained. Instead, we propose a new neural
network that can generate complex vector graphics with varying topologies, and
only requires indirect supervision from readily-available raster training
images (i.e., with no vector counterparts). To enable this, we use a
differentiable rasterization pipeline that renders the generated vector shapes
and composites them together onto a raster canvas. We demonstrate our method on
a range of datasets, and provide comparison with state-of-the-art SVG-VAE and
DeepSVG, both of which require explicit vector graphics supervision. Finally,
we also demonstrate our approach on the MNIST dataset, for which no groundtruth
vector representation is available. Source code, datasets, and more results are
available at <a href=""http://geometry.cs.ucl.ac.uk/projects/2020/Im2Vec/"">this http URL</a>
</p>
"
Federated mmWave Beam Selection Utilizing LIDAR Data. (arXiv:2102.02802v1 [cs.IT]),http://arxiv.org/abs/2102.02802,"<p>Efficient link configuration in millimeter wave (mmWave) communication
systems is a crucial yet challenging task due to the overhead imposed by beam
selection on the network performance. For vehicle-to-infrastructure (V2I)
networks, side information from LIDAR sensors mounted on the vehicles has been
leveraged to reduce the beam search overhead. In this letter, we propose
distributed LIDAR aided beam selection for V2I mmWave communication systems
utilizing federated training. In the proposed scheme, connected vehicles
collaborate to train a shared neural network (NN) on their locally available
LIDAR data during normal operation of the system. We also propose an
alternative reduced-complexity convolutional NN (CNN) architecture and LIDAR
preprocessing, which significantly outperforms previous works in terms of both
the performance and the complexity.
</p>
"
A Deeper Look into Convolutions via Pruning. (arXiv:2102.02804v1 [cs.CV]),http://arxiv.org/abs/2102.02804,"<p>Convolutional neural networks (CNNs) are able to attain better visual
recognition performance than fully connected neural networks despite having
much less parameters due to their parameter sharing principle. Hence, modern
architectures are designed to contain a very small number of fully-connected
layers, often at the end, after multiple layers of convolutions. It is
interesting to observe that we can replace large fully-connected layers with
relatively small groups of tiny matrices applied on the entire image. Moreover,
although this strategy already reduces the number of parameters, most of the
convolutions can be eliminated as well, without suffering any loss in
recognition performance. However, there is no solid recipe to detect this
hidden subset of convolutional neurons that is responsible for the majority of
the recognition work. Hence, in this work, we use the matrix characteristics
based on eigenvalues in addition to the classical weight-based importance
assignment approach for pruning to shed light on the internal mechanisms of a
widely used family of CNNs, namely residual neural networks (ResNets), for the
image classification problem using CIFAR-10, CIFAR-100 and Tiny ImageNet
datasets.
</p>
"
Rethinking Quadratic Regularizers: Explicit Movement Regularization for Continual Learning. (arXiv:2102.02805v1 [cs.LG]),http://arxiv.org/abs/2102.02805,"<p>Quadratic regularizers are often used for mitigating catastrophic forgetting
in deep neural networks (DNNs), but are unable to compete with recent continual
learning methods. To understand this behavior, we analyze parameter updates
under quadratic regularization and demonstrate such regularizers prevent
forgetting of past tasks by implicitly performing a weighted average between
current and previous values of model parameters. Our analysis shows the
inferior performance of quadratic regularizers arises from (a) dependence of
weighted averaging on training hyperparameters, which often results in unstable
training and (b) assignment of lower importance to deeper layers, which are
generally the cause for forgetting in DNNs. To address these limitations, we
propose Explicit Movement Regularization (EMR), a continual learning algorithm
that modifies quadratic regularization to remove the dependence of weighted
averaging on training hyperparameters and uses a relative measure for
importance to avoid problems caused by lower importance assignment to deeper
layers. Compared to quadratic regularization, EMR achieves 6.2% higher average
accuracy and 4.5% lower average forgetting.
</p>
"
Multi-Stage Progressive Image Restoration. (arXiv:2102.02808v1 [cs.CV]),http://arxiv.org/abs/2102.02808,"<p>Image restoration tasks demand a complex balance between spatial details and
high-level contextualized information while recovering images. In this paper,
we propose a novel synergistic design that can optimally balance these
competing goals. Our main proposal is a multi-stage architecture, that
progressively learns restoration functions for the degraded inputs, thereby
breaking down the overall recovery process into more manageable steps.
Specifically, our model first learns the contextualized features using
encoder-decoder architectures and later combines them with a high-resolution
branch that retains local information. At each stage, we introduce a novel
per-pixel adaptive design that leverages in-situ supervised attention to
reweight the local features. A key ingredient in such a multi-stage
architecture is the information exchange between different stages. To this end,
we propose a two-faceted approach where the information is not only exchanged
sequentially from early to late stages, but lateral connections between feature
processing blocks also exist to avoid any loss of information. The resulting
tightly interlinked multi-stage architecture, named as MPRNet, delivers strong
performance gains on ten datasets across a range of tasks including image
deraining, deblurring, and denoising. For example, on the Rain100L, GoPro and
DND datasets, we obtain PSNR gains of 4 dB, 0.81 dB and 0.21 dB, respectively,
compared to the state-of-the-art. The source code and pre-trained models are
available at https://github.com/swz30/MPRNet.
</p>
"
Controlling Hallucinations at Word Level in Data-to-Text Generation. (arXiv:2102.02810v1 [cs.CL]),http://arxiv.org/abs/2102.02810,"<p>Data-to-Text Generation (DTG) is a subfield of Natural Language Generation
aiming at transcribing structured data in natural language descriptions. The
field has been recently boosted by the use of neural-based generators which
exhibit on one side great syntactic skills without the need of hand-crafted
pipelines; on the other side, the quality of the generated text reflects the
quality of the training data, which in realistic settings only offer
imperfectly aligned structure-text pairs. Consequently, state-of-art neural
models include misleading statements - usually called hallucinations - in their
outputs. The control of this phenomenon is today a major challenge for DTG, and
is the problem addressed in the paper.
</p>
<p>Previous work deal with this issue at the instance level: using an alignment
score for each table-reference pair. In contrast, we propose a finer-grained
approach, arguing that hallucinations should rather be treated at the word
level. Specifically, we propose a Multi-Branch Decoder which is able to
leverage word-level labels to learn the relevant parts of each training
instance. These labels are obtained following a simple and efficient scoring
procedure based on co-occurrence analysis and dependency parsing. Extensive
evaluations, via automated metrics and human judgment on the standard WikiBio
benchmark, show the accuracy of our alignment labels and the effectiveness of
the proposed Multi-Branch Decoder. Our model is able to reduce and control
hallucinations, while keeping fluency and coherence in generated texts. Further
experiments on a degraded version of ToTTo show that our model could be
successfully used on very noisy settings.
</p>
"
SelfNorm and CrossNorm for Out-of-Distribution Robustness. (arXiv:2102.02811v1 [cs.CV]),http://arxiv.org/abs/2102.02811,"<p>Normalization techniques are crucial in stabilizing and accelerating the
training of deep neural networks. However, they are mainly designed for the
independent and identically distributed (IID) data, not satisfying many
real-world out-of-distribution (OOD) situations. Unlike most previous works,
this paper presents two normalization methods, SelfNorm and CrossNorm, to
promote OOD generalization. SelfNorm uses attention to recalibrate statistics
(channel-wise mean and variance), while CrossNorm exchanges the statistics
between feature maps. SelfNorm and CrossNorm can complement each other in OOD
generalization, though exploring different directions in statistics usage.
Extensive experiments on different domains (vision and language), tasks
(classification and segmentation), and settings (supervised and
semi-supervised) show their effectiveness.
</p>
"
Proximal Navigation Graphs and t-spanners. (arXiv:1404.1646v2 [cs.CG] UPDATED),http://arxiv.org/abs/1404.1646,"<p>Let $(X,\mathbf{d})$ be a metric space, $V\subseteq X$ a finite set, and $E
\subseteq V \times V$. We call the graph $G(E,V)$ a {\em metric} graph if each
edge $(u,v) \in E$ has weight $d(u,v)$. In particular edge $(u,u)$ is in the
graph and have distance $0$. We call $G$ a {\em proximal navigation graph} or
$PN$-graph if for each edge $(u,v) \in E$ either $u=v$ or there is a node $u_1$
such that $(u,u_1) \in E$ and $\mathbf{d}(u,v) &gt; \mathbf{d}(u_1,v)$. In such
graph it is possible to navigate greedily from an arbitrary source node to an
arbitrary target node by reducing the distance between the current node and the
target node in each step. The complete graph, the Delaunay triangulation and
the Half Space Proximal (HSP) graph (defined below in the paper) are examples
of $PN$-graphs.
</p>
<p>In this paper we study the relationship between $PN$-graphs and $t$-spanners
and prove that there are $PN$-graphs that are not $t$-spanners for any $t$. On
the positive side we give sufficient conditions for a $PN$-graph to be a
$t$-spanner and prove that any $PN$-graph over $\mathbb{R}^n$ under the
euclidean distance is a $t$-spanner.
</p>
"
FLAME: A Fast Large-scale Almost Matching Exactly Approach to Causal Inference. (arXiv:1707.06315v9 [stat.ML] UPDATED),http://arxiv.org/abs/1707.06315,"<p>A classical problem in causal inference is that of matching, where treatment
units need to be matched to control units based on covariate information. In
this work, we propose a method that computes high quality almost-exact matches
for high-dimensional categorical datasets. This method, called FLAME (Fast
Large-scale Almost Matching Exactly), learns a distance metric for matching
using a hold-out training data set. In order to perform matching efficiently
for large datasets, FLAME leverages techniques that are natural for query
processing in the area of database management, and two implementations of FLAME
are provided: the first uses SQL queries and the second uses bit-vector
techniques. The algorithm starts by constructing matches of the highest quality
(exact matches on all covariates), and successively eliminates variables in
order to match exactly on as many variables as possible, while still
maintaining interpretable high-quality matches and balance between treatment
and control groups. We leverage these high quality matches to estimate
conditional average treatment effects (CATEs). Our experiments show that FLAME
scales to huge datasets with millions of observations where existing
state-of-the-art methods fail, and that it achieves significantly better
performance than other matching methods.
</p>
"
A New Approach To Estimate The Collision Probability For Automotive Applications. (arXiv:1711.07060v9 [eess.SY] UPDATED),http://arxiv.org/abs/1711.07060,"<p>We revisit the computation of probability of collision in the context of
automotive collision avoidance (the estimation of a potential collision is also
referred to as conflict detection in other contexts). After reviewing existing
approaches to the definition and computation of a collision probability we
argue that the question ""What is the probability of collision within the next
three seconds?"" can be answered on the basis of a collision probability rate.
</p>
<p>Using results on level crossings for vector stochastic processes we derive a
general expression for the upper bound of the distribution of the collision
probability rate. This expression is valid for arbitrary prediction models
including process noise.
</p>
<p>We demonstrate in several examples that distributions obtained by large-scale
Monte-Carlo simulations obey this bound and in many cases approximately
saturate the bound. We derive an approximation for the distribution of the
collision probability rate that can be computed on an embedded platform. An
upper bound of the probability of collision is then obtained by one-dimensional
numerical integration over the time period of interest.
</p>
<p>A straightforward application of this method applies to the collision of an
extended object with a second point-like object. Using an abstraction of the
second object by salient points of its boundary we propose an application of
this method to two extended objects with arbitrary orientation.
</p>
<p>Finally, the distribution of the collision probability rate is identified as
the distribution of the time-to-collision.
</p>
"
Spatial Deep Learning for Wireless Scheduling. (arXiv:1808.01486v3 [eess.SP] UPDATED),http://arxiv.org/abs/1808.01486,"<p>The optimal scheduling of interfering links in a dense wireless network with
full frequency reuse is a challenging task. The traditional method involves
first estimating all the interfering channel strengths then optimizing the
scheduling based on the model. This model-based method is however resource
intensive and computationally hard because channel estimation is expensive in
dense networks; furthermore, finding even a locally optimal solution of the
resulting optimization problem may be computationally complex. This paper shows
that by using a deep learning approach, it is possible to bypass the channel
estimation and to schedule links efficiently based solely on the geographic
locations of the transmitters and the receivers, due to the fact that in many
propagation environments, the wireless channel strength is largely a function
of the distance dependent path-loss. This is accomplished by unsupervised
training over randomly deployed networks, and by using a novel neural network
architecture that computes the geographic spatial convolutions of the
interfering or interfered neighboring nodes along with subsequent multiple
feedback stages to learn the optimum solution. The resulting neural network
gives near-optimal performance for sum-rate maximization and is capable of
generalizing to larger deployment areas and to deployments of different link
densities. Moreover, to provide fairness, this paper proposes a novel
scheduling approach that utilizes the sum-rate optimal scheduling algorithm
over judiciously chosen subsets of links for maximizing a proportional fairness
objective over the network. The proposed approach shows highly competitive and
generalizable network utility maximization results.
</p>
"
Improving the accuracy of nearest-neighbor classification using principled construction and stochastic sampling of training-set centroids. (arXiv:1809.02599v3 [cs.LG] UPDATED),http://arxiv.org/abs/1809.02599,"<p>A conceptually simple way to classify images is to directly compare test-set
data and training-set data. The accuracy of this approach is limited by the
method of comparison used, and by the extent to which the training-set data
cover configuration space. Here we show that this coverage can be substantially
increased using coarse graining (replacing groups of images by their centroids)
and stochastic sampling (using distinct sets of centroids in combination). We
use the MNIST and Fashion-MNIST data sets to show that a principled
coarse-graining algorithm can convert training images into fewer image
centroids without loss of accuracy of classification of test-set images by
nearest-neighbor classification. Distinct batches of centroids can be used in
combination as a means of stochastically sampling configuration space, and can
classify test-set data more accurately than can the unaltered training set. On
the MNIST and Fashion-MNIST data sets this approach converts nearest-neighbor
classification from a mid-ranking- to an upper-ranking member of the set of
classical machine-learning techniques.
</p>
"
How many matchings cover the nodes of a graph?. (arXiv:1811.07327v2 [math.CO] UPDATED),http://arxiv.org/abs/1811.07327,"<p>Given an undirected graph, are there $k$ matchings whose union covers all of
its nodes, that is, a matching-$k$-cover? A first, easy polynomial solution
from matroid union is possible, as already observed by Wang, Song and Yuan
(Mathematical Programming, 2014). However, it was not satisfactory neither from
the algorithmic viewpoint nor for proving graphic theorems, since the
corresponding matroid ignores the edges of the graph.
</p>
<p>We prove here, simply and algorithmically: all nodes of a graph can be
covered with $k\ge 2$ matchings if and only if for every stable set $S$ we have
$|S|\le k\cdot|N(S)|$. When $k=1$, an exception occurs: this condition is not
enough to guarantee the existence of a matching-$1$-cover, that is, the
existence of a perfect matching, in this case Tutte's famous matching theorem
(J. London Math. Soc., 1947) provides the right `good' characterization. The
condition above then guarantees only that a perfect $2$-matching exists, as
known from another theorem of Tutte (Proc. Amer. Math. Soc., 1953).
</p>
<p>Some results are then deduced as consequences with surprisingly simple
proofs, using only the level of difficulty of bipartite matchings. We give some
generalizations, as well as a solution for minimization if the edge-weights are
non-negative, while the edge-cardinality maximization of matching-$2$-covers
turns out to be already NP-hard.
</p>
<p>We have arrived at this problem as the line graph special case of a model
arising for manufacturing integrated circuits with the technology called
`Directed Self Assembly'.
</p>
"
First-order Newton-type Estimator for Distributed Estimation and Inference. (arXiv:1811.11368v2 [stat.ML] UPDATED),http://arxiv.org/abs/1811.11368,"<p>This paper studies distributed estimation and inference for a general
statistical problem with a convex loss that could be non-differentiable. For
the purpose of efficient computation, we restrict ourselves to stochastic
first-order optimization, which enjoys low per-iteration complexity. To
motivate the proposed method, we first investigate the theoretical properties
of a straightforward Divide-and-Conquer Stochastic Gradient Descent (DC-SGD)
approach. Our theory shows that there is a restriction on the number of
machines and this restriction becomes more stringent when the dimension $p$ is
large. To overcome this limitation, this paper proposes a new multi-round
distributed estimation procedure that approximates the Newton step only using
stochastic subgradient. The key component in our method is the proposal of a
computationally efficient estimator of $\Sigma^{-1} w$, where $\Sigma$ is the
population Hessian matrix and $w$ is any given vector. Instead of estimating
$\Sigma$ (or $\Sigma^{-1}$) that usually requires the second-order
differentiability of the loss, the proposed First-Order Newton-type Estimator
(FONE) directly estimates the vector of interest $\Sigma^{-1} w$ as a whole and
is applicable to non-differentiable losses. Our estimator also facilitates the
inference for the empirical risk minimizer. It turns out that the key term in
the limiting covariance has the form of $\Sigma^{-1} w$, which can be estimated
by FONE.
</p>
"
Complexity-Theoretic Aspects of Expanding Cellular Automata. (arXiv:1902.05487v3 [cs.CC] UPDATED),http://arxiv.org/abs/1902.05487,"<p>The expanding cellular automata (XCA) variant of cellular automata is
investigated and characterized from a complexity-theoretical standpoint. An XCA
is a one-dimensional cellular automaton which can dynamically create new cells
between existing ones. The respective polynomial-time complexity class is shown
to coincide with ${\le_{tt}^p}(\mathsf{NP})$, that is, the class of decision
problems polynomial-time truth-table reducible to problems in $\mathsf{NP}$. An
alternative characterization based on a variant of non-deterministic Turing
machines is also given. In addition, corollaries on select XCA variants are
proven: XCAs with multiple accept and reject states are shown to be
polynomial-time equivalent to the original XCA model. Finally, XCAs with
alternative acceptance conditions are considered and classified in terms of
${\le_{tt}^p}(\mathsf{NP})$ and the Turing machine polynomial-time class
$\mathsf{P}$.
</p>
"
Memory Augmented Neural Network Adaptive Controllers: Performance and Stability. (arXiv:1905.02832v13 [cs.SY] UPDATED),http://arxiv.org/abs/1905.02832,"<p>In this paper, we propose a novel control architecture, inspired from
neuroscience, for adaptive control of continuous-time systems. A key objective
explored in this paper is to design control architectures and algorithms that
can {\it learn and adapt quickly to changes that are even abrupt}. The proposed
architecture, in the setting of standard neural network (NN) based adaptive
control, augments an {\it external working memory} to the NN. The controller
through a write operation, writes the hidden layer feature vector of the NN to
the external working memory and can also update this information with the
observed error in the output. The controller through a read operation retrieves
information from the working memory to modify the final control signal.
Primarily, the use of external working memory is aimed at improving the context
thereby inducing the learning system to search in a particular direction. This
directed learning allows the learning system to find a good approximation of
the unknown function even after abrupt changes quickly. We consider a model
reference NN adaptive controller for linear systems with matched uncertainty
for concrete development of our ideas. We prove that the resulting controller
leads to Uniformly Bounded (UB) stable closed loop system. Through extensive
simulations and specific metrics we show that memory augmentation improves
learning significantly even when the system undergoes sudden changes.
</p>
"
Variational Federated Multi-Task Learning. (arXiv:1906.06268v2 [cs.LG] UPDATED),http://arxiv.org/abs/1906.06268,"<p>In federated learning, a central server coordinates the training of a single
model on a massively distributed network of devices. This setting can be
naturally extended to a multi-task learning framework, to handle real-world
federated datasets that typically show strong statistical heterogeneity among
devices. Despite federated multi-task learning being shown to be an effective
paradigm for real-world datasets, it has been applied only on convex models. In
this work, we introduce VIRTUAL, an algorithm for federated multi-task learning
for general non-convex models. In VIRTUAL the federated network of the server
and the clients is treated as a star-shaped Bayesian network, and learning is
performed on the network using approximated variational inference. We show that
this method is effective on real-world federated datasets, outperforming the
current state-of-the-art for federated learning, and concurrently allowing
sparser gradient updates.
</p>
"
On linear convergence of two decentralized algorithms. (arXiv:1906.07225v2 [math.OC] UPDATED),http://arxiv.org/abs/1906.07225,"<p>Decentralized algorithms solve multi-agent problems over a connected network,
where the information can only be exchanged with the accessible neighbors.
Though there exist several decentralized optimization algorithms, there are
still gaps in convergence conditions and rates between decentralized and
centralized algorithms. In this paper, we fill some gaps by considering two
decentralized algorithms: EXTRA and NIDS. They both converge linearly with
strongly convex objective functions. We will answer two questions regarding
them. What are the optimal upper bounds for their stepsizes? Do decentralized
algorithms require more properties on the functions for linear convergence than
centralized ones? More specifically, we relax the required conditions for
linear convergence of both algorithms. For EXTRA, we show that the stepsize is
comparable to that of centralized algorithms. For NIDS, the upper bound of the
stepsize is shown to be exactly the same as the centralized ones. In addition,
we relax the requirement for the objective functions and the mixing matrices.
We provide the linear convergence results for both algorithms under the weakest
conditions.
</p>
"
"Lags in the Release, Adoption, and Propagation of npm Vulnerability Fixes. (arXiv:1907.03407v4 [cs.SE] UPDATED)",http://arxiv.org/abs/1907.03407,"<p>Security vulnerability in third-party dependencies is a growing concern not
only for developers of the affected software, but for the risks it poses to an
entire software ecosystem, e.g., Heartbleed vulnerability. Recent studies show
that developers are slow to respond to the threat of vulnerability, sometimes
taking four to eleven months to act. To ensure quick adoption and propagation
of a release that contains the fix (fixing release), we conduct an empirical
investigation to identify lags that may occur between the vulnerable release
and its fixing release (package-side fixing release). Through a preliminary
study of 231 package-side fixing release of npm projects on GitHub, we observe
that a fixing release is rarely released on its own, with up to 85.72% of the
bundled commits being unrelated to a fix. We then compare the package-side
fixing release with changes on a client-side (client-side fixing release).
Through an empirical study of the adoption and propagation tendencies of 1,290
package-side fixing releases that impact throughout a network of 1,553,325
releases of npm packages, we find that stale clients require additional
migration effort, even if the package-side fixing release was quick (i.e.,
package patch landing). Furthermore, we show the influence of factors such as
the branch that the package-side fixing release lands on and the severity of
vulnerability on its propagation. In addition to these lags we identify and
characterize, this paper lays the groundwork for future research on how to
mitigate lags in an ecosystem.
</p>
"
"On the Optimality of Reconfigurable Intelligent Surfaces (RISs): Passive Beamforming, Modulation, and Resource Allocation. (arXiv:1910.00968v2 [cs.IT] UPDATED)",http://arxiv.org/abs/1910.00968,"<p>Reconfigurable intelligent surfaces (RISs) have recently emerged as a
promising technology that can achieve high spectrum and energy efficiency for
future wireless networks by integrating a massive number of low-cost and
passive reflecting elements. An RIS can manipulate the properties of an
incident wave, such as the frequency, amplitude, and phase, and, then, reflect
this manipulated wave to a desired destination, without the need for complex
signal processing. In this paper, the asymptotic optimality of achievable rate
in a downlink RIS system is analyzed under a practical RIS environment with its
associated limitations. In particular, a passive beamformer that can achieve
the asymptotic optimal performance by controlling the incident wave properties
is designed, under a limited RIS control link and practical reflection
coefficients. In order to increase the achievable system sum-rate, a modulation
scheme that can be used in an RIS without interfering with existing users is
proposed and its average symbol error ratio is asymptotically derived.
Moreover, a new resource allocation algorithm that jointly considers user
scheduling and power control is designed, under consideration of the proposed
passive beamforming and modulation schemes. Simulation results show that the
proposed schemes are in close agreement with their upper bounds in presence of
a large number of RIS reflecting elements thereby verifying that the achievable
rate in practical RISs satisfies the asymptotic optimality.
</p>
"
Stochastic Optimal Control of HVAC system for Energy-efficient Buildings. (arXiv:1911.00840v3 [eess.SY] UPDATED),http://arxiv.org/abs/1911.00840,"<p>The heating, ventilation and air-conditioning (HVAC) system accounts for
substantial energy use in buildings, whereas a large group of occupants are
still not actually feeling comfortable staying inside. This poses the issue of
developing energy-efficient HVAC control, i.e., reduce energy use (cost) while
simultaneously enhancing human comfort. This paper pursues the objective and
studies the stochastic optimal HVAC control subject to uncertain thermal demand
(i.e., the weather and occupancy etc). Particularly, we involve the elaborate
predicted mean vote (PMV) thermal comfort model in the optimization. The
problem is computationally challenging due to the non-linear and non-analytical
constraints imposed by the system dynamics and PMV model. We make the following
contributions to address it. First, we formulate the problem as a Markov
decision process (MDP) which is a desirable modeling technique capable of
handling the complexities. Second, we propose a gradient-based learning (GB-L)
method for progressively learning a stochastic control policy off-line and
store it for on-line execution. Third, we prove the learning method converge to
the optimal policies theoretically, and its performance (i.e., energy cost,
thermal comfort and on-line computation) for HVAC control via simulations. The
comparisons with the existing model predictive control based relaxation (MPC-R)
method which is assumed with accurate future information and supposed to
provide the near-optimal bounds, show that though there exists some performance
loss in energy cost reduction (i.e., 6.5%), the proposed method can enable
efficient on-line implementation (less than 1 second) and provide high
probability of thermal comfort under uncertainties.
</p>
"
Space-time multilevel Monte Carlo methods and their application to cardiac electrophysiology. (arXiv:1911.06066v2 [cs.CE] UPDATED),http://arxiv.org/abs/1911.06066,"<p>We present a novel approach aimed at high-performance uncertainty
quantification for time-dependent problems governed by partial differential
equations. In particular, we consider input uncertainties described by a
Karhunen-Loeeve expansion and compute statistics of high-dimensional
quantities-of-interest, such as the cardiac activation potential. Our
methodology relies on a close integration of multilevel Monte Carlo methods,
parallel iterative solvers, and a space-time discretization. This combination
allows for space-time adaptivity, time-changing domains, and to take advantage
of past samples to initialize the space-time solution. The resulting sequence
of problems is distributed using a multilevel parallelization strategy,
allocating batches of samples having different sizes to a different number of
processors. We assess the performance of the proposed framework by showing in
detail its application to the solution of nonlinear equations arising from
cardiac electrophysiology. Specifically, we study the effect of
spatially-correlated perturbations of the heart fibers conductivities on the
mean and variance of the resulting activation map. As shown by the experiments,
the theoretical rates of convergence of multilevel Monte Carlo are achieved.
Moreover, the total computational work for a prescribed accuracy is reduced by
an order of magnitude with respect to standard Monte Carlo methods.
</p>
"
Semantic Segmentation for Compound figures. (arXiv:1912.07142v3 [cs.CV] UPDATED),http://arxiv.org/abs/1912.07142,"<p>Scientific literature contains large volumes of unstructured data,with over
30\% of figures constructed as a combination of multiple images, these compound
figures cannot be analyzed directly with existing information retrieval tools.
In this paper, we propose a semantic segmentation approach for compound figure
separation, decomposing the compound figures into ""master images"". Each master
image is one part of a compound figure governed by a subfigure label (typically
""(a), (b), (c), etc""). In this way, the separated subfigures can be easily
associated with the description information in the caption. In particular, we
propose an anchor-based master image detection algorithm, which leverages the
correlation between master images and subfigure labels and locates the master
images in a two-step manner. First, a subfigure label detector is built to
extract the global layout information of the compound figure. Second, the
layout information is combined with local features to locate the master images.
We validate the effectiveness of proposed method on our labeled testing dataset
both quantitatively and qualitatively.
</p>
"
In Nomine Function: Naming Functions in Stripped Binaries with Neural Networks. (arXiv:1912.07946v3 [cs.LG] UPDATED),http://arxiv.org/abs/1912.07946,"<p>In this paper we investigate the problem of automatically naming pieces of
assembly code. Where by naming we mean assigning to an assembly function a
string of words that would likely be assigned by a human reverse engineer. We
formally and precisely define the framework in which our investigation takes
place. That is we define the problem, we provide reasonable justifications for
the choices that we made for the design of training and the tests. We performed
an analysis on a large real-world corpora constituted by nearly 9 millions of
functions taken from more than 22k softwares. In such framework we test
baselines coming from the field of Natural Language Processing (e.g., Seq2Seq
networks and Transformer). Interestingly, our evaluation shows promising
results beating the state-of-the-art and reaching good performance. We
investigate the applicability of tine-tuning (i.e., taking a model already
trained on a large generic corpora and retraining it for a specific task). Such
technique is popular and well-known in the NLP field. Our results confirm that
fine-tuning is effective even when neural networks are applied to binaries. We
show that a model, pre-trained on the aforementioned corpora, when fine-tuned
has higher performances on specific domains (such as predicting names in system
utilites, malware, etc).
</p>
"
A Theory of Trotter Error. (arXiv:1912.08854v3 [quant-ph] UPDATED),http://arxiv.org/abs/1912.08854,"<p>The Lie-Trotter formula, together with its higher-order generalizations,
provides a direct approach to decomposing the exponential of a sum of
operators. Despite significant effort, the error scaling of such product
formulas remains poorly understood. We develop a theory of Trotter error that
overcomes the limitations of prior approaches based on truncating the
Baker-Campbell-Hausdorff expansion. Our analysis directly exploits the
commutativity of operator summands, producing tighter error bounds for both
real- and imaginary-time evolutions. Whereas previous work achieves similar
goals for systems with geometric locality or Lie-algebraic structure, our
approach holds in general. We give a host of improved algorithms for digital
quantum simulation and quantum Monte Carlo methods, including simulations of
second-quantized plane-wave electronic structure, $k$-local Hamiltonians,
rapidly decaying power-law interactions, clustered Hamiltonians, the transverse
field Ising model, and quantum ferromagnets, nearly matching or even
outperforming the best previous results. We obtain further speedups using the
fact that product formulas can preserve the locality of the simulated system.
Specifically, we show that local observables can be simulated with complexity
independent of the system size for power-law interacting systems, which implies
a Lieb-Robinson bound as a byproduct. Our analysis reproduces known tight
bounds for first- and second-order formulas. Our higher-order bound
overestimates the complexity of simulating a one-dimensional Heisenberg model
with an even-odd ordering of terms by only a factor of $5$, and is close to
tight for power-law interactions and other orderings of terms. This suggests
that our theory can accurately characterize Trotter error in terms of both
asymptotic scaling and constant prefactor.
</p>
"
A Deep Learning Algorithm for High-Dimensional Exploratory Item Factor Analysis. (arXiv:2001.07859v4 [stat.ME] UPDATED),http://arxiv.org/abs/2001.07859,"<p>Marginal maximum likelihood (MML) estimation is the preferred approach to
fitting item response theory models in psychometrics due to the MML estimator's
consistency, normality, and efficiency as the sample size tends to infinity.
However, state-of-the-art MML estimation procedures such as the
Metropolis-Hastings Robbins-Monro (MH-RM) algorithm as well as approximate MML
estimation procedures such as variational inference (VI) are computationally
time-consuming when the sample size and the number of latent factors are very
large. In this work, we investigate a deep learning-based VI algorithm for
exploratory item factor analysis (IFA) that is computationally fast even in
large data sets with many latent factors. The proposed approach applies a deep
artificial neural network model called an importance-weighted autoencoder
(IWAE) for exploratory IFA. The IWAE approximates the MML estimator using an
importance sampling technique wherein increasing the number of
importance-weighted (IW) samples drawn during fitting improves the
approximation, typically at the cost of decreased computational efficiency. We
provide a real data application that recovers results aligning with
psychological theory across random starts. Via simulation studies, we show that
the IWAE yields more accurate estimates as either the sample size or the number
of IW samples increases (although factor correlation and intercepts estimates
exhibit some bias) and obtains similar results to MH-RM in less time. Our
simulations also suggest that the proposed approach performs similarly to and
is potentially faster than constrained joint maximum likelihood estimation, a
fast procedure that is consistent when the sample size and the number of items
simultaneously tend to infinity.
</p>
"
Simultaneous State and Unknown Input Set-Valued Observers for Some Classes of Nonlinear Dynamical Systems. (arXiv:2001.10125v2 [eess.SY] UPDATED),http://arxiv.org/abs/2001.10125,"<p>In this paper, we propose fixed-order set-valued (in the form of l2-norm
hyperballs) observers for some classes of nonlinear bounded-error dynamical
systems with unknown input signals that simultaneously find bounded hyperballs
of states and unknown inputs that include the true states and inputs. Necessary
and sufficient conditions in the form of Linear Matrix Inequalities (LMIs) for
the stability (in the sense of quadratic stability) of the proposed observers
are derived for ($\mathcal{M},\gamma$)- Quadratically Constrained
(($\mathcal{M},\gamma$)-QC) systems, which includes several classes of
nonlinear systems: (I) Lipschitz continuous, (II) ($\mathcal{A},\gamma$)-QC*
and (III) Linear Parameter-Varying (LPV) systems. This new quadratic constraint
property is at least as general as the incremental quadratic constraint
property for nonlinear systems and is proven in the paper to embody a broad
range of nonlinearities. In addition, we design the optimal
$\mathcal{H}_{\infty}$ observer among those that satisfy the quadratic
stability conditions and show that the design results in Uniformly
Bounded-Input Bounded-State (UBIBS) estimate radii/error dynamics and uniformly
bounded sequences of the estimate radii. Furthermore, we provide closed-form
upper bound sequences for the estimate radii and sufficient condition for their
convergence to steady state. Finally, the effectiveness of the proposed
set-valued observers is demonstrated through illustrative examples, where we
compare the performance of our observers with some existing observers.
</p>
"
Zeta Functions and the (Linear) Logic of Markov Processes. (arXiv:2001.11906v3 [cs.LO] UPDATED),http://arxiv.org/abs/2001.11906,"<p>In a series of papers, the author introduced models of linear logic known as
""Interaction Graphs"". These models generalise Girard's various geometry of
interaction constructions, providing a unifying framework for those. In this
work, we exhibit how these models can be understood mathematically through a
cocycle property satisfied by zeta functions of dynamical systems. Focussing on
probabilistic models, we then explain how the notion of graphings used in the
models captures a natural class of Markov processes. We further extend previous
constructions to provide a model of second-order linear logic as a type system
over the set of all (discrete-time) sub-Markov processes.
</p>
"
Bidirectional Trajectory Computation for Odometer-Aided Visual-Inertial SLAM. (arXiv:2002.00195v2 [cs.RO] UPDATED),http://arxiv.org/abs/2002.00195,"<p>Odometer-aided visual-inertial SLAM systems typically have a good performance
for navigation of wheeled platforms, while they usually suffer from degenerate
cases before the first turning. In this paper, firstly we perform an
observability analysis w.r.t. the extrinsic parameters before the first
turning, which is a complement of the existing results of observability
analyses. Secondly, inspired by the above observability analyses, we propose a
bidirectional trajectory computation method, by which the poses before the
first turning are refined in the backward computation thread, and the real-time
trajectory is adjusted accordingly. Experimental results prove that our
proposed method not only solves the problem of the unobservability of
accelerometer bias and extrinsic parameters before the first turning, but also
results in more accurate trajectories in comparison with the state-of-the-art
approaches.
</p>
"
"Lower bounds for algebraic machines, semantically. (arXiv:2002.10888v2 [cs.CC] UPDATED)",http://arxiv.org/abs/2002.10888,"<p>This paper presents a new semantic method for proving lower bounds in
computational complexity. We use it to prove that maxflow, a PTIME complete
problem, is not computable in polylogarithmic time on parallel random access
machines (PRAMs) working with integers, showing that NCZ \neq PTIME, where NCZ
is the complexity class defined by such machines, and PTIME is the standard
class of polynomial time computable problems (on, say, a Turing machine). On
top of showing this new separation result, we show our method captures previous
lower bounds results from the literature: Steele and Yao's lower bounds for
algebraic decision trees, Ben-Or's lower bounds for algebraic computation
trees, Cucker's proof that NC is not equal to PTIME on the reals, and
Mulmuley's lower bounds for ""PRAMs without bit operations"".
</p>
"
Fast Linear Convergence of Randomized BFGS. (arXiv:2002.11337v4 [math.OC] UPDATED),http://arxiv.org/abs/2002.11337,"<p>Since the late 1950's when quasi-Newton methods first appeared, they have
become one of the most widely used and efficient algorithmic paradigms for
unconstrained optimization. Despite their immense practical success, there is
little theory that shows why these methods are so efficient. We provide a
semi-local rate of convergence for the randomized BFGS method which can be
significantly better than that of gradient descent, finally giving theoretical
evidence supporting the superior empirical performance of the method.
</p>
"
Pursuing Sources of Heterogeneity in Modeling Clustered Population. (arXiv:2003.04787v2 [stat.ME] UPDATED),http://arxiv.org/abs/2003.04787,"<p>Researchers often have to deal with heterogeneous population with mixed
regression relationships, increasingly so in the era of data explosion. In such
problems, when there are many candidate predictors, it is not only of interest
to identify the predictors that are associated with the outcome, but also to
distinguish the true sources of heterogeneity, i.e., to identify the predictors
that have different effects among the clusters and thus are the true
contributors to the formation of the clusters. We clarify the concepts of the
source of heterogeneity that account for potential scale differences of the
clusters and propose a regularized finite mixture effects regression to achieve
heterogeneity pursuit and feature selection simultaneously. As the name
suggests, the problem is formulated under an effects-model parameterization, in
which the cluster labels are missing and the effect of each predictor on the
outcome is decomposed to a common effect term and a set of cluster-specific
terms. A constrained sparse estimation of these effects leads to the
identification of both the variables with common effects and those with
heterogeneous effects. We propose an efficient algorithm and show that our
approach can achieve both estimation and selection consistency. Simulation
studies further demonstrate the effectiveness of our method under various
practical scenarios. Three applications are presented, namely, an imaging
genetics study for linking genetic factors and brain neuroimaging traits in
Alzheimer's disease, a public health study for exploring the association
between suicide risk among adolescents and their school district
characteristics, and a sport analytics study for understanding how the salary
levels of baseball players are associated with their performance and
contractual status.
</p>
"
Answering Complex Queries in Knowledge Graphs with Bidirectional Sequence Encoders. (arXiv:2004.02596v4 [cs.AI] UPDATED),http://arxiv.org/abs/2004.02596,"<p>Representation learning for knowledge graphs (KGs) has focused on the problem
of answering simple link prediction queries. In this work we address the more
ambitious challenge of predicting the answers of conjunctive queries with
multiple missing entities. We propose Bi-Directional Query Embedding (BIQE), a
method that embeds conjunctive queries with models based on bi-directional
attention mechanisms. Contrary to prior work, bidirectional self-attention can
capture interactions among all the elements of a query graph. We introduce a
new dataset for predicting the answer of conjunctive query and conduct
experiments that show BIQE significantly outperforming state of the art
baselines.
</p>
"
A Fair and Privacy-Aware EV Discharging Strategy using Decentralized Whale Optimization Algorithm for Minimizing Cost of EVs and the EV Aggregator. (arXiv:2004.05280v2 [eess.SY] UPDATED),http://arxiv.org/abs/2004.05280,"<p>A key motivation to fasten roll-out of electric vehicles (EVs) to the market
is to implement Vehicle-to-Grid (V2G) functionalities. With V2G in place, EV
owners can have extra freedom to interact their battery energy with power
grids, namely by selling their energy to the grid when their EVs are not in
use. On the other hand, EV aggregators and utility companies can leverage the
flexibility of the collected energy to implement various ancillary services to
the grids, which may significantly reduce costs of, for instance, running
spinning reserve of traditional power plants on the grid side. However, this
extra freedom also poses practical challenges in terms of how to devise a
discharging strategy for a group of EVs that is fair and in some sense optimal.
In this paper, we present a new design of EV discharging strategy in a typical
V2G energy trading framework whilst leveraging the whale optimization algorithm
in a decentralized manner, a metaheuristic algorithm that has been shown
effective in solving large-scale centralized optimization problems. We
demonstrate that by using simple ideas of data shuffling and aggregation, one
can design an EV discharging strategy in a fair, optimal and privacy-aware
manner, where the privacy refers to the fact that no critical information of
EVs should be exchanged with the EV aggregator, and vice versa. The fairness
implies that a common discharge rate needs to be sought for all EVs so that no
one gets better benefits than others in the same V2G programme. Simulation
results are presented to illustrate the efficacy of our proposed system.
</p>
"
Cross-lingual Contextualized Topic Models with Zero-shot Learning. (arXiv:2004.07737v2 [cs.CL] UPDATED),http://arxiv.org/abs/2004.07737,"<p>Many data sets (e.g., reviews, forums, news, etc.) exist parallelly in
multiple languages. They all cover the same content, but the linguistic
differences make it impossible to use traditional, bag-of-word-based topic
models. Models have to be either single-language or suffer from a huge, but
extremely sparse vocabulary. Both issues can be addressed by transfer learning.
In this paper, we introduce a zero-shot cross-lingual topic model. Our model
learns topics on one language (here, English), and predicts them for unseen
documents in different languages (here, Italian, French, German, and
Portuguese). We evaluate the quality of the topic predictions for the same
document in different languages. Our results show that the transferred topics
are coherent and stable across languages, which suggests exciting future
research directions.
</p>
"
Elastic weight consolidation for better bias inoculation. (arXiv:2004.14366v2 [cs.CL] UPDATED),http://arxiv.org/abs/2004.14366,"<p>The biases present in training datasets have been shown to affect models for
sentence pair classification tasks such as natural language inference (NLI) and
fact verification. While fine-tuning models on additional data has been used to
mitigate them, a common issue is that of catastrophic forgetting of the
original training dataset. In this paper, we show that elastic weight
consolidation (EWC) allows fine-tuning of models to mitigate biases while being
less susceptible to catastrophic forgetting. In our evaluation on fact
verification and NLI stress tests, we show that fine-tuning with EWC dominates
standard fine-tuning, yielding models with lower levels of forgetting on the
original (biased) dataset for equivalent gains in accuracy on the fine-tuning
(unbiased) dataset.
</p>
"
IsoBN: Fine-Tuning BERT with Isotropic Batch Normalization. (arXiv:2005.02178v2 [cs.CL] UPDATED),http://arxiv.org/abs/2005.02178,"<p>Fine-tuning pre-trained language models (PTLMs), such as BERT and its better
variant RoBERTa, has been a common practice for advancing performance in
natural language understanding (NLU) tasks. Recent advance in representation
learning shows that isotropic (i.e., unit-variance and uncorrelated) embeddings
can significantly improve performance on downstream tasks with faster
convergence and better generalization. The isotropy of the pre-trained
embeddings in PTLMs, however, is relatively under-explored. In this paper, we
analyze the isotropy of the pre-trained [CLS] embeddings of PTLMs with
straightforward visualization, and point out two major issues: high variance in
their standard deviation, and high correlation between different dimensions. We
also propose a new network regularization method, isotropic batch normalization
(IsoBN) to address the issues, towards learning more isotropic representations
in fine-tuning by dynamically penalizing dominating principal components. This
simple yet effective fine-tuning method yields about 1.0 absolute increment on
the average of seven NLU tasks.
</p>
"
From industry-wide parameters to aircraft-centric on-flight inference: improving aeronautics performance prediction with machine learning. (arXiv:2005.05286v3 [stat.AP] UPDATED),http://arxiv.org/abs/2005.05286,"<p>Aircraft performance models play a key role in airline operations, especially
in planning a fuel-efficient flight. In practice, manufacturers provide
guidelines which are slightly modified throughout the aircraft life cycle via
the tuning of a single factor, enabling better fuel predictions. However this
has limitations, in particular they do not reflect the evolution of each
feature impacting the aircraft performance. Our goal here is to overcome this
limitation. The key contribution of the present article is to foster the use of
machine learning to leverage the massive amounts of data continuously recorded
during flights performed by an aircraft and provide models reflecting its
actual and individual performance. We illustrate our approach by focusing on
the estimation of the drag and lift coefficients from recorded flight data. As
these coefficients are not directly recorded, we resort to aerodynamics
approximations. As a safety check, we provide bounds to assess the accuracy of
both the aerodynamics approximation and the statistical performance of our
approach. We provide numerical results on a collection of machine learning
algorithms. We report excellent accuracy on real-life data and exhibit
empirical evidence to support our modelling, in coherence with aerodynamics
principles.
</p>
"
Increasing-Margin Adversarial (IMA) Training to Improve Adversarial Robustness of Neural Networks. (arXiv:2005.09147v3 [cs.CV] UPDATED),http://arxiv.org/abs/2005.09147,"<p>Convolutional neural network (CNN) has surpassed traditional methods for
med-ical image classification. However, CNN is vulnerable to adversarial
attacks which may lead to disastrous consequences in medical applications.
Although adversarial noises are usually generated by attack algorithms,
white-noise-induced adversarial samples can exist, and therefore the threats
are real. In this study, we propose a novel training method, named IMA, to
improve the robust-ness of CNN against adversarial noises. During training, the
IMA method in-creases the margins of training samples in the input space, i.e.,
moving CNN de-cision boundaries far away from the training samples to improve
robustness. The IMA method is evaluated on four publicly available datasets
under strong 100-PGD white-box adversarial attacks, and the results show that
the proposed meth-od significantly improved CNN classification accuracy on
noisy data while keep-ing a relatively high accuracy on clean data. We hope our
approach may facilitate the development of robust applications in medical
field.
</p>
"
Analyzing the Impact of Covid-19 Control Policies on Campus Occupancy and Mobility via Passive WiFi Sensing. (arXiv:2005.12050v3 [cs.CY] UPDATED),http://arxiv.org/abs/2005.12050,"<p>Mobile sensing has played a key role in providing digital solutions to aid
with COVID-19 containment policies. These solutions include, among other
efforts, enforcing social distancing and monitoring crowd movements in indoor
spaces. However, such solutions may not be effective without mass adoption. As
more and more countries reopen from lockdowns, there remains a pressing need to
minimize crowd movements and interactions, particularly in enclosed spaces.
This paper conjectures that analyzing user occupancy and mobility via deployed
WiFi infrastructure can help institutions monitor and maintain safety
compliance according to the public health guidelines. Using smartphones as a
proxy for user location, our analysis demonstrates how coarse-grained WiFi data
can sufficiently reflect indoor occupancy spectrum when different COVID-19
policies were enacted. Our work analyzes staff and students' mobility data from
three different university campuses. Two of these campuses are in Singapore,
and the third is in the Northeastern United States. Our results show that
online learning, split-team, and other space management policies effectively
lower occupancy. However, they do not change the mobility for individuals
transitioning between spaces. We demonstrate how this data source can be put to
practical application for institutional crowd control and discuss the
implications of our findings for policy-making.
</p>
"
Topological Drawings meet Classical Theorems from Convex Geometry. (arXiv:2005.12568v3 [math.CO] UPDATED),http://arxiv.org/abs/2005.12568,"<p>In this article we discuss classical theorems from Convex Geometry in the
context of topological drawings and beyond. In a simple topological drawing of
the complete graph $K_n$, any two edges share at most one point: either a
common vertex or a point where they cross. Triangles of simple topological
drawings can be viewed as convex sets. This gives a link to convex geometry.
</p>
<p>As our main result, we present a generalization of Kirchberger's Theorem that
is of purely combinatorial nature. It turned out that this classical theorem
also applies to ""generalized signotopes"" - a combinatorial generalization of
simple topological drawings, which we introduce and investigate in the course
of this article. As indicated by the name they are a generalization of
signotopes, a structure studied in the context of encodings for arrangements of
pseudolines.
</p>
<p>We also present a family of simple topological drawings with arbitrarily
large Helly number, and a new proof of a topological generalization of
Carath\'{e}odory's Theorem in the plane and discuss further classical theorems
from Convex Geometry in the context of simple topological drawings.
</p>
"
Spatial organisation of French research from the scholarly publication standpoint (1999-2017): Long-standing dynamics and policy-induced disorder. (arXiv:2005.13240v3 [cs.DL] UPDATED),http://arxiv.org/abs/2005.13240,"<p>In social processes, long-term trends can be influenced or disrupted by
various factors, including public policy. When public policies depend on a
misrepresentation of trends in the areas they are aimed at, they become random
and disruptive, which can be interpreted as a source of disorder. Here we
consider policies on the spatial organization of the French Higher Education
and Research system, which reflects the authorities' hypothesis that scientific
excellence is the prerogative of a few large urban agglomerations. By
geographically identifying all the French publications listed in the Web of
Science databases between 1999 and 2017, we highlight a spatial deconcentration
trend, which has slowed down in recent years due to a freezed growth of the
teaching force. This deconcentration continues, however, to sustain the growth
of scientific production in small and medium-sized towns. An examination of the
large conurbations shows the relative decline of sites that nevertheless have
been highlighted as examples to be followed by the Excellence policies
(Strasbourg among others). The number of students and faculty has grown less
there, and it is a plaussible explanation for the relative decline in
scientific production. We show that the publication output of a given site
depends directly and strongly on the number of researchers hosted there. Based
on precise data at the French level, our results confirm what is already known
at world scale. In conclusion, we question the amount of disorder resulting
from policies aligned with poorly assessed trends.
</p>
"
Analysis of Tree-Algorithms with Multi-Packet Reception. (arXiv:2005.13898v2 [cs.NI] UPDATED),http://arxiv.org/abs/2005.13898,"<p>In this paper, we analyze binary-tree algorithms in a setup in which the
receiver can perform multi-packet reception (MPR) of up to and including K
packets simultaneously. The analysis addresses both traffic-independent
performance as well as performance under Poisson arrivals. For the former case,
we show that the throughput, when normalized with respect to the assumed linear
increase in resources required to achieve K-MPR capability, tends to the same
value that holds for the single-reception setup. However, when coupled with
Poisson arrivals in the windowed access scheme, the normalized throughput
increases with K, and we present evidence that it asymptotically tends to 1. We
also provide performance results for the modified tree algorithm with K-MPR in
the clipped access scheme. To the best of our knowledge, this is the first
paper that provides an analytical treatment and a number of fundamental
insights in the performance of tree-algorithms with MPR.
</p>
"
Interferobot: aligning an optical interferometer by a reinforcement learning agent. (arXiv:2006.02252v2 [cs.RO] UPDATED),http://arxiv.org/abs/2006.02252,"<p>Limitations in acquiring training data restrict potential applications of
deep reinforcement learning (RL) methods to the training of real-world robots.
Here we train an RL agent to align a Mach-Zehnder interferometer, which is an
essential part of many optical experiments, based on images of interference
fringes acquired by a monocular camera. The agent is trained in a simulated
environment, without any hand-coded features or a priori information about the
physics, and subsequently transferred to a physical interferometer. Thanks to a
set of domain randomizations simulating uncertainties in physical measurements,
the agent successfully aligns this interferometer without any fine tuning,
achieving a performance level of a human expert.
</p>
"
Unifying Regularisation Methods for Continual Learning. (arXiv:2006.06357v2 [cs.LG] UPDATED),http://arxiv.org/abs/2006.06357,"<p>Continual Learning addresses the challenge of learning a number of different
tasks sequentially. The goal of maintaining knowledge of earlier tasks without
re-accessing them starkly conflicts with standard SGD training for artificial
neural networks. An influential method to tackle this problem without storing
old data are so-called regularisation approaches. They measure the importance
of each parameter for solving a given task and subsequently protect important
parameters from large changes. In the literature, three ways to measure
parameter importance have been put forward and they have inspired a large body
of follow-up work. Here, we present strong theoretical and empirical evidence
that these three methods, Elastic Weight Consolidation (EWC), Synaptic
Intelligence (SI) and Memory Aware Synapses (MAS), are surprisingly similar and
are all linked to the same theoretical quantity. Concretely, we show that,
despite stemming from very different motivations, both SI and MAS approximate
the square root of the Fisher Information, with the Fisher being the
theoretically justified basis of EWC. Moreover, we show that for SI the
relation to the Fisher -- and in fact its performance -- is due to a previously
unknown bias. On top of uncovering unknown similarities and unifying
regularisation approaches, we also demonstrate that our insights enable
practical performance improvements for large batch training.
</p>
"
Disentangled Representation Learning and Generation with Manifold Optimization. (arXiv:2006.07046v2 [cs.LG] UPDATED),http://arxiv.org/abs/2006.07046,"<p>Disentanglement is a useful property in representation learning which
increases the interpretability of generative models such as Variational
Auto-Encoders (VAE), Generative Adversarial Models, and their many variants.
Typically in such models, an increase in disentanglement performance is
traded-off with generation quality. In the context of latent space models, this
work presents a representation learning framework that explicitly promotes
disentanglement by encouraging orthogonal directions of variations. The
proposed objective is the sum of an auto-encoder error term along with a
Principal Component Analysis reconstruction error in the feature space. This
has an interpretation of a Restricted Kernel Machine with an interconnection
matrix on the Stiefel manifold. Our analysis shows that such a construction
promotes disentanglement by matching the principal directions in latent space
with the directions of orthogonal variation in data space. The training
algorithm involves a stochastic optimization method on the Stiefel manifold,
which increases only marginally the computing time compared to an analogous
VAE. Our theoretical discussion and various experiments show that the proposed
model improves over many VAE variants in terms of both generation quality and
disentangled representation learning.
</p>
"
Partial Extended Observability Certification and Optimal Design of Moving Horizon Estimators Under Uncertainties. (arXiv:2006.11112v2 [eess.SY] UPDATED),http://arxiv.org/abs/2006.11112,"<p>This paper addresses the observability analysis and the optimal design of
observation parameters in the presence of noisy measurements and parametric
uncertainties. The main underlying frameworks are the nonlinear constrained
moving horizon estimator design and the probabilistic certification via
randomized optimization. As the perfect observability concept is not relevant
under the considered uncertain and noisy context, the notion of almost
$\epsilon$-observability is introduced and a systematic procedure to assess its
satisfaction for a given system with a priori known measurement noise
statistics and parameter discrepancy is sketched. A nice feature in the
proposed framework is that the observability is not necessarily defined as the
ability to reconstruct the whole state, rather, the more general concept of
observation-target quantities is used so that one can analyze the precision
with which specific chosen expressions of the state and the parameters can be
reconstructed. The overall framework is exposed and validated through an
illustrative example.
</p>
"
Deep Implicit Coordination Graphs for Multi-agent Reinforcement Learning. (arXiv:2006.11438v2 [cs.LG] UPDATED),http://arxiv.org/abs/2006.11438,"<p>Multi-agent reinforcement learning (MARL) requires coordination to
efficiently solve certain tasks. Fully centralized control is often infeasible
in such domains due to the size of joint action spaces. Coordination graph
based formalization allows reasoning about the joint action based on the
structure of interactions. However, they often require domain expertise in
their design. This paper introduces the deep implicit coordination graph (DICG)
architecture for such scenarios. DICG consists of a module for inferring the
dynamic coordination graph structure which is then used by a graph neural
network based module to learn to implicitly reason about the joint actions or
values. DICG allows learning the tradeoff between full centralization and
decentralization via standard actor-critic methods to significantly improve
coordination for domains with large number of agents. We apply DICG to both
centralized-training-centralized-execution and
centralized-training-decentralized-execution regimes. We demonstrate that DICG
solves the relative overgeneralization pathology in predatory-prey tasks as
well as outperforms various MARL baselines on the challenging StarCraft II
Multi-agent Challenge (SMAC) and traffic junction environments.
</p>
"
Adaptive Discretization for Adversarial Lipschitz Bandits. (arXiv:2006.12367v2 [cs.LG] UPDATED),http://arxiv.org/abs/2006.12367,"<p>Lipschitz bandits is a prominent version of multi-armed bandits that studies
large, structured action spaces such as the [0,1] interval, where similar
actions are guaranteed to have similar rewards. A central theme here is the
adaptive discretization of the action space, which gradually ""zooms in"" on the
more promising regions thereof. The goal is to take advantage of ""nicer""
problem instances, while retaining near-optimal worst-case performance. While
the stochastic version of the problem is well-understood, the general version
with adversarial rewards is not.
</p>
<p>We provide the first algorithm for adaptive discretization in the adversarial
version, and derive instance-dependent regret bounds. In particular, we recover
the worst-case optimal regret bound for the adversarial version, and the
instance-dependent regret bound for the stochastic version. Further, an
application of our algorithm to dynamic pricing (where a seller repeatedly
adjusts prices for a product) enjoys these regret bounds without any smoothness
assumptions.
</p>
"
Invertible Concept-based Explanations for CNN Models with Non-negative Concept Activation Vectors. (arXiv:2006.15417v3 [cs.CV] UPDATED),http://arxiv.org/abs/2006.15417,"<p>Convolutional neural network (CNN) models for computer vision are powerful
but lack explainability in their most basic form. This deficiency remains a key
challenge when applying CNNs in important domains. Recent work for explanations
through feature importance of approximate linear models has moved from
input-level features (pixels or segments) to features from mid-layer feature
maps in the form of concept activation vectors (CAVs). CAVs contain
concept-level information and could be learnt via clustering. In this work, we
rethink the ACE algorithm of Ghorbani et al., proposing an alternative
inevitable concept-based explanation (ICE) framework to overcome its
shortcomings. Based on the requirements of fidelity (approximate models to
target models) and interpretability (being meaningful to people), we design
measurements and evaluate a range of matrix factorization methods with our
framework. We find that \emph{non-negative concept activation vectors} (NCAVs)
from non-negative matrix factorization provide superior performance in
interpretability and fidelity based on computational and human subject
experiments. Our framework provides both local and global concept-level
explanations for pre-trained CNN models.
</p>
"
Multimodal Text Style Transfer for Outdoor Vision-and-Language Navigation. (arXiv:2007.00229v3 [cs.CL] UPDATED),http://arxiv.org/abs/2007.00229,"<p>One of the most challenging topics in Natural Language Processing (NLP) is
visually-grounded language understanding and reasoning. Outdoor
vision-and-language navigation (VLN) is such a task where an agent follows
natural language instructions and navigates a real-life urban environment. Due
to the lack of human-annotated instructions that illustrate intricate urban
scenes, outdoor VLN remains a challenging task to solve. This paper introduces
a Multimodal Text Style Transfer (MTST) learning approach and leverages
external multimodal resources to mitigate data scarcity in outdoor navigation
tasks. We first enrich the navigation data by transferring the style of the
instructions generated by Google Maps API, then pre-train the navigator with
the augmented external outdoor navigation dataset. Experimental results show
that our MTST learning approach is model-agnostic, and our MTST approach
significantly outperforms the baseline models on the outdoor VLN task,
improving task completion rate by 8.7% relatively on the test set.
</p>
"
Knowledge-Aware Language Model Pretraining. (arXiv:2007.00655v2 [cs.CL] UPDATED),http://arxiv.org/abs/2007.00655,"<p>How much knowledge do pretrained language models hold? Recent research
observed that pretrained transformers are adept at modeling semantics but it is
unclear to what degree they grasp human knowledge, or how to ensure they do so.
In this paper we incorporate knowledge-awareness in language model pretraining
without changing the transformer architecture, inserting explicit knowledge
layers, or adding external storage of semantic information. Rather, we simply
signal the existence of entities to the input of the transformer in
pretraining, with an entity-extended tokenizer; and at the output, with an
additional entity prediction task. Our experiments show that solely by adding
these entity signals in pretraining, significantly more knowledge is packed
into the transformer parameters: we observe improved language modeling
accuracy, factual correctness in LAMA knowledge probing tasks, and semantics in
the hidden representations through edge probing.We also show that our
knowledge-aware language model (KALM) can serve as a drop-in replacement for
GPT-2 models, significantly improving downstream tasks like zero-shot
question-answering with no task-related training.
</p>
"
MPC Protocol for G-module and its Application in Secure Compare and ReLU. (arXiv:2007.03975v3 [cs.CR] UPDATED),http://arxiv.org/abs/2007.03975,"<p>Secure comparison and secure selection are two fundamental MPC (secure
Multi-Party Computation) protocols. One important application of these
protocols is the secure ReLU and DReLU computation in privacy preserving deep
learning. In this paper, we introduce G-module, a mathematics tool, to
re-design such protocols. In mathematics, given a group G, a G-module is an
abelian group M on which G acts compatibly with the abelian group structure on
M.
</p>
<p>We design three secure protocols for three G-module operations. i.e.
""G-module action"", ""Cross G-module action"" and ""G-module recover"". As far as we
know, this is the first work on secure G-module operations. Based on them, we
design secure comparison, selection, ReLU and DReLU protocols, which improve
communication efficiency by 2X to 10X compared with state of arts. Our
protocols are very computation efficient too. They do not require public key
operations or any other expensive operations.
</p>
"
Algorithmic applications of the corestriction of central simple algebras. (arXiv:2007.06981v4 [math.NT] UPDATED),http://arxiv.org/abs/2007.06981,"<p>Let $L$ be a separable quadratic extension of either $\mathbb{Q}$ or
$\mathbb{F}_q(t)$. We propose efficient algorithms for finding isomorphisms
between quaternion algebras over $L$. Our techniques are based on computing
maximal one-sided ideals of the corestriction of a central simple $L$-algebra.
In order to obtain efficient algorithms in the characteristic 2 case, we
propose an algorithm for finding nontrivial zeros of a regular quadratic form
in four variables over $\mathbb{F}_{2^k}(t)$.
</p>
"
Relaxed-Responsibility Hierarchical Discrete VAEs. (arXiv:2007.07307v2 [stat.ML] UPDATED),http://arxiv.org/abs/2007.07307,"<p>Successfully training Variational Autoencoders (VAEs) with a hierarchy of
discrete latent variables remains an area of active research.
</p>
<p>Vector-Quantised VAEs are a powerful approach to discrete VAEs, but naive
hierarchical extensions can be unstable when training. Leveraging insights from
classical methods of inference we introduce \textit{Relaxed-Responsibility
Vector-Quantisation}, a novel way to parameterise discrete latent variables, a
refinement of relaxed Vector-Quantisation that gives better performance and
more stable training. This enables a novel approach to hierarchical discrete
variational autoencoders with numerous layers of latent variables (here up to
32) that we train end-to-end. Within hierarchical probabilistic deep generative
models with discrete latent variables trained end-to-end, we achieve
state-of-the-art bits-per-dim results for various standard datasets. % Unlike
discrete VAEs with a single layer of latent variables, we can produce samples
by ancestral sampling: it is not essential to train a second autoregressive
generative model over the learnt latent representations to then sample from and
then decode. % Moreover, that latter approach in these deep hierarchical models
would require thousands of forward passes to generate a single sample. Further,
we observe different layers of our model become associated with different
aspects of the data.
</p>
"
An empirical study of Linespots: A novel past-fault algorithm. (arXiv:2007.09394v2 [cs.SE] UPDATED),http://arxiv.org/abs/2007.09394,"<p>This paper proposes the novel past-faults fault prediction algorithm
Linespots, based on the Bugspots algorithm. We analyze the predictive
performance and runtime of Linespots compared to Bugspots with an empirical
study using the most significant self-built dataset as of now, including
high-quality samples for validation. As a novelty in fault prediction, we use
Bayesian data analysis and Directed Acyclic Graphs to model the effects. We
found consistent improvements in the predictive performance of Linespots over
Bugspots for all seven evaluation metrics. We conclude that Linespots should be
used over Bugspots in all cases where no real-time performance is necessary.
</p>
"
Calibration of Google Trends Time Series. (arXiv:2007.13861v5 [cs.SI] UPDATED),http://arxiv.org/abs/2007.13861,"<p>Google Trends is a tool that allows researchers to analyze the popularity of
Google search queries across time and space. In a single request, users can
obtain time series for up to 5 queries on a common scale, normalized to the
range from 0 to 100 and rounded to integer precision. Despite the overall value
of Google Trends, rounding causes major problems, to the extent that entirely
uninformative, all-zero time series may be returned for unpopular queries when
requested together with more popular queries. We address this issue by
proposing Google Trends Anchor Bank (G-TAB), an efficient solution for the
calibration of Google Trends data. Our method expresses the popularity of an
arbitrary number of queries on a common scale without being compromised by
rounding errors. The method proceeds in two phases. In the offline
preprocessing phase, an ""anchor bank"" is constructed, a set of queries spanning
the full spectrum of popularity, all calibrated against a common reference
query by carefully chaining together multiple Google Trends requests. In the
online deployment phase, any given search query is calibrated by performing an
efficient binary search in the anchor bank. Each search step requires one
Google Trends request, but few steps suffice, as we demonstrate in an empirical
evaluation. We make our code publicly available as an easy-to-use library at
https://github.com/epfl-dlab/GoogleTrendsAnchorBank.
</p>
"
A Process Mining Software Comparison. (arXiv:2007.14038v3 [cs.SE] UPDATED),http://arxiv.org/abs/2007.14038,"<p>www.processmining-software.com is a dedicated website for process mining
software comparison and was developed to give practitioners and researchers an
overview of commercial tools available on the market. Based on literature
review and experimental tool testing, a set of criteria was developed in order
to assess the tools' functional capabilities in an objective manner. With our
publicly accessible website, we intend to increase the transparency of tool
functionality. Being an academic endeavour, the non-commercial nature of the
study ensures a less biased assessment as compared with reports from analyst
firms.
</p>
"
An Empirical Survey of Data Augmentation for Time Series Classification with Neural Networks. (arXiv:2007.15951v2 [cs.LG] UPDATED),http://arxiv.org/abs/2007.15951,"<p>In recent times, deep artificial neural networks have achieved many successes
in pattern recognition. Part of this success can be attributed to the reliance
on big data to increase generalization. However, in the field of time series
recognition, many datasets are often very small. One method of addressing this
problem is through the use of data augmentation. In this paper, we survey data
augmentation techniques for time series and their application to time series
classification with neural networks. We outline four families of time series
data augmentation, including transformation-based methods, pattern mixing,
generative models, and decomposition methods, and detail their taxonomy.
Furthermore, we empirically evaluate 12 time series data augmentation methods
on 128 time series classification datasets with 6 different types of neural
networks. Through the results, we are able to analyze the characteristics,
advantages and disadvantages, and recommendations of each data augmentation
method. This survey aims to help in the selection of time series data
augmentation for neural network applications.
</p>
"
Noise-Response Analysis of Deep Neural Networks Quantifies Robustness and Fingerprints Structural Malware. (arXiv:2008.00123v2 [cs.LG] UPDATED),http://arxiv.org/abs/2008.00123,"<p>The ubiquity of deep neural networks (DNNs), cloud-based training, and
transfer learning is giving rise to a new cybersecurity frontier in which
unsecure DNNs have `structural malware' (i.e., compromised weights and
activation pathways). In particular, DNNs can be designed to have backdoors
that allow an adversary to easily and reliably fool an image classifier by
adding a pattern of pixels called a trigger. It is generally difficult to
detect backdoors, and existing detection methods are computationally expensive
and require extensive resources (e.g., access to the training data). Here, we
propose a rapid feature-generation technique that quantifies the robustness of
a DNN, `fingerprints' its nonlinearity, and allows us to detect backdoors (if
present). Our approach involves studying how a DNN responds to noise-infused
images with varying noise intensity, which we summarize with titration curves.
We find that DNNs with backdoors are more sensitive to input noise and respond
in a characteristic way that reveals the backdoor and where it leads (its
`target'). Our empirical results demonstrate that we can accurately detect
backdoors with high confidence orders-of-magnitude faster than existing
approaches (seconds versus hours).
</p>
"
Accuracy and Fairness Trade-offs in Machine Learning: A Stochastic Multi-Objective Approach. (arXiv:2008.01132v2 [cs.LG] UPDATED),http://arxiv.org/abs/2008.01132,"<p>In the application of machine learning to real-life decision-making systems,
e.g., credit scoring and criminal justice, the prediction outcomes might
discriminate against people with sensitive attributes, leading to unfairness.
The commonly used strategy in fair machine learning is to include fairness as a
constraint or a penalization term in the minimization of the prediction loss,
which ultimately limits the information given to decision-makers. In this
paper, we introduce a new approach to handle fairness by formulating a
stochastic multi-objective optimization problem for which the corresponding
Pareto fronts uniquely and comprehensively define the accuracy-fairness
trade-offs. We have then applied a stochastic approximation-type method to
efficiently obtain well-spread and accurate Pareto fronts, and by doing so we
can handle training data arriving in a streaming way.
</p>
"
Point Proposal Network: Accelerating Point Source Detection Through Deep Learning. (arXiv:2008.02093v2 [cs.CV] UPDATED),http://arxiv.org/abs/2008.02093,"<p>Point source detection techniques are used to identify and localise point
sources in radio astronomical surveys. With the development of the Square
Kilometre Array (SKA) telescope, survey images will see a massive increase in
size from Gigapixels to Terapixels. Point source detection has already proven
to be a challenge in recent surveys performed by SKA pathfinder telescopes.
This paper proposes the Point Proposal Network (PPN): a point source detector
that utilises deep convolutional neural networks for fast source detection.
Results measured on simulated MeerKAT images show that, although less precise
when compared to leading alternative approaches, PPN performs source detection
faster and is able to scale to large images, unlike the alternative approaches.
</p>
"
Data-driven reduced-order models via regularized operator inference for a single-injector combustion process. (arXiv:2008.02862v2 [cs.CE] UPDATED),http://arxiv.org/abs/2008.02862,"<p>This paper derives predictive reduced-order models for rocket engine
combustion dynamics via Operator Inference, a scientific machine learning
approach that blends data-driven learning with physics-based modeling. The
non-intrusive nature of the approach enables variable transformations that
expose system structure. The specific contribution of this paper is to advance
the formulation robustness and algorithmic scalability of the Operator
Inference approach. Regularization is introduced to the formulation to avoid
over-fitting. The task of determining an optimal regularization is posed as an
optimization problem that balances training error and stability of long-time
integration dynamics. A scalable algorithm and open-source implementation are
presented, then demonstrated for a single-injector rocket combustion example.
This example exhibits rich dynamics that are difficult to capture with
state-of-the-art reduced models. With appropriate regularization and an
informed selection of learning variables, the reduced-order models exhibit high
accuracy in re-predicting the training regime and acceptable accuracy in
predicting future dynamics, while achieving close to a million times speedup in
computational cost. When compared to a state-of-the-art model reduction method,
the Operator Inference models provide the same or better accuracy at
approximately one thousandth of the computational cost.
</p>
"
On construction and (non)existence of $c$-(almost) perfect nonlinear functions. (arXiv:2008.03953v3 [math.CO] UPDATED),http://arxiv.org/abs/2008.03953,"<p>Functions with low differential uniformity have relevant applications in
cryptography. Recently, functions with low $c$-differential uniformity
attracted lots of attention. In particular, so-called APcN and PcN functions
(generalization of APN and PN functions) have been investigated. Here, we
provide a characterization of such functions via quadratic polynomials as well
as non-existence results.
</p>
"
Distributed Stochastic Subgradient Optimization Algorithms Over Random and Noisy Networks. (arXiv:2008.08796v4 [eess.SY] UPDATED),http://arxiv.org/abs/2008.08796,"<p>We study distributed stochastic optimization by networked nodes to
cooperatively minimize a sum of convex cost functions. The network is modeled
by a sequence of time-varying random digraphs with each node representing a
local optimizer and each edge representing a communication link. We consider
the distributed subgradient optimization algorithm with noisy measurements of
local cost functions' subgradients, additive and multiplicative noises among
information exchanging between each pair of nodes. By stochastic Lyapunov
method, convex analysis, algebraic graph theory and martingale convergence
theory, it is proved that if the local subgradient functions grow linearly and
the sequence of digraphs is conditionally balanced and uniformly conditionally
jointly connected, then proper algorithm step sizes can be designed so that all
nodes' states converge to the global optimal solution almost surely.
</p>
"
Active Disturbance Rejection Control Design with Suppression of Sensor Noise Effects in Application to DC-DC Buck Power Converter. (arXiv:2009.02948v2 [eess.SY] UPDATED),http://arxiv.org/abs/2009.02948,"<p>The performance of active disturbance rejection control (ADRC) algorithms can
be limited in practice by high-frequency measurement noise. In this work, this
problem is addressed by transforming the high-gain extended state observer
(ESO), which is the inherent element of ADRC, into a new cascade observer
structure. Set of experiments, performed on a DC-DC buck power converter
system, show that the new cascade ESO design, compared to the conventional
approach, effectively suppresses the detrimental effect of sensor noise
over-amplification while increasing the estimation/control performance. The
proposed design is also analyzed with a low-pass filter at the converter
output, which is a common technique for reducing measurement noise in
industrial applications.
</p>
"
Ebb-and-Flow Protocols: A Resolution of the Availability-Finality Dilemma. (arXiv:2009.04987v3 [cs.CR] UPDATED),http://arxiv.org/abs/2009.04987,"<p>The CAP theorem says that no blockchain can be live under dynamic
participation and safe under temporary network partitions. To resolve this
availability-finality dilemma, we formulate a new class of flexible consensus
protocols, ebb-and-flow protocols, which support a full dynamically available
ledger in conjunction with a finalized prefix ledger. The finalized ledger
falls behind the full ledger when the network partitions but catches up when
the network heals. Gasper, the current candidate protocol for Ethereum 2.0's
beacon chain, combines the finality gadget Casper FFG with the LMD GHOST fork
choice rule and aims to achieve this property. However, we discovered an attack
in the standard synchronous network model, highlighting a general difficulty
with existing finality-gadget-based designs. We present a construction of
provably secure ebb-and-flow protocols with optimal resilience. Nodes run an
off-the-shelf dynamically available protocol, take snapshots of the growing
available ledger, and input them into a separate off-the-shelf BFT protocol to
finalize a prefix. We explore connections with flexible BFT and improve upon
the state-of-the-art for that problem.
</p>
"
Contrastive Triple Extraction with Generative Transformer. (arXiv:2009.06207v4 [cs.CL] UPDATED),http://arxiv.org/abs/2009.06207,"<p>Triple extraction is an essential task in information extraction for natural
language processing and knowledge graph construction. In this paper, we revisit
the end-to-end triple extraction task for sequence generation. Since generative
triple extraction may struggle to capture long-term dependencies and generate
unfaithful triples, we introduce a novel model, contrastive triple extraction
with a generative transformer. Specifically, we introduce a single shared
transformer module for encoder-decoder-based generation. To generate faithful
results, we propose a novel triplet contrastive training object. Moreover, we
introduce two mechanisms to further improve model performance (i.e., batch-wise
dynamic attention-masking and triple-wise calibration). Experimental results on
three datasets (i.e., NYT, WebNLG, and MIE) show that our approach achieves
better performance than that of baselines.
</p>
"
A categorical duality for algebras of partial functions. (arXiv:2009.07895v2 [math.RA] UPDATED),http://arxiv.org/abs/2009.07895,"<p>We prove a categorical duality between a class of abstract algebras of
partial functions and a class of (small) topological categories. The algebras
are the isomorphs of collections of partial functions closed under the
operations of composition, antidomain, range, and preferential union (or
'override'). The topological categories are those whose space of objects is a
Stone space, source map is a local homeomorphism, target map is open, and all
of whose arrows are epimorphisms.
</p>
"
A General Framework for the Security Analysis of Blockchain Protocols. (arXiv:2009.09480v3 [cs.DC] UPDATED),http://arxiv.org/abs/2009.09480,"<p>Blockchain protocols differ in fundamental ways, including the mechanics of
selecting users to produce blocks (e.g., proof-of-work vs. proof-of-stake) and
the method to establish consensus (e.g., longest chain rules vs. Byzantine
fault-tolerant (BFT) inspired protocols). These fundamental differences have
hindered ""apples-to-apples"" comparisons between different categories of
blockchain protocols and, in turn, the development of theory to formally
discuss their relative merits.
</p>
<p>This paper presents a parsimonious abstraction sufficient for capturing and
comparing properties of many well-known permissionless blockchain protocols,
simultaneously capturing essential properties of both proof-of-work (PoW) and
proof-of-stake (PoS) protocols, and of both longest-chain-type and BFT-type
protocols. Our framework blackboxes the precise mechanics of the user selection
process, allowing us to isolate the properties of the selection process that
are significant for protocol design.
</p>
<p>We demonstrate the utility of our general framework with several concrete
results:
</p>
<p>1. We prove a CAP-type impossibility theorem asserting that liveness with an
unknown level of participation rules out security in a partially synchronous
setting.
</p>
<p>2. Delving deeper into the partially synchronous setting, we prove that a
necessary and sufficient condition for security is the production of
""certificates,"" meaning stand-alone proofs of block confirmation.
</p>
<p>3. Restricting to synchronous settings, we prove that typical protocols with
a known level of participation (including longest chain-type PoS protocols) can
be adapted to provide certificates, but those with an unknown level of
participation cannot.
</p>
<p>4. Finally, we use our framework to articulate a modular two-step approach to
blockchain security analysis that effectively reduces the permissionless case
to the permissioned case.
</p>
"
Semi-supervised Semantic Segmentation of Prostate and Organs-at-Risk on 3D Pelvic CT Images. (arXiv:2009.09571v3 [cs.CV] UPDATED),http://arxiv.org/abs/2009.09571,"<p>Automated segmentation can assist radiotherapy treatment planning by saving
manual contouring efforts and reducing intra-observer and inter-observer
variations. The recent development of deep learning approaches has revoluted
medical data processing, including semantic segmentation, by dramatically
improving performance. However, training effective deep learning models usually
require a large amount of high-quality labeled data, which are often costly to
collect. We developed a novel semi-supervised adversarial deep learning
approach for 3D pelvic CT image semantic segmentation. Unlike supervised deep
learning methods, the new approach can utilize both annotated and un-annotated
data for training. It generates un-annotated synthetic data by a data
augmentation scheme using generative adversarial networks (GANs). We applied
the new approach to segmenting multiple organs in male pelvic CT images, where
CT images without annotations and GAN-synthesized un-annotated images were used
in semi-supervised learning. Experimental results, evaluated by three metrics
(Dice similarity coefficient, average Hausdorff distance, and average surface
Hausdorff distance), showed that the new method achieved either comparable
performance with substantially fewer annotated images or better performance
with the same amount of annotated data, outperforming the existing
state-of-the-art methods.
</p>
"
Dark Patterns and the Legal Requirements of Consent Banners: An Interaction Criticism Perspective. (arXiv:2009.10194v2 [cs.HC] UPDATED),http://arxiv.org/abs/2009.10194,"<p>User engagement with data privacy and security through consent banners has
become a ubiquitous part of interacting with internet services. While previous
work has addressed consent banners from either interaction design, legal, and
ethics-focused perspectives, little research addresses the connections among
multiple disciplinary approaches, including tensions and opportunities that
transcend disciplinary boundaries. In this paper, we draw together perspectives
and commentary from HCI, design, privacy and data protection, and legal
research communities, using the language and strategies of ""dark patterns"" to
perform an interaction criticism reading of three different types of consent
banners. Our analysis builds upon designer, interface, user, and social context
lenses to raise tensions and synergies that arise together in complex,
contingent, and conflicting ways in the act of designing consent banners. We
conclude with opportunities for transdisciplinary dialogue across legal,
ethical, computer science, and interactive systems scholarship to translate
matters of ethical concern into public policy.
</p>
"
The Ultimate DataFlow for Ultimate SuperComputers-on-a-Chips. (arXiv:2009.10593v4 [cs.DC] UPDATED),http://arxiv.org/abs/2009.10593,"<p>This article starts from the assumption that near future 100BTransistor
SuperComputers-on-a-Chip will include N big multi-core processors, 1000N small
many-core processors, a TPU-like fixed-structure systolic array accelerator for
the most frequently used Machine Learning algorithms needed in bandwidth-bound
applications and a flexible-structure reprogrammable accelerator for less
frequently used Machine Learning algorithms needed in latency-critical
applications.
</p>
"
EEG based Major Depressive disorder and Bipolar disorder detection using Neural Networks: A review. (arXiv:2009.13402v2 [q-bio.NC] UPDATED),http://arxiv.org/abs/2009.13402,"<p>Mental disorders represent critical public health challenges as they are
leading contributors to the global burden of disease and intensely influence
social and financial welfare of individuals. The present comprehensive review
concentrate on the two mental disorders: Major depressive Disorder (MDD) and
Bipolar Disorder (BD) with noteworthy publications during the last ten years.
There is a big need nowadays for phenotypic characterization of psychiatric
disorders with biomarkers. Electroencephalography (EEG) signals could offer a
rich signature for MDD and BD and then they could improve understanding of
pathophysiological mechanisms underling these mental disorders. In this review,
we focus on the literature works adopting neural networks fed by EEG signals.
Among those studies using EEG and neural networks, we have discussed a variety
of EEG based protocols, biomarkers and public datasets for depression and
bipolar disorder detection. We conclude with a discussion and valuable
recommendations that will help to improve the reliability of developed models
and for more accurate and more deterministic computational intelligence based
systems in psychiatry. This review will prove to be a structured and valuable
initial point for the researchers working on depression and bipolar disorders
recognition by using EEG signals.
</p>
"
Graph Neural Networks with Heterophily. (arXiv:2009.13566v2 [cs.LG] UPDATED),http://arxiv.org/abs/2009.13566,"<p>Graph Neural Networks (GNNs) have proven to be useful for many different
practical applications. However, many existing GNN models have implicitly
assumed homophily among the nodes connected in the graph, and therefore have
largely overlooked the important setting of heterophily, where most connected
nodes are from different classes. In this work, we propose a novel framework
called CPGNN that generalizes GNNs for graphs with either homophily or
heterophily. The proposed framework incorporates an interpretable compatibility
matrix for modeling the heterophily or homophily level in the graph, which can
be learned in an end-to-end fashion, enabling it to go beyond the assumption of
strong homophily. Theoretically, we show that replacing the compatibility
matrix in our framework with the identity (which represents pure homophily)
reduces to GCN. Our extensive experiments demonstrate the effectiveness of our
approach in more realistic and challenging experimental settings with
significantly less training data compared to previous works: CPGNN variants
achieve state-of-the-art results in heterophily settings with or without
contextual node features, while maintaining comparable performance in homophily
settings.
</p>
"
Engineering In-place (Shared-memory) Sorting Algorithms. (arXiv:2009.13569v2 [cs.DC] UPDATED),http://arxiv.org/abs/2009.13569,"<p>We present sorting algorithms that represent the fastest known techniques for
a wide range of input sizes, input distributions, data types, and machines. A
part of the speed advantage is due to the feature to work in-place. Previously,
the in-place feature often implied performance penalties. Our main algorithmic
contribution is a blockwise approach to in-place data distribution that is
provably cache-efficient. We also parallelize this approach taking dynamic load
balancing and memory locality into account. Our comparison-based algorithm,
In-place Superscalar Samplesort (IPS$^4$o), combines this technique with
branchless decision trees. By taking cases with many equal elements into
account and by adapting the distribution degree dynamically, we obtain a highly
robust algorithm that outperforms the best in-place parallel comparison-based
competitor by almost a factor of three. IPS$^4$o also outperforms the best
comparison-based competitors in the in-place or not in-place, parallel or
sequential settings. IPS$^4$o even outperforms the best integer sorting
algorithms in a wide range of situations. In many of the remaining cases (often
involving near-uniform input distributions, small keys, or a sequential
setting), our new in-place radix sorter turns out to be the best algorithm.
Claims to have the, in some sense, ""best"" sorting algorithm can be found in
many papers which cannot all be true. Therefore, we base our conclusions on
extensive experiments involving a large part of the cross product of 21
state-of-the-art sorting codes, 6 data types, 10 input distributions, 4
machines, 4 memory allocation strategies, and input sizes varying over 7 orders
of magnitude. This confirms the robust performance of our algorithms while
revealing major performance problems in many competitors outside the concrete
set of measurements reported in the associated publications.
</p>
"
On Statistical Discrimination as a Failure of Social Learning: A Multi-Armed Bandit Approach. (arXiv:2010.01079v4 [econ.TH] UPDATED),http://arxiv.org/abs/2010.01079,"<p>We analyze statistical discrimination using a multi-armed bandit model where
myopic firms face candidate workers arriving with heterogeneous observable
characteristics. The association between the worker's skill and characteristics
is unknown ex ante; thus, firms need to learn it. In such an environment,
laissez-faire may result in a highly unfair and inefficient outcome -- myopic
firms are reluctant to hire minority workers because the lack of data about
minority workers prevents accurate estimation of their performance.
Consequently, minority groups could be perpetually underestimated -- they are
never hired, and therefore, data about them is never accumulated. We proved
that this problem becomes more serious when the population ratio is imbalanced,
as is the case in many extant discrimination problems. We consider two
affirmative-action policies for solving this dilemma: One is a subsidy rule
that is based on the popular upper confidence bound algorithm, and another is
the Rooney Rule, which requires firms to interview at least one minority worker
for each hiring opportunity. Our results indicate temporary affirmative actions
are effective for statistical discrimination caused by data insufficiency.
</p>
"
Regret Guarantees for Online Receding Horizon Learning Control. (arXiv:2010.07269v8 [math.OC] UPDATED),http://arxiv.org/abs/2010.07269,"<p>In this paper we provide provable regret guarantees for an online learning
receding horizon type control policy in a setting where the system to be
controlled is an unknown linear dynamical system, the cost for the controller
is a general additive function over a finite period $T$, and there exist
control input constraints that when violated incur an additional cost. We show
that the learning based receding horizon control policy achieves the regret of
$\tilde{O}(T^{3/4})$ for both the controller's cost and cumulative constraint
violation w.r.t the baseline receding horizon control policy that has full
knowledge of the system.
</p>
"
Entropic proofs of Singleton bounds for quantum error-correcting codes. (arXiv:2010.07902v2 [quant-ph] UPDATED),http://arxiv.org/abs/2010.07902,"<p>We show that a relatively simple reasoning using von Neumann entropy
inequalities yields a robust proof of the quantum Singleton bound for quantum
error-correcting codes (QECC). For entanglement-assisted quantum
error-correcting codes (EAQECC) and catalytic codes (CQECC), the generalised
quantum Singleton bound was believed to hold for many years until recently one
of us found a counterexample [MG, <a href=""/abs/2007.01249"">arXiv:2007.01249</a>]. Here, we rectify this
state of affairs by proving the correct generalised quantum Singleton bound for
CQECC, extending the above-mentioned proof method for QECC; we also prove
information-theoretically tight bounds on the entanglement-communication
tradeoff for EAQECC. All of the bounds relate block length $n$ and code length
$k$ for given minimum distance $d$ and we show that they are robust, in the
sense that they hold with small perturbations for codes which only correct most
of the erasure errors of less than $d$ letters. In contrast to the classical
case, the bounds take on qualitatively different forms depending on whether the
minimum distance is smaller or larger than half the block length. We also
provide a propagation rule, where any pure QECC yields an EAQECC with the same
distance and dimension but of shorter block length.
</p>
"
Optimal Index Assignment for Scalar Quantizers and M-PSK via a Discrete Convolution-Rearrangement Inequality. (arXiv:2010.10300v3 [cs.IT] UPDATED),http://arxiv.org/abs/2010.10300,"<p>This paper investigates the problem of finding an optimal nonbinary index
assignment from \(M\) quantization levels of a maximum entropy scalar quantizer
to \(M\)-PSK symbols transmitted over a symmetric memoryless channel with
additive noise following decreasing probability density function (such as the
AWGN channel) so as to minimize the channel mean-squared distortion. The
so-called zigzag mapping under maximum-likelihood (ML) decoding was known to be
asymptotically optimal, but the problem of determining the optimal index
assignment for any given signal-to-noise ratio (SNR) is still open. Based on a
generalized version of the Hardy-Littlewood convolution-rearrangement
inequality, we prove that the zigzag mapping under ML decoding is optimal for
all SNRs. It is further proved that the same optimality results also hold under
minimum mean-square-error (MMSE) decoding. Numerical results are presented to
verify our optimality results and to demonstrate the performance gain of the
optimal \(M\)-ary index assignment over the state-of-the-art binary counterpart
for the case of \(8\)-PSK over the AWGN channel.
</p>
"
FastEmit: Low-latency Streaming ASR with Sequence-level Emission Regularization. (arXiv:2010.11148v2 [eess.AS] UPDATED),http://arxiv.org/abs/2010.11148,"<p>Streaming automatic speech recognition (ASR) aims to emit each hypothesized
word as quickly and accurately as possible. However, emitting fast without
degrading quality, as measured by word error rate (WER), is highly challenging.
Existing approaches including Early and Late Penalties and Constrained
Alignments penalize emission delay by manipulating per-token or per-frame
probability prediction in sequence transducer models. While being successful in
reducing delay, these approaches suffer from significant accuracy regression
and also require additional word alignment information from an existing model.
In this work, we propose a sequence-level emission regularization method, named
FastEmit, that applies latency regularization directly on per-sequence
probability in training transducer models, and does not require any alignment.
We demonstrate that FastEmit is more suitable to the sequence-level
optimization of transducer models for streaming ASR by applying it on various
end-to-end streaming ASR networks including RNN-Transducer,
Transformer-Transducer, ConvNet-Transducer and Conformer-Transducer. We achieve
150-300 ms latency reduction with significantly better accuracy over previous
techniques on a Voice Search test set. FastEmit also improves streaming ASR
accuracy from 4.4%/8.9% to 3.1%/7.5% WER, meanwhile reduces 90th percentile
latency from 210 ms to only 30 ms on LibriSpeech.
</p>
"
HateBERT: Retraining BERT for Abusive Language Detection in English. (arXiv:2010.12472v2 [cs.CL] UPDATED),http://arxiv.org/abs/2010.12472,"<p>In this paper, we introduce HateBERT, a re-trained BERT model for abusive
language detection in English. The model was trained on RAL-E, a large-scale
dataset of Reddit comments in English from communities banned for being
offensive, abusive, or hateful that we have collected and made available to the
public. We present the results of a detailed comparison between a general
pre-trained language model and the abuse-inclined version obtained by
retraining with posts from the banned communities on three English datasets for
offensive, abusive language and hate speech detection tasks. In all datasets,
HateBERT outperforms the corresponding general BERT model. We also discuss a
battery of experiments comparing the portability of the generic pre-trained
language model and its corresponding abusive language-inclined counterpart
across the datasets, indicating that portability is affected by compatibility
of the annotated phenomena.
</p>
"
Overlapping Domain Decomposition Methods for Ptychographic Imaging. (arXiv:2011.00162v2 [math.NA] UPDATED),http://arxiv.org/abs/2011.00162,"<p>In ptychography experiments, redundant scanning is usually required to
guarantee the stable recovery, such that a huge amount of frames are generated,
and thus it poses a great demand of parallel computing in order to solve this
large-scale inverse problem. In this paper, we propose the overlapping Domain
Decomposition Methods(DDMs) to solve the nonconvex optimization problem in
ptychographic imaging. They decouple the problem defined on the whole domain
into subproblems only defined on the subdomains with synchronizing information
in the overlapping regions of these subdomains,thus leading to highly parallel
algorithms with good load balance. More specifically, for the nonblind recovery
(with known probe in advance), by enforcing the continuity of the overlapping
regions for the image (sample), the nonlinear optimization model is established
based on a novel smooth-truncated amplitude-Gaussian metric (ST-AGM). Such
metric allows for fast calculation of the proximal mapping with closed form,
and meanwhile provides the possibility for the convergence guarantee of the
first-order nonconvex optimization algorithm due to its Lipschitz smoothness.
Then the Alternating Direction Method of Multipliers (ADMM) is utilized to
generate an efficient Overlapping Domain Decomposition based Ptychography
algorithm(OD2P) for the two-subdomain domain decomposition (DD), where all
subproblems can be computed with close-form solutions.Due to the Lipschitz
continuity for the gradient of the objective function with ST-AGM, the
convergence of the proposed OD2P is derived under mild conditions. Moreover, it
is extended to more general case including multiple-subdomain DD and blind
recovery. Numerical experiments are further conducted to show the performance
of proposed algorithms, demonstrating good convergence speed and robustness to
the noise.
</p>
"
Exponential Polynomial Block Methods. (arXiv:2011.00670v2 [math.NA] UPDATED),http://arxiv.org/abs/2011.00670,"<p>In this paper we extend the polynomial time integration framework to include
exponential integration for both partitioned and unpartitioned initial value
problems. We then demonstrate the utility of the exponential polynomial
framework by constructing a new class of parallel exponential polynomial block
methods (EPBMs) based on the Legendre points. These new integrators can be
constructed at arbitrary orders of accuracy and have improved stability
compared to existing exponential linear multistep methods. Moreover, if the ODE
right-hand side evaluations can be parallelized efficiently, then high-order
EPBMs are significantly more efficient at obtaining highly accurate solutions
than exponential linear multistep methods and exponential spectral deferred
correction methods of equivalent order.
</p>
"
CUTIE: Beyond PetaOp/s/W Ternary DNN Inference Acceleration with Better-than-Binary Energy Efficiency. (arXiv:2011.01713v2 [cs.AR] UPDATED),http://arxiv.org/abs/2011.01713,"<p>We present a 3.1 POp/s/W fully digital hardware accelerator for ternary
neural networks. CUTIE, the Completely Unrolled Ternary Inference Engine,
focuses on minimizing non-computational energy and switching activity so that
dynamic power spent on storing (locally or globally) intermediate results is
minimized. This is achieved by 1) a data path architecture completely unrolled
in the feature map and filter dimensions to reduce switching activity by
favoring silencing over iterative computation and maximizing data re-use, 2)
targeting ternary neural networks which, in contrast to binary NNs, allow for
sparse weights which reduce switching activity, and 3) introducing an optimized
training method for higher sparsity of the filter weights, resulting in a
further reduction of the switching activity. Compared with state-of-the-art
accelerators, CUTIE achieves greater or equal accuracy while decreasing the
overall core inference energy cost by a factor of 4.8x-21x.
</p>
"
Motion Generation Using Bilateral Control-Based Imitation Learning with Autoregressive Learning. (arXiv:2011.06192v5 [cs.RO] UPDATED),http://arxiv.org/abs/2011.06192,"<p>Robots that can execute various tasks automatically on behalf of humans are
becoming an increasingly important focus of research in the field of robotics.
Imitation learning has been studied as an efficient and high-performance
method, and imitation learning based on bilateral control has been proposed as
a method that can realize fast motion. However, because this method cannot
implement autoregressive learning, this method may not generate desirable
long-term behavior. Therefore, in this paper, we propose a method of
autoregressive learning for bilateral control-based imitation learning. A new
neural network model for implementing autoregressive learning is proposed. In
this study, three types of experiments are conducted to verify the
effectiveness of the proposed method. The performance is improved compared to
conventional approaches; the proposed method has the highest rate of success.
Owing to the structure and autoregressive learning of the proposed model, the
proposed method can generate the desirable motion for successful tasks and have
a high generalization ability for environmental changes.
</p>
"
Unified Multi-Modal Landmark Tracking for Tightly Coupled Lidar-Visual-Inertial Odometry. (arXiv:2011.06838v2 [cs.RO] UPDATED),http://arxiv.org/abs/2011.06838,"<p>We present an efficient multi-sensor odometry system for mobile platforms
that jointly optimizes visual, lidar, and inertial information within a single
integrated factor graph. This runs in real-time at full framerate using fixed
lag smoothing. To perform such tight integration, a new method to extract 3D
line and planar primitives from lidar point clouds is presented. This approach
overcomes the suboptimality of typical frame-to-frame tracking methods by
treating the primitives as landmarks and tracking them over multiple scans.
True integration of lidar features with standard visual features and IMU is
made possible using a subtle passive synchronization of lidar and camera
frames. The lightweight formulation of the 3D features allows for real-time
execution on a single CPU. Our proposed system has been tested on a variety of
platforms and scenarios, including underground exploration with a legged robot
and outdoor scanning with a dynamically moving handheld device, for a total
duration of 96 min and 2.4 km traveled distance. In these test sequences, using
only one exteroceptive sensor leads to failure due to either underconstrained
geometry (affecting lidar) or textureless areas caused by aggressive lighting
changes (affecting vision). In these conditions, our factor graph naturally
uses the best information available from each sensor modality without any hard
switches.
</p>
"
Risk-Constrained Thompson Sampling for CVaR Bandits. (arXiv:2011.08046v4 [cs.LG] UPDATED),http://arxiv.org/abs/2011.08046,"<p>The multi-armed bandit (MAB) problem is a ubiquitous decision-making problem
that exemplifies the exploration-exploitation tradeoff. Standard formulations
exclude risk in decision making. Risk notably complicates the basic
reward-maximising objective, in part because there is no universally agreed
definition of it. In this paper, we consider a popular risk measure in
quantitative finance known as the Conditional Value at Risk (CVaR). We explore
the performance of a Thompson Sampling-based algorithm CVaR-TS under this risk
measure. We provide comprehensive comparisons between our regret bounds with
state-of-the-art L/UCB-based algorithms in comparable settings and demonstrate
their clear improvement in performance. We also include numerical simulations
to empirically verify that CVaR-TS outperforms other L/UCB-based algorithms.
</p>
"
Time-Series Snapshot Network for Partner Recommendation: A Case Study on OSS. (arXiv:2011.09883v3 [cs.SI] UPDATED),http://arxiv.org/abs/2011.09883,"<p>The last decade has witnessed the rapid growth of open source software (OSS).
Still, all contributors may find it difficult to assimilate into OSS community
even they are enthusiastic to make contributions. We thus suggest that partner
recommendation across different roles may benefit both the users and
developers, i.e., once we are able to make successful recommendation for those
in need, it may dramatically contribute to the productivity of developers and
the enthusiasm of users, thus further boosting OSS projects' development.
Motivated by this potential, we model the partner recommendation as link
prediction task from email data via network embedding methods. In this paper,
we introduce time-series snapshot network (TSSN) which is a mixture network to
model the interactions among users and developers. Based on the established
TSSN, we perform temporal biased walk (TBW) to automatically capture both
temporal and structural information of the email network, i.e., the behavioral
similarity between individuals in the OSS email network. Experiments on ten
Apache datasets demonstrate that the proposed TBW significantly outperforms a
number of advanced random walk based embedding methods, leading to the
state-of-the-art recommendation performance.
</p>
"
Zero Queueing for Multi-Server Jobs. (arXiv:2011.10521v2 [cs.PF] UPDATED),http://arxiv.org/abs/2011.10521,"<p>Cloud computing today is dominated by multi-server jobs. These are jobs that
request multiple servers simultaneously and hold onto all of these servers for
the duration of the job. Multi-server jobs add a lot of complexity to the
traditional one-job-per-server model: an arrival might not ""fit"" into the
available servers and might have to queue, blocking later arrivals and leaving
servers idle. From a queueing perspective, almost nothing is understood about
multi-server job queueing systems; even understanding the exact stability
region is a very hard problem.
</p>
<p>In this paper, we investigate a multi-server job queueing model under scaling
regimes where the number of servers in the system grows. Specifically, we
consider a system with multiple classes of jobs, where jobs from different
classes can request different numbers of servers and have different service
time distributions, and jobs are served in first-come-first-served order. The
multi-server job model opens up new scaling regimes where both the number of
servers that a job needs and the system load scale with the total number of
servers. Within these scaling regimes, we derive the first results on
stability, queueing probability, and the transient analysis of the number of
jobs in the system for each class. In particular we derive sufficient
conditions for zero queueing. Our analysis introduces a novel way of extracting
information from the Lyapunov drift, which can be applicable to a broader scope
of problems in queueing systems.
</p>
"
Logical Obstruction to Set Agreement Tasks for Superset-Closed Adversaries. (arXiv:2011.13630v2 [cs.DC] UPDATED),http://arxiv.org/abs/2011.13630,"<p>In their recent paper (GandALF 2018), Goubault, Ledent, and Rajsbaum provided
a formal epistemic model for distributed computing. Their logical model, as an
alternative to the well-studied topological model, provides an attractive
framework for refuting the solvability of a given distributed task by means of
logical obstruction: One just needs to devise a formula, in the formal language
of epistemic logic, that describes a discrepancy between the model of
computation and that of the task. However, few instances of logical obstruction
were presented in their paper and specifically logical obstruction to the
wait-free 2-set agreement task was left as an open problem. Soon later, Nishida
affirmatively answered to the problem by providing inductively defined logical
obstruction formulas to the wait-free $k$-set agreement tasks.
</p>
<p>The present paper refines Nishida's work and devises logical obstruction
formulas to $k$-set agreement tasks for superset-closed adversaries, which
supersede the wait-free model. These instances of logical obstruction formulas
exemplify that the logical framework can provide yet another feasible method
for showing impossibility of distributed tasks, though it is currently being
confined to one-round distributed protocols. The logical method has an
advantage over the topological method that it enjoys a self-contained,
elementary induction proof. This is in contrast to topological methods, in
which sophisticated topological tools, such as Nerve lemma, are often assumed
as granted.
</p>
"
Emergent Complexity and Zero-shot Transfer via Unsupervised Environment Design. (arXiv:2012.02096v2 [cs.LG] UPDATED),http://arxiv.org/abs/2012.02096,"<p>A wide range of reinforcement learning (RL) problems - including robustness,
transfer learning, unsupervised RL, and emergent complexity - require
specifying a distribution of tasks or environments in which a policy will be
trained. However, creating a useful distribution of environments is error
prone, and takes a significant amount of developer time and effort. We propose
Unsupervised Environment Design (UED) as an alternative paradigm, where
developers provide environments with unknown parameters, and these parameters
are used to automatically produce a distribution over valid, solvable
environments. Existing approaches to automatically generating environments
suffer from common failure modes: domain randomization cannot generate
structure or adapt the difficulty of the environment to the agent's learning
progress, and minimax adversarial training leads to worst-case environments
that are often unsolvable. To generate structured, solvable environments for
our protagonist agent, we introduce a second, antagonist agent that is allied
with the environment-generating adversary. The adversary is motivated to
generate environments which maximize regret, defined as the difference between
the protagonist and antagonist agent's return. We call our technique
Protagonist Antagonist Induced Regret Environment Design (PAIRED). Our
experiments demonstrate that PAIRED produces a natural curriculum of
increasingly complex environments, and PAIRED agents achieve higher zero-shot
transfer performance when tested in highly novel environments.
</p>
"
Using topological autoencoders as a filtering function for global and local topology. (arXiv:2012.03383v2 [cs.CG] UPDATED),http://arxiv.org/abs/2012.03383,"<p>Choosing a suitable filtering function for the Mapper algorithm can be
difficult due to its arbitrariness and domain-specific requirements. Finding a
general filtering function that can be applied across domains is therefore of
interest, since it would improve the representation of manifolds in higher
dimensions. In this extended abstract, we propose that topological autoencoders
is a suitable candidate for this and report initial results strengthening this
hypothesis for one set of high-dimensional manifolds. The results indicate a
potential for an easier choice of filtering function when using the Mapper
algorithm, allowing for a more general and descriptive representation of
high-dimensional data.
</p>
"
Certified Robustness of Nearest Neighbors against Data Poisoning Attacks. (arXiv:2012.03765v2 [cs.CR] UPDATED),http://arxiv.org/abs/2012.03765,"<p>Data poisoning attacks aim to corrupt a machine learning model via modifying,
adding, and/or removing some carefully selected training examples, such that
the corrupted model predicts any or attacker-chosen incorrect labels for
testing examples. The key idea of state-of-the-art certified defenses against
data poisoning attacks is to create a \emph{majority vote} mechanism to predict
the label of a testing example. Moreover, each voter is a base classifier
trained on a subset of the training dataset. Classical simple learning
algorithms such as $k$ nearest neighbors (kNN) and radius nearest neighbors
(rNN) have intrinsic majority vote mechanisms. In this work, we show that the
intrinsic majority vote mechanisms in kNN and rNN already provide certified
robustness guarantees against general data poisoning attacks. Moreover, our
evaluation results on MNIST and CIFAR10 show that the intrinsic certified
robustness guarantees of kNN and rNN outperform those provided by
state-of-the-art certified defenses. Our results serve as standard baselines
for future certified defenses against data poisoning attacks.
</p>
"
Model-agnostic Fits for Understanding Information Seeking Patterns in Humans. (arXiv:2012.04858v2 [cs.AI] UPDATED),http://arxiv.org/abs/2012.04858,"<p>In decision making tasks under uncertainty, humans display characteristic
biases in seeking, integrating, and acting upon information relevant to the
task. Here, we reexamine data from previous carefully designed experiments,
collected at scale, that measured and catalogued these biases in aggregate
form. We design deep learning models that replicate these biases in aggregate,
while also capturing individual variation in behavior. A key finding of our
work is that paucity of data collected from each individual subject can be
overcome by sampling large numbers of subjects from the population, while still
capturing individual differences. In addition, we can predict human behavior
with high accuracy without making any assumptions about task goals, reward
structure, or individual biases, thus providing a model-agnostic fit to human
behavior in the task. Such an approach can sidestep potential limitations in
modeler-specified inductive biases, and has implications for computational
modeling of human cognitive function in general, and of human-AI interfaces in
particular.
</p>
"
Empirical Analysis of Unlabeled Entity Problem in Named Entity Recognition. (arXiv:2012.05426v4 [cs.CL] UPDATED),http://arxiv.org/abs/2012.05426,"<p>In many scenarios, named entity recognition (NER) models severely suffer from
unlabeled entity problem, where the entities of a sentence may not be fully
annotated. Through empirical studies performed on synthetic datasets, we find
two causes of the performance degradation. One is the reduction of annotated
entities and the other is treating unlabeled entities as negative instances.
The first cause has less impact than the second one and can be mitigated by
adopting pretraining language models. The second cause seriously misguides a
model in training and greatly affects its performances. Based on the above
observations, we propose a general approach that is capable of eliminating the
misguidance brought by unlabeled entities. The core idea is using negative
sampling to keep the probability of training with unlabeled entities at a very
low level. Experiments on synthetic datasets and real-world datasets show that
our model is robust to unlabeled entity problem and surpasses prior baselines.
On well-annotated datasets, our model is competitive with state-of-the-art
method.
</p>
"
Application-aware Congestion Mitigation for High-Performance Computing Systems. (arXiv:2012.07755v2 [cs.DC] UPDATED),http://arxiv.org/abs/2012.07755,"<p>High-performance computing (HPC) systems frequently experience congestion
leading to significant application performance variation. However, the impact
of congestion on application runtime differs from application to application
depending on their network characteristics (such as bandwidth and latency
requirements). We leverage this insight to develop Netscope, an automated
ML-driven framework that considers those network characteristics to dynamically
mitigate congestion. We evaluate Netscope on four Cray Aries systems, including
a production supercomputer on real scientific applications. Netscope has a
lower training cost and accurately estimates the impact of congestion on
application runtime with a correlation between 0.7and 0.9 for common scientific
applications. Moreover, we find that Netscope reduces tail runtime variability
by up to 14.9 times while improving median system utility by 12%.
</p>
"
The Parameterized Suffix Tray. (arXiv:2012.10092v2 [cs.DS] UPDATED),http://arxiv.org/abs/2012.10092,"<p>Let $\Sigma$ and $\Pi$ be disjoint alphabets, respectively called the static
alphabet and the parameterized alphabet. Two strings $x$ and $y$ over $\Sigma
\cup \Pi$ of equal length are said to parameterized match (p-match) if there
exists a renaming bijection $f$ on $\Sigma$ and $\Pi$ which is identity on
$\Sigma$ and maps the characters of $x$ to those of $y$ so that the two strings
become identical. The indexing version of the problem of finding p-matching
occurrences of a given pattern in the text is a well-studied topic in string
matching. In this paper, we present a state-of-the-art indexing structure for
p-matching called the parameterized suffix tray of an input text $T$, denoted
by $\mathsf{PSTray}(T)$. We show that $\mathsf{PSTray}(T)$ occupies $O(n)$
space and supports pattern matching queries in $O(m + \log (\sigma+\pi) +
\mathit{occ})$ time, where $n$ is the length of $T$, $m$ is the length of a
query pattern $P$, $\pi$ is the number of distinct symbols of $|\Pi|$ in $T$,
$\sigma$ is the number of distinct symbols of $|\Sigma|$ in $T$ and
$\mathit{occ}$ is the number of p-matching occurrences of $P$ in $T$. We also
present how to build $\mathsf{PSTray}(T)$ in $O(n)$ time from the parameterized
suffix tree of $T$.
</p>
"
FcaNet: Frequency Channel Attention Networks. (arXiv:2012.11879v3 [cs.CV] UPDATED),http://arxiv.org/abs/2012.11879,"<p>Attention mechanism, especially channel attention, has gained great success
in the computer vision field. Many works focus on how to design efficient
channel attention mechanisms while ignoring a fundamental problem, i.e., using
global average pooling (GAP) as the unquestionable pre-processing method. In
this work, we start from a different view and rethink channel attention using
frequency analysis. Based on the frequency analysis, we mathematically prove
that the conventional GAP is a special case of the feature decomposition in the
frequency domain. With the proof, we naturally generalize the pre-processing of
channel attention mechanism in the frequency domain and propose FcaNet with
novel multi-spectral channel attention. The proposed method is simple but
effective. We can change only one line of code in the calculation to implement
our method within existing channel attention methods. Moreover, the proposed
method achieves state-of-the-art results compared with other channel attention
methods on image classification, object detection, and instance segmentation
tasks. Our method could improve by 1.8% in terms of Top-1 accuracy on ImageNet
compared with the baseline SENet-50, with the same number of parameters and the
same computational cost. Our code and models are publicly available at
https://github.com/cfzd/FcaNet.
</p>
"
Fundamental Limits on the Maximum Deviations in Control Systems: How Short Can Distribution Tails be Made by Feedback?. (arXiv:2012.12174v4 [eess.SY] UPDATED),http://arxiv.org/abs/2012.12174,"<p>This paper is on the application of information theory to the analysis of
fundamental lower bounds on the maximum deviations in feedback control systems,
where the plant is linear time-invariant while the controller can generically
be any causal functions as long as it stabilizes the plant. It is seen in
general that the lower bounds are characterized by the unstable poles (or
nonminimum-phase zeros) of the plant as well as the conditional entropy of the
disturbance. Such bounds provide fundamental limits on how short the
distribution tails in control systems can be made by feedback.
</p>
"
Existence and Computation of Maximin Fair Allocations Under Matroid-Rank Valuations. (arXiv:2012.12710v2 [cs.GT] UPDATED),http://arxiv.org/abs/2012.12710,"<p>We study fair and economically efficient allocation of indivisible goods
among agents whose valuations are rank functions of matroids. Such valuations
constitute a well-studied class of submodular functions (i.e., they exhibit a
diminishing returns property) and model preferences in several
resource-allocation settings. We prove that, for matroid-rank valuations, a
social welfare-maximizing allocation that gives each agent her maximin share
always exists. Furthermore, such an allocation can be computed in polynomial
time. We establish similar existential and algorithmic results for the pairwise
maximin share guarantee as well.
</p>
<p>To complement these results, we show that if the agents have binary XOS
valuations or weighted-rank valuations, then maximin fair allocations are not
guaranteed to exist. Both of these valuation classes are immediate
generalizations of matroid-rank functions.
</p>
"
Reinforcement Learning for Control of Valves. (arXiv:2012.14668v2 [cs.LG] UPDATED),http://arxiv.org/abs/2012.14668,"<p>This paper is a study of reinforcement learning (RL) as an optimal-control
strategy for control of nonlinear valves. It is evaluated against the PID
(proportional-integral-derivative) strategy, using a unified framework. RL is
an autonomous learning mechanism that learns by interacting with its
environment. It is gaining increasing attention in the world of control systems
as a means of building optimal-controllers for challenging dynamic and
nonlinear processes. Published RL research often uses open-source tools (Python
and OpenAI Gym environments). We use MATLAB's recently launched (R2019a)
Reinforcement Learning Toolbox to develop the valve controller; trained using
the DDPG (Deep Deterministic Policy-Gradient) algorithm and Simulink to
simulate the nonlinear valve and create the experimental test-bench for
evaluation. Simulink allows industrial engineers to quickly adapt and
experiment with other systems of their choice. Results indicate that the RL
controller is extremely good at tracking the signal with speed and produces a
lower error with respect to the reference signal. The PID, however, is better
at disturbance rejection and hence provides a longer life for the valves.
Successful machine learning involves tuning many hyperparameters requiring
significant investment of time and efforts. We introduce ""Graded Learning"" as a
simplified, application oriented adaptation of the more formal and algorithmic
""Curriculum for Reinforcement Learning"". It is shown via experiments that it
helps converge the learning task of complex non-linear real world systems.
Finally, experiential learnings gained from this research are corroborated
against published research.
</p>
"
Towards User Scheduling for 6G: A Fairness-Oriented Scheduler Using Multi-Agent Reinforcement Learning. (arXiv:2012.15081v3 [cs.OS] UPDATED),http://arxiv.org/abs/2012.15081,"<p>User scheduling is a classical problem and key technology in wireless
communication, which will still plays an important role in the prospective 6G.
There are many sophisticated schedulers that are widely deployed in the base
stations, such as Proportional Fairness (PF) and Round-Robin Fashion (RRF). It
is known that the Opportunistic (OP) scheduling is the optimal scheduler for
maximizing the average user data rate (AUDR) considering the full buffer
traffic. But the optimal strategy achieving the highest fairness still remains
largely unknown both in the full buffer traffic and the bursty traffic. In this
work, we investigate the problem of fairness-oriented user scheduling,
especially for the RBG allocation. We build a user scheduler using Multi-Agent
Reinforcement Learning (MARL), which conducts distributional optimization to
maximize the fairness of the communication system. The agents take the
cross-layer information (e.g. RSRP, Buffer size) as state and the RBG
allocation result as action, then explore the optimal solution following a
well-defined reward function designed for maximizing fairness. Furthermore, we
take the 5%-tile user data rate (5TUDR) as the key performance indicator (KPI)
of fairness, and compare the performance of MARL scheduling with PF scheduling
and RRF scheduling by conducting extensive simulations. And the simulation
results show that the proposed MARL scheduling outperforms the traditional
schedulers.
</p>
"
CASS: Towards Building a Social-Support Chatbot for Online Health Community. (arXiv:2101.01583v3 [cs.HC] UPDATED),http://arxiv.org/abs/2101.01583,"<p>Chatbots systems, despite their popularity in today's HCI and CSCW research,
fall short for one of the two reasons: 1) many of the systems use a rule-based
dialog flow, thus they can only respond to a limited number of pre-defined
inputs with pre-scripted responses; or 2) they are designed with a focus on
single-user scenarios, thus it is unclear how these systems may affect other
users or the community. In this paper, we develop a generalizable chatbot
architecture (CASS) to provide social support for community members in an
online health community. The CASS architecture is based on advanced neural
network algorithms, thus it can handle new inputs from users and generate a
variety of responses to them. CASS is also generalizable as it can be easily
migrate to other online communities. With a follow-up field experiment, CASS is
proven useful in supporting individual members who seek emotional support. Our
work also contributes to fill the research gap on how a chatbot may influence
the whole community's engagement.
</p>
"
Scalable Parallel Linear Solver for Compact Banded Systems on Heterogeneous Architectures. (arXiv:2101.02286v2 [cs.DC] UPDATED),http://arxiv.org/abs/2101.02286,"<p>A scalable algorithm for solving compact banded linear systems on distributed
memory architectures is presented. The proposed method factorizes the original
system into two levels of memory hierarchies, and solves it using parallel
cyclic reduction on both distributed and shared memory. This method has a lower
communication footprint across distributed memory partitions compared to
conventional algorithms involving data transpose or re-partitioning. The
algorithm developed in this work is generalized to cyclic compact banded
systems with flexible data decompositions. For cyclic compact banded systems,
the method is a direct solver with a deterministic operation and communication
counts depending on the matrix size, its bandwidth, and the partition strategy.
The implementation and runtime configuration details are discussed for
performance optimization. Scalability is demonstrated on the linear solver as
well as on a representative fluid mechanics application problem, in which the
dominant computational cost is solving the cyclic tridiagonal linear systems of
compact numerical schemes on a 3D periodic domain. The algorithm is
particularly useful for solving the linear systems arising from the application
of compact finite difference operators to a wide range of partial differential
equation problems, such as but not limited to the numerical simulations of
compressible turbulent flows, aeroacoustics, elastic-plastic wave propagation,
and electromagnetics. It alleviates obstacles to their use on modern high
performance computing hardware, where memory and computational power are
distributed across nodes with multi-threaded processing units.
</p>
"
The joint role of geometry and illumination on material recognition. (arXiv:2101.02496v2 [cs.CV] UPDATED),http://arxiv.org/abs/2101.02496,"<p>Observing and recognizing materials is a fundamental part of our daily life.
Under typical viewing conditions, we are capable of effortlessly identifying
the objects that surround us and recognizing the materials they are made of.
Nevertheless, understanding the underlying perceptual processes that take place
to accurately discern the visual properties of an object is a long-standing
problem. In this work, we perform a comprehensive and systematic analysis of
how the interplay of geometry, illumination, and their spatial frequencies
affects human performance on material recognition tasks. We carry out
large-scale behavioral experiments where participants are asked to recognize
different reference materials among a pool of candidate samples. In the
different experiments, we carefully sample the information in the frequency
domain of the stimuli. From our analysis, we find significant first-order
interactions between the geometry and the illumination, of both the reference
and the candidates. In addition, we observe that simple image statistics and
higher-order image histograms do not correlate with human performance.
Therefore, we perform a high-level comparison of highly non-linear statistics
by training a deep neural network on material recognition tasks. Our results
show that such models can accurately classify materials, which suggests that
they are capable of defining a meaningful representation of material appearance
from labeled proximal image data. Last, we find preliminary evidence that these
highly non-linear models and humans may use similar high-level factors for
material recognition tasks.
</p>
"
"Explain and Predict, and then Predict Again. (arXiv:2101.04109v2 [cs.CL] UPDATED)",http://arxiv.org/abs/2101.04109,"<p>A desirable property of learning systems is to be both effective and
interpretable. Towards this goal, recent models have been proposed that first
generate an extractive explanation from the input text and then generate a
prediction on just the explanation called explain-then-predict models. These
models primarily consider the task input as a supervision signal in learning an
extractive explanation and do not effectively integrate rationales data as an
additional inductive bias to improve task performance. We propose a novel yet
simple approach ExPred, that uses multi-task learning in the explanation
generation phase effectively trading-off explanation and prediction losses. And
then we use another prediction network on just the extracted explanations for
optimizing the task performance. We conduct an extensive evaluation of our
approach on three diverse language datasets -- fact verification, sentiment
classification, and QA -- and find that we substantially outperform existing
approaches.
</p>
"
"What Do We Mean by ""Accessibility Research""? A Literature Survey of Accessibility Papers in CHI and ASSETS from 1994 to 2019. (arXiv:2101.04271v4 [cs.HC] UPDATED)",http://arxiv.org/abs/2101.04271,"<p>Accessibility research has grown substantially in the past few decades, yet
there has been no literature review of the field. To understand current and
historical trends, we created and analyzed a dataset of accessibility papers
appearing at CHI and ASSETS since ASSETS' founding in 1994. We qualitatively
coded areas of focus and methodological decisions for the past 10 years
(2010-2019, N=506 papers), and analyzed paper counts and keywords over the full
26 years (N=836 papers). Our findings highlight areas that have received
disproportionate attention and those that are underserved--for example, over
43% of papers in the past 10 years are on accessibility for blind and low
vision people. We also capture common study characteristics, such as the roles
of disabled and nondisabled participants as well as sample sizes (e.g., a
median of 13 for participant groups with disabilities and older adults). We
close by critically reflecting on gaps in the literature and offering guidance
for future work in the field.
</p>
"
Robustness of on-device Models: Adversarial Attack to Deep Learning Models on Android Apps. (arXiv:2101.04401v2 [cs.LG] UPDATED),http://arxiv.org/abs/2101.04401,"<p>Deep learning has shown its power in many applications, including object
detection in images, natural-language understanding, and speech recognition. To
make it more accessible to end users, many deep learning models are now
embedded in mobile apps. Compared to offloading deep learning from smartphones
to the cloud, performing machine learning on-device can help improve latency,
connectivity, and power consumption. However, most deep learning models within
Android apps can easily be obtained via mature reverse engineering, while the
models' exposure may invite adversarial attacks. In this study, we propose a
simple but effective approach to hacking deep learning models using adversarial
attacks by identifying highly similar pre-trained models from TensorFlow Hub.
All 10 real-world Android apps in the experiment are successfully attacked by
our approach. Apart from the feasibility of the model attack, we also carry out
an empirical study that investigates the characteristics of deep learning
models used by hundreds of Android apps on Google Play. The results show that
many of them are similar to each other and widely use fine-tuning techniques to
pre-trained models on the Internet.
</p>
"
Survival of the strictest: Stable and unstable equilibria under regularized learning with partial information. (arXiv:2101.04667v2 [cs.GT] UPDATED),http://arxiv.org/abs/2101.04667,"<p>In this paper, we examine the Nash equilibrium convergence properties of
no-regret learning in general N-player games. For concreteness, we focus on the
archetypal follow the regularized leader (FTRL) family of algorithms, and we
consider the full spectrum of uncertainty that the players may encounter - from
noisy, oracle-based feedback, to bandit, payoff-based information. In this
general context, we establish a comprehensive equivalence between the stability
of a Nash equilibrium and its support: a Nash equilibrium is stable and
attracting with arbitrarily high probability if and only if it is strict (i.e.,
each equilibrium strategy has a unique best response). This equivalence extends
existing continuous-time versions of the folk theorem of evolutionary game
theory to a bona fide algorithmic learning setting, and it provides a clear
refinement criterion for the prediction of the day-to-day behavior of no-regret
learning in games
</p>
"
ChemNODE: A Neural Ordinary Differential Equations Approach for Chemical Kinetics Solvers. (arXiv:2101.04749v2 [cs.CE] UPDATED),http://arxiv.org/abs/2101.04749,"<p>The main bottleneck when performing computational fluid dynamics (CFD)
simulations of combustion systems is the computation and integration of the
highly non-linear and stiff chemical source terms. In recent times, machine
learning has emerged as a promising tool to accelerate combustion chemistry,
involving the use of regression models to predict the chemical source terms as
functions of the thermochemical state of the system. However, combustion is a
highly nonlinear phenomenon, and this often leads to divergence from the true
solution when the neural network representation of chemical kinetics is
integrated in time. This is because these approaches minimize the error during
training without guaranteeing successful integration with ordinary differential
equation (ODE) solvers. In this work, a novel neural ODE approach to combustion
modeling, ChemNODE, is developed to address this issue. The source terms
predicted by the neural network are integrated during training, and by
backpropagating errors through the ODE solver, the neural network weights are
adjusted accordingly to minimize the difference between the predicted and
actual ODE solutions. It is shown that even when the dimensionality of the
thermochemical manifold is trimmed to remove redundant species, the proposed
approach accurately captures the correct physical behavior and reproduces the
results obtained using the full chemical kinetic mechanism.
</p>
"
MFFCN: Multi-layer Feature Fusion Convolution Network for Audio-visual Speech Enhancement. (arXiv:2101.05975v2 [eess.AS] UPDATED),http://arxiv.org/abs/2101.05975,"<p>The purpose of speech enhancement is to extract target speech signal from a
mixture of sounds generated from several sources. Speech enhancement can
potentially benefit from the visual information from the target speaker, such
as lip move-ment and facial expressions, because the visual aspect of speech
isessentially unaffected by acoustic environment. In order to fuse audio and
visual information, an audio-visual fusion strategy is proposed, which goes
beyond simple feature concatenation and learns to automatically align the two
modalities, leading to more powerful representation which increase
intelligibility in noisy conditions. The proposed model fuses audio-visual
featureslayer by layer, and feed these audio-visual features to each
corresponding decoding layer. Experiment results show relative improvement from
6% to 24% on test sets over the audio modalityalone, depending on audio noise
level. Moreover, there is a significant increase of PESQ from 1.21 to 2.06 in
our -15 dB SNR experiment.
</p>
"
AMFFCN: Attentional Multi-layer Feature Fusion Convolution Network for Audio-visual Speech Enhancement. (arXiv:2101.06268v2 [eess.AS] UPDATED),http://arxiv.org/abs/2101.06268,"<p>Audio-visual speech enhancement system is regarded to be one of promising
solutions for isolating and enhancing speech of desired speaker. Conventional
methods focus on predicting clean speech spectrum via a naive convolution
neural network based encoder-decoder architecture, and these methods a) not
adequate to use data fully and effectively, b) cannot process features
selectively. The proposed model addresses these drawbacks, by a) applying a
model that fuses audio and visual features layer by layer in encoding phase,
and that feeds fused audio-visual features to each corresponding decoder layer,
and more importantly, b) introducing soft threshold attention into the model to
select the informative modality softly. This paper proposes attentional
audio-visual multi-layer feature fusion model, in which soft threshold
attention unit are applied on feature mapping at every layer of decoder. The
proposed model demonstrates the superior performance of the network against the
state-of-the-art models.
</p>
"
Byzantine Generals in the Permissionless Setting. (arXiv:2101.07095v2 [cs.DC] UPDATED),http://arxiv.org/abs/2101.07095,"<p>Consensus protocols have traditionally been studied in a setting where all
participants are known to each other from the start of the protocol execution.
In the parlance of the 'blockchain' literature, this is referred to as the
permissioned setting. What differentiates Bitcoin from these previously studied
protocols is that it operates in a permissionless setting, i.e. it is a
protocol for establishing consensus over an unknown network of participants
that anybody can join, with as many identities as they like in any role. The
arrival of this new form of protocol brings with it many questions. Beyond
Bitcoin, what can we prove about permissionless protocols in a general sense?
How does recent work on permissionless protocols in the blockchain literature
relate to the well-developed history of research on permissioned protocols in
distributed computing?
</p>
<p>To answer these questions, we describe a formal framework for the analysis of
both permissioned and permissionless systems. Our framework allows for
""apples-to-apples"" comparisons between different categories of protocols and,
in turn, the development of theory to formally discuss their relative merits. A
major benefit of the framework is that it facilitates the application of a rich
history of proofs and techniques in distributed computing to problems in
blockchain and the study of permissionless systems. Within our framework, we
then address the questions above. We consider the Byzantine Generals Problem as
a formalisation of the problem of reaching consensus, and address a programme
of research that asks, ""Under what adversarial conditions, and for what types
of permissionless protocol, is consensus possible?"" We prove a number of
results for this programme, our main result being that deterministic consensus
is not possible for decentralised permissionless protocols. To close, we give a
list of seven open questions.
</p>
"
Deep Reinforcement Learning for Active High Frequency Trading. (arXiv:2101.07107v2 [cs.LG] UPDATED),http://arxiv.org/abs/2101.07107,"<p>We introduce the first end-to-end Deep Reinforcement Learning (DRL) based
framework for active high frequency trading. We train DRL agents to trade one
unit of Intel Corporation stock by employing the Proximal Policy Optimization
algorithm. The training is performed on three contiguous months of high
frequency Limit Order Book data, of which the last month constitutes the
validation data. In order to maximise the signal to noise ratio in the training
data, we compose the latter by only selecting training samples with largest
price changes. The test is then carried out on the following month of data.
Hyperparameters are tuned using the Sequential Model Based Optimization
technique. We consider three different state characterizations, which differ in
their LOB-based meta-features. Analysing the agents' performances on test data,
we argue that the agents are able to create a dynamic representation of the
underlying environment. They identify occasional regularities present in the
data and exploit them to create long-term profitable trading strategies.
Indeed, agents learn trading strategies able to produce stable positive returns
in spite of the highly stochastic and non-stationary environment.
</p>
"
Policy choices can help keep 4G and 5G universal broadband affordable. (arXiv:2101.07820v2 [econ.GN] UPDATED),http://arxiv.org/abs/2101.07820,"<p>In recognition of the transformative opportunities that broadband
connectivity presents, the United Nations Broadband Commission has committed
the international community to accelerate universal access across the
developing world. However, the cost of meeting this objective, and the
feasibility of doing so on a commercially viable basis, are not well
understood. This paper compares the global cost-effectiveness of different
infrastructure strategies for the developing world to achieve universal 4G or
5G mobile broadband. Utilizing remote sensing and geospatial infrastructure
simulation, least-cost network designs are developed for eight representative
low and middle-income countries (Malawi, Uganda, Kenya, Senegal, Pakistan,
Albania, Peru and Mexico), the results from which form the basis for
aggregation to the global level. To provide at least 2 Mbps per user, 4G is
often the cheapest option to reach universal coverage. The cost of meeting the
UN Broadband Commission target of a minimum 10 Mbps per user is estimated at
$1.7 trillion using 5G NSA, equating to approximately 0.6% of annual GDP for
the developing world over the next decade. However, by creating a favorable
regulatory environment, governments can bring down these costs by as much as
three quarters, to $0.5 trillion (approximately 0.2% of annual GDP), and avoid
the need for public subsidy. Providing governments make judicious choices,
adopting fiscal and regulatory regimes conducive to lowering costs, broadband
universal service may be within reach of most developing countries over the
next decade.
</p>
"
TCLR: Temporal Contrastive Learning for Video Representation. (arXiv:2101.07974v2 [cs.CV] UPDATED),http://arxiv.org/abs/2101.07974,"<p>Contrastive learning has nearly closed the gap between supervised and
self-supervised learning of image representations. Existing extensions of
contrastive learning to the domain of video data however do not explicitly
attempt to represent the internal distinctiveness across the temporal dimension
of video clips. We develop a new temporal contrastive learning framework
consisting of two novel losses to improve upon existing contrastive
self-supervised video representation learning methods. The first loss adds the
task of discriminating between non-overlapping clips from the same video,
whereas the second loss aims to discriminate between timesteps of the feature
map of an input clip in order to increase the temporal diversity of the
features. Temporal contrastive learning achieves significant improvement over
the state-of-the-art results in downstream video understanding tasks such as
action recognition, limited-label action classification, and nearest-neighbor
video retrieval on video datasets across multiple 3D CNN architectures. With
the commonly used 3D-ResNet-18 architecture, we achieve 82.4% (+5.1% increase
over the previous best) top-1 accuracy on UCF101 and 52.9% (+5.4% increase) on
HMDB51 action classification, and 56.2% (+11.7% increase) Top-1 Recall on
UCF101 nearest neighbor video retrieval.
</p>
"
Few-shot Action Recognition with Prototype-centered Attentive Learning. (arXiv:2101.08085v2 [cs.CV] UPDATED),http://arxiv.org/abs/2101.08085,"<p>Few-shot action recognition aims to recognize action classes with few
training samples. Most existing methods adopt a meta-learning approach with
episodic training. In each episode, the few samples in a meta-training task are
split into support and query sets. The former is used to build a classifier,
which is then evaluated on the latter using a query-centered loss for model
updating. There are however two major limitations: lack of data efficiency due
to the query-centered only loss design and inability to deal with the support
set outlying samples and inter-class distribution overlapping problems. In this
paper, we overcome both limitations by proposing a new Prototype-centered
Attentive Learning (PAL) model composed of two novel components. First, a
prototype-centered contrastive learning loss is introduced to complement the
conventional query-centered learning objective, in order to make full use of
the limited training samples in each episode. Second, PAL further integrates a
hybrid attentive learning mechanism that can minimize the negative impacts of
outliers and promote class separation. Extensive experiments on four standard
few-shot action benchmarks show that our method clearly outperforms previous
state-of-the-art methods, with the improvement particularly significant (10+\%)
on the most challenging fine-grained action recognition benchmark.
</p>
"
Rank the Episodes: A Simple Approach for Exploration in Procedurally-Generated Environments. (arXiv:2101.08152v2 [cs.LG] UPDATED),http://arxiv.org/abs/2101.08152,"<p>Exploration under sparse reward is a long-standing challenge of model-free
reinforcement learning. The state-of-the-art methods address this challenge by
introducing intrinsic rewards to encourage exploration in novel states or
uncertain environment dynamics. Unfortunately, methods based on intrinsic
rewards often fall short in procedurally-generated environments, where a
different environment is generated in each episode so that the agent is not
likely to visit the same state more than once. Motivated by how humans
distinguish good exploration behaviors by looking into the entire episode, we
introduce RAPID, a simple yet effective episode-level exploration method for
procedurally-generated environments. RAPID regards each episode as a whole and
gives an episodic exploration score from both per-episode and long-term views.
Those highly scored episodes are treated as good exploration behaviors and are
stored in a small ranking buffer. The agent then imitates the episodes in the
buffer to reproduce the past good exploration behaviors. We demonstrate our
method on several procedurally-generated MiniGrid environments, a
first-person-view 3D Maze navigation task from MiniWorld, and several sparse
MuJoCo tasks. The results show that RAPID significantly outperforms the
state-of-the-art intrinsic reward strategies in terms of sample efficiency and
final performance. The code is available at https://github.com/daochenzha/rapid
</p>
"
"A novel DL approach to PE malware detection: exploring Glove vectorization, MCC_RCNN and feature fusion. (arXiv:2101.08969v3 [cs.CR] UPDATED)",http://arxiv.org/abs/2101.08969,"<p>In recent years, malware becomes more threatening. Concerning the increasing
malware variants, there comes Machine Learning (ML)-based and Deep Learning
(DL)-based approaches for heuristic detection. Nevertheless, the prediction
accuracy of both needs to be improved. In response to the above issues in the
PE malware domain, we propose the DL-based approaches for detection and use
static-based features fed up into models. The contributions are as follows: we
recapitulate existing malware detection methods. That is, we propose a
vec-torized representation model of the malware instruction layer and semantic
layer based on Glove. We implement a neural network model called MCC_RCNN
(Malware Detection and Recurrent Convolutional Neural Network), comprising of
the combination with CNN and RNN. Moreover, we provide a description of feature
fusion in static behavior levels. With the numerical results generated from
several comparative experiments towards evaluating the Glove-based
vectoriza-tion, MCC_RCNN-based classification methodology and feature fusion
stages, our proposed classification methods can obtain a higher prediction
accuracy than the other baseline methods.
</p>
"
Beyond Domain APIs: Task-oriented Conversational Modeling with Unstructured Knowledge Access Track in DSTC9. (arXiv:2101.09276v3 [cs.CL] UPDATED),http://arxiv.org/abs/2101.09276,"<p>Most prior work on task-oriented dialogue systems are restricted to a limited
coverage of domain APIs, while users oftentimes have domain related requests
that are not covered by the APIs. This challenge track aims to expand the
coverage of task-oriented dialogue systems by incorporating external
unstructured knowledge sources. We define three tasks: knowledge-seeking turn
detection, knowledge selection, and knowledge-grounded response generation. We
introduce the data sets and the neural baseline models for three tasks. The
challenge track received a total of 105 entries from 24 participating teams. In
the evaluation results, the ensemble methods with different large-scale
pretrained language models achieved high performances with improved knowledge
selection capability and better generalization into unseen data.
</p>
"
Advances and Challenges in Conversational Recommender Systems: A Survey. (arXiv:2101.09459v4 [cs.IR] UPDATED),http://arxiv.org/abs/2101.09459,"<p>Recommender systems exploit interaction history to estimate user preference,
having been heavily used in a wide range of industry applications. However,
static recommendation models are difficult to answer two important questions
well due to inherent shortcomings: (a) What exactly does a user like? (b) Why
does a user like an item? The shortcomings are due to the way that static
models learn user preference, i.e., without explicit instructions and active
feedback from users. The recent rise of conversational recommender systems
(CRSs) changes this situation fundamentally. In a CRS, users and the system can
dynamically communicate through natural language interactions, which provide
unprecedented opportunities to explicitly obtain the exact preference of users.
Considerable efforts, spread across disparate settings and applications, have
been put into developing CRSs. Existing models, technologies, and evaluation
methods for CRSs are far from mature. In this paper, we provide a systematic
review of the techniques used in current CRSs. We summarize the key challenges
of developing CRSs into five directions: (1) Question-based user preference
elicitation. (2) Multi-turn conversational recommendation strategies. (3)
Dialogue understanding and generation. (4) Exploitation-exploration trade-offs.
(5) Evaluation and user simulation. These research directions involve multiple
research fields like information retrieval (IR), natural language processing
(NLP), and human-computer interaction (HCI). Based on these research
directions, we discuss some future challenges and opportunities. We provide a
road map for researchers from multiple communities to get started in this area.
We hope this survey helps to identify and address challenges in CRSs and
inspire future research.
</p>
"
Set Reconciliation for Blockchains with Slepian-Wolf Coding: Deletion Polar Codes. (arXiv:2101.09963v2 [cs.IT] UPDATED),http://arxiv.org/abs/2101.09963,"<p>In this paper, we propose a polar coding based scheme for set reconciliation
between two network nodes. The system is modeled as a well-known Slepian-Wolf
setting induced by a fixed number of deletions. The set reconciliation process
is divided into two phases: 1) a deletion polar code is employed to help one
node to identify the possible deletion indices, which may be larger than the
number of genuine deletions; 2) a lossless compression polar code is then
designed to feedback those indices with minimum overhead. Our scheme can be
viewed as a generalization of polar codes to some emerging network-based
applications such as the package synchronization in blockchains. Some
connections with the existing schemes based on the invertible Bloom lookup
tables (IBLTs) and network coding are also observed and briefly discussed.
</p>
"
Classical simulations of communication channels. (arXiv:2101.10985v2 [cs.IT] UPDATED),http://arxiv.org/abs/2101.10985,"<p>We investigate whether certain non-classical communication channels can be
simulated by a classical channel with a given number of states and a given
amount of noise. It is proved that any noisy quantum channel can be simulated
by the corresponding noisy classical channel. General probabilistic channels
are also studied.
</p>
"
Launchers and Targets in Social Networks. (arXiv:2101.11337v2 [cs.SI] UPDATED),http://arxiv.org/abs/2101.11337,"<p>Influence propagation in social networks is a subject of growing interest. A
relevant issue in those networks involves the identification of key
influencers. These players have an important role on viral marketing strategies
and message propagation, including political propaganda and fake news. In
effect, an important way to fight malicious usage on social networks is to
understand their properties, their structure and the way messages propagate.
</p>
<p>This paper proposes two new indices for analysing message propagation in
social networks, based on the network topological nature and the power of the
message. The first index involves the strength of each node as a launcher of
the message, dividing the nodes into launchers and non-launchers. The second
index addresses the potential of each member as a receiver (target) of the
message, dividing the nodes into targets and non-targets. Launcher individuals
should indicate strong influencers and target individuals should identify the
best target consumers. These indices can assist other known metrics when used
to select efficient influencers in a social network. For instance, instead of
choosing a strong and probably expensive member according to its degree in the
network (number of followers), we may previously select those belonging to the
launchers group and look for the lowest degree members, which are probably
cheaper but still guarantying almost the same influence effectiveness as the
largest degree members.
</p>
<p>On a different direction, using the second index, the strong target members
should characterize relevant consumers of information in the network, which may
include fake news' regular collectors.
</p>
<p>We discuss these indices using small-world randomly generated graphs and a
number of real-world social networks available in known datasets repositories.
</p>
"
SimBle: Generating privacy preserving real-world BLE traces with ground truth. (arXiv:2101.11728v2 [cs.CR] UPDATED),http://arxiv.org/abs/2101.11728,"<p>Bluetooth has become critical as many IoT devices are arriving in the market.
Most of the current literature focusing on Bluetooth simulation concentrates on
the network protocols' performances and completely neglects the privacy
protection recommendations introduced in the BLE standard. Indeed, privacy
protection is one of the main issues handled in the Bluetooth standard. For
instance, the current standard forces devices to change the identifier they
embed within the public and private packets, known as MAC address
randomization. Although randomizing MAC addresses is intended to preserve
device privacy, recent literature shows many challenges that are still present.
One of them is the correlation between the public packets and the emitters.
Unfortunately, existing evaluation tools such as NS-3 are not designed to
reproduce this Bluetooth standard's essential functionality. This makes it
impossible to test solutions for different device-fingerprinting strategies as
there is a lack of ground truth for large-scale scenarios with the majority of
current BLE devices implementing MAC address randomization. In this paper, we
first introduce a solution of standard-compliant MAC address randomization in
the NS-3 framework, capable of emulating any real BLE device in the simulation
and generating real-world Bluetooth traces. In addition, since the simulation
run-time for trace-collection grows exponentially with the number of devices,
we introduce an optimization to linearize public-packet sniffing. This made the
large-scale trace-collection practically feasible. Then, we use the generated
traces and associated ground truth to do a case study on the evaluation of a
generic MAC address association available in the literature. Our case study
reveals that close to 90 percent of randomized addresses could be correctly
linked even in highly dense and mobile scenarios.
</p>
"
Exploring Lightweight Interventions at Posting Time to Reduce the Sharing of Misinformation on Social Media. (arXiv:2101.11824v3 [cs.HC] UPDATED),http://arxiv.org/abs/2101.11824,"<p>When users on social media share content without considering its veracity,
they may unwittingly be spreading misinformation. In this work, we investigate
the design of lightweight interventions that nudge users to assess the accuracy
of information as they share it. Such assessment may deter users from posting
misinformation in the first place, and their assessments may also provide
useful guidance to friends aiming to assess those posts themselves. In support
of lightweight assessment, we first develop a taxonomy of the reasons why
people believe a news claim is or is not true; this taxonomy yields a checklist
that can be used at posting time. We conduct evaluations to demonstrate that
the checklist is an accurate and comprehensive encapsulation of people's
free-response rationales. In a second experiment, we study the effects of three
behavioral nudges -- 1) checkboxes indicating whether headings are accurate, 2)
tagging reasons (from our taxonomy) that a post is accurate via a checklist and
3) providing free-text rationales for why a headline is or is not accurate --
on people's intention of sharing the headline on social media. From an
experiment with 1668 participants, we find that both providing accuracy
assessment and rationale reduce the sharing of false content. They also reduce
the sharing of true content, but to a lesser degree that yields an overall
decrease in the fraction of shared content that is false. Our findings have
implications for designing social media and news sharing platforms that draw
from richer signals of content credibility contributed by users. In addition,
our validated taxonomy can be used by platforms and researchers as a way to
gather rationales in an easier fashion than free-response.
</p>
"
Modeling Context in Answer Sentence Selection Systems on a Latency Budget. (arXiv:2101.12093v2 [cs.CL] UPDATED),http://arxiv.org/abs/2101.12093,"<p>Answer Sentence Selection (AS2) is an efficient approach for the design of
open-domain Question Answering (QA) systems. In order to achieve low latency,
traditional AS2 models score question-answer pairs individually, ignoring any
information from the document each potential answer was extracted from. In
contrast, more computationally expensive models designed for machine reading
comprehension tasks typically receive one or more passages as input, which
often results in better accuracy. In this work, we present an approach to
efficiently incorporate contextual information in AS2 models. For each answer
candidate, we first use unsupervised similarity techniques to extract relevant
sentences from its source document, which we then feed into an efficient
transformer architecture fine-tuned for AS2. Our best approach, which leverages
a multi-way attention architecture to efficiently encode context, improves 6%
to 11% over noncontextual state of the art in AS2 with minimal impact on system
latency. All experiments in this work were conducted in English.
</p>
"
From Geometry to Topology: Inverse Theorems for Distributed Persistence. (arXiv:2101.12288v2 [math.AT] UPDATED),http://arxiv.org/abs/2101.12288,"<p>What is the ""right"" topological invariant of a large point cloud X? Prior
research has focused on estimating the full persistence diagram of X, a
quantity that is very expensive to compute, unstable to outliers, and far from
a sufficient statistic. We therefore propose that the correct invariant is not
the persistence diagram of X, but rather the collection of persistence diagrams
of many small subsets. This invariant, which we call ""distributed persistence,""
is trivially parallelizable, more stable to outliers, and has a rich inverse
theory. The map from the space of point clouds (with the quasi-isometry metric)
to the space of distributed persistence invariants (with the
Hausdorff-Bottleneck distance) is a global quasi-isometry. This is a much
stronger property than simply being injective, as it implies that the inverse
of a small neighborhood is a small neighborhood, and is to our knowledge the
only result of its kind in the TDA literature. Moreover, the quasi-isometry
bounds depend on the size of the subsets taken, so that as the size of these
subsets goes from small to large, the invariant interpolates between a purely
geometric one and a topological one. Lastly, we note that our inverse results
do not actually require considering all subsets of a fixed size (an enormous
collection), but a relatively small collection satisfying certain covering
properties that arise with high probability when randomly sampling subsets.
These theoretical results are complemented by two synthetic experiments
demonstrating the use of distributed persistence in practice.
</p>
"
NeMo: Neural Mesh Models of Contrastive Features for Robust 3D Pose Estimation. (arXiv:2101.12378v3 [cs.CV] UPDATED),http://arxiv.org/abs/2101.12378,"<p>3D pose estimation is a challenging but important task in computer vision. In
this work, we show that standard deep learning approaches to 3D pose estimation
are not robust when objects are partially occluded or viewed from a previously
unseen pose. Inspired by the robustness of generative vision models to partial
occlusion, we propose to integrate deep neural networks with 3D generative
representations of objects into a unified neural architecture that we term
NeMo. In particular, NeMo learns a generative model of neural feature
activations at each vertex on a dense 3D mesh. Using differentiable rendering
we estimate the 3D object pose by minimizing the reconstruction error between
NeMo and the feature representation of the target image. To avoid local optima
in the reconstruction loss, we train the feature extractor to maximize the
distance between the individual feature representations on the mesh using
contrastive learning. Our extensive experiments on PASCAL3D+,
occluded-PASCAL3D+ and ObjectNet3D show that NeMo is much more robust to
partial occlusion and unseen pose compared to standard deep networks, while
retaining competitive performance on regular data. Interestingly, our
experiments also show that NeMo performs reasonably well even when the mesh
representation only crudely approximates the true object geometry with a
cuboid, hence revealing that the detailed 3D geometry is not needed for
accurate 3D pose estimation. The code is publicly available at
https://github.com/Angtian/NeMo.
</p>
"
Disparate Impact Diminishes Consumer Trust Even for Advantaged Users. (arXiv:2101.12715v2 [cs.HC] UPDATED),http://arxiv.org/abs/2101.12715,"<p>Systems aiming to aid consumers in their decision-making (e.g., by
implementing persuasive techniques) are more likely to be effective when
consumers trust them. However, recent research has demonstrated that the
machine learning algorithms that often underlie such technology can act
unfairly towards specific groups (e.g., by making more favorable predictions
for men than for women). An undesired disparate impact resulting from this kind
of algorithmic unfairness could diminish consumer trust and thereby undermine
the purpose of the system. We studied this effect by conducting a
between-subjects user study investigating how (gender-related) disparate impact
affected consumer trust in an app designed to improve consumers' financial
decision-making. Our results show that disparate impact decreased consumers'
trust in the system and made them less likely to use it. Moreover, we find that
trust was affected to the same degree across consumer groups (i.e., advantaged
and disadvantaged users) despite both of these consumer groups recognizing
their respective levels of personal benefit. Our findings highlight the
importance of fairness in consumer-oriented artificial intelligence systems.
</p>
"
OpenMatch: An Open-Source Package for Information Retrieval. (arXiv:2102.00166v2 [cs.IR] UPDATED),http://arxiv.org/abs/2102.00166,"<p>Information Retrieval (IR) is an important task and can be used in many
applications. Neural IR (Neu-IR) models overcome the vocabulary mismatch
problem of sparse retrievers and thrive on the ranking pipeline with semantic
matching. Recent progress in IR mainly focuses on Neu-IR models, including
efficient dense retrieval, advanced neural architectures and robustly training
for few-shot IR that lacks training data. In order to integrate these
advantages for researchers and engineers to utilize and develop, OpenMatch
provides various functional neural modules based on PyTorch to maintain
sufficient extensibility, making it easy to build customized and
higher-capacity IR systems. Besides, OpenMatch consists of complicated
optimization tricks, various sparse/dense retrieval methods, and advanced
few-shot training methods, liberating users from surplus labor in baseline
reimplementation and neural model finetuning. With OpenMatch, we achieve
reasonable performance on various ranking datasets, rank first of the automatic
group in TREC COVID (Round 2) and rank top on the MS MARCO Document Ranking
leaderboard. The library, experimental methodologies and results of OpenMatch
are all publicly available at https://github.com/thunlp/OpenMatch.
</p>
"
Estimating the Unique Information of Continuous Variables. (arXiv:2102.00218v2 [cs.IT] UPDATED),http://arxiv.org/abs/2102.00218,"<p>Partial information decompositions (PIDs) identify different modes in which
information from multiple sources may affect a target, by isolating
synergistic, redundant, and unique contributions to the mutual information.
While many works have studied aspects of PIDs for Gaussian and discrete
distributions, the case of general continuous distributions is still uncharted
territory. In this work we present a method for estimating the unique
information in continuous distributions, for the case of two sources and one
target. Our method solves the associated optimization problem over the space of
distributions with constrained marginals by combining copula decompositions and
techniques developed to optimize variational autoencoders. We illustrate our
approach by showing excellent agreement with known analytic results for
Gaussians and by analyzing model systems of three coupled random variables.
</p>
"
Choosing the Variable Ordering for Cylindrical Algebraic Decomposition via Exploiting Chordal Structure. (arXiv:2102.00823v2 [cs.SC] UPDATED),http://arxiv.org/abs/2102.00823,"<p>Cylindrical algebraic decomposition (CAD) plays an important role in the
field of real algebraic geometry and many other areas. As is well-known, the
choice of variable ordering while computing CAD has a great effect on the time
and memory use of the computation as well as the number of sample points
computed. In this paper, we indicate that typical CAD algorithms, if executed
with respect to a special kind of variable orderings (called ""the perfect
elimination orderings""), naturally preserve chordality, which is an important
property on sparsity of variables. Experimentation suggests that if the
associated graph of the polynomial system in question is chordal (\emph{resp.},
is nearly chordal), then a perfect elimination ordering of the associated graph
(\emph{resp.}, of a minimal chordal completion of the associated graph) can be
a good variable ordering for the CAD computation. That is, by using the perfect
elimination orderings, the CAD computation may produce a much smaller full set
of projection polynomials than by using other naive variable orderings. More
importantly, for the complexity analysis of the CAD computation via a perfect
elimination ordering, a so-called $(m,d)$-property of the full set of
projection polynomials obtained via such an ordering is given, through which
the ""size"" of this set is characterized. This property indicates that when the
corresponding perfect elimination tree has a lower height, the full set of
projection polynomials also tends to have a smaller ""size"". This is well
consistent with the experimental results, hence the perfect elimination
orderings with lower elimination tree height are further recommended to be used
in the CAD projection.
</p>
"
On the dichromatic number of surfaces. (arXiv:2102.01034v2 [math.CO] UPDATED),http://arxiv.org/abs/2102.01034,"<p>In this paper, we give bounds on the dichromatic number $\vec{\chi}(\Sigma)$
of a surface $\Sigma$, which is the maximum dichromatic number of an oriented
graph embeddable on $\Sigma$. We determine the asymptotic behaviour of
$\vec{\chi}(\Sigma)$ by showing that there exist constants $a_1$ and $a_2$ such
that,
</p>
<p>$ a_1\frac{\sqrt{-c}}{\log(-c)} \leq \vec{\chi}(\Sigma) \leq a_2
\frac{\sqrt{-c}}{\log(-c)} $ for every surface $\Sigma$ with Euler
characteristic $c\leq -2$. We then give more explicit bounds for some surfaces
with high Euler characteristic. In particular, we show that the dichromatic
numbers of the projective plane $\mathbb{N}_1$, the Klein bottle
$\mathbb{N}_2$, the torus $\mathbb{S}_1$, and Dyck's surface $\mathbb{N}_3$ are
all equal to $3$, and that the dichromatic numbers of the $5$-torus
$\mathbb{S}_5$ and the $10$-cross surface $\mathbb{N}_{10}$ are equal to $4$.
We also consider the complexity of deciding whether a given digraph or oriented
graph embedabble in a fixed surface is $k$-dicolourable. In particular, we show
that for any surface, deciding whether a digraph embeddable on this surface is
$2$-dicolourable is NP-complete, and that deciding whether a planar oriented
graph is $2$-dicolourable is NP-complete unless all planar oriented graphs are
$2$-dicolourable (which was conjectured by Neumann-Lara).
</p>
"
A Statistician Teaches Deep Learning. (arXiv:2102.01194v2 [stat.ML] UPDATED),http://arxiv.org/abs/2102.01194,"<p>Deep learning (DL) has gained much attention and become increasingly popular
in modern data science. Computer scientists led the way in developing deep
learning techniques, so the ideas and perspectives can seem alien to
statisticians. Nonetheless, it is important that statisticians become involved
-- many of our students need this expertise for their careers. In this paper,
developed as part of a program on DL held at the Statistical and Applied
Mathematical Sciences Institute, we address this culture gap and provide tips
on how to teach deep learning to statistics graduate students. After some
background, we list ways in which DL and statistical perspectives differ,
provide a recommended syllabus that evolved from teaching two iterations of a
DL graduate course, offer examples of suggested homework assignments, give an
annotated list of teaching resources, and discuss DL in the context of two
research areas.
</p>
"
T3AB: Transparent and Trustworthy Third-party Authority using Blockchain. (arXiv:2102.01249v2 [cs.CR] UPDATED),http://arxiv.org/abs/2102.01249,"<p>Increasingly, information systems rely on computational, storage, and network
resources deployed in third-party facilities or are supported by service
providers. Such an approach further exacerbates cybersecurity concerns
constantly raised by numerous incidents of security and privacy attacks
resulting in data leakage and identity theft, among others. These have in turn
forced the creation of stricter security and privacy related regulations and
have eroded the trust in cyberspace. In particular, security related services
and infrastructures such as Certificate Authorities (CAs) that provide digital
certificate service and Third-Party Authorities (TPAs) that provide
cryptographic key services, are critical components for establishing trust in
Internet enabled applications and services. To address such trust issues,
various transparency frameworks and approaches have been recently proposed in
the literature. In this paper, we propose a Transparent and Trustworthy TPA
using Blockchain (T3AB) to provide transparency and accountability to the
trusted third-party entities, such as honest-but-curious third-party IaaS
servers, and coordinators in various privacy-preserving machine learning (PPML)
approaches. T3AB employs the Ethereum blockchain as the underlying public
ledger and also includes a novel smart contract to automate accountability with
an incentive mechanism that motivates participants' to participate in auditing,
and punishes unintentional or malicious behaviors. We implement T3AB, and show
through experimental evaluation in the Ethereum official test network, Rinkeby,
that the framework is efficient. We also formally show the security guarantee
provided by T3AB, and analyze the privacy guarantee and trustworthiness it
provides.
</p>
"
Gaussian Experts Selection using Graphical Models. (arXiv:2102.01496v2 [cs.LG] UPDATED),http://arxiv.org/abs/2102.01496,"<p>Local approximations are popular methods to scale Gaussian processes (GPs) to
big data. Local approximations reduce time complexity by dividing the original
dataset into subsets and training a local expert on each subset. Aggregating
the experts' prediction is done assuming either conditional dependence or
independence between the experts. Imposing the \emph{conditional independence
assumption} (CI) between the experts renders the aggregation of different
expert predictions time efficient at the cost of poor uncertainty
quantification. On the other hand, modeling dependent experts can provide
precise predictions and uncertainty quantification at the expense of
impractically high computational costs. By eliminating weak experts via a
theory-guided expert selection step, we substantially reduce the computational
cost of aggregating dependent experts while ensuring calibrated uncertainty
quantification. We leverage techniques from the literature on undirected
graphical models, using sparse precision matrices that encode conditional
dependencies between experts to select the most important experts. Moreov
</p>
"
Permute & Add Network Codes via Group Algebras. (arXiv:2102.01519v2 [cs.IT] UPDATED),http://arxiv.org/abs/2102.01519,"<p>A class of network codes have been proposed in the literature where the
symbols transmitted on network edges are binary vectors and the coding
operation performed in network nodes consists of the application of (possibly
several) permutations on each incoming vector and XOR-ing the results to obtain
the outgoing vector. These network codes, which we will refer to as
permute-and-add network codes, involve simpler operations and are known to
provide lower complexity solutions than scalar linear codes. The complexity of
these codes is determined by their degree which is the number of permutations
applied on each incoming vector to compute an outgoing vector. Constructions of
permute-and-add network codes for multicast networks are known. In this paper,
we provide a new framework based on group algebras to design permute-and-add
network codes for arbitrary (not necessarily multicast) networks. Our framework
allows the use of any finite group of permutations (including circular shifts,
proposed in prior work) and admits a trade-off between coding rate and the
degree of the code. Further, our technique permits elegant recovery and
generalizations of the key results on permute-and-add network codes known in
the literature.
</p>
"
A New Design of Cache-aided Multiuser Private Information Retrieval with Uncoded Prefetching. (arXiv:2102.01643v2 [cs.IT] UPDATED),http://arxiv.org/abs/2102.01643,"<p>In the problem of cache-aided multiuser private information retrieval
(MuPIR), a set of $K_{\rm u}$ cache-equipped users wish to privately download a
set of messages from $N$ distributed databases each holding a library of $K$
messages. The system works in two phases: {\it cache placement (prefetching)
phase} in which the users fill up their cache memory, and {\it private delivery
phase} in which the users' demands are revealed and they download an answer
from each database so that the their desired messages can be recovered while
each individual database learns nothing about the identities of the requested
messages. The goal is to design the placement and the private delivery phases
such that the \emph{load}, which is defined as the total number of downloaded
bits normalized by the message size, is minimized given any user memory size.
This paper considers the MuPIR problem with two messages, arbitrary number of
users and databases where uncoded prefetching is assumed, i.e., the users
directly copy some bits from the library as their cached contents. We propose a
novel MuPIR scheme inspired by the Maddah-Ali and Niesen (MAN) coded caching
scheme. The proposed scheme achieves lower load than any existing schemes,
especially the product design (PD), and is shown to be optimal within a factor
of $8$ in general and exactly optimal at very high or low memory regime.
</p>
"
On Greedy Approaches to Hierarchical Aggregation. (arXiv:2102.01730v2 [cs.DS] UPDATED),http://arxiv.org/abs/2102.01730,"<p>We analyze greedy algorithms for the Hierarchical Aggregation (HAG) problem,
a strategy introduced in [Jia et al., KDD 2020] for speeding up learning on
Graph Neural Networks (GNNs). The idea of HAG is to identify and remove
redundancies in computations performed when training GNNs. The associated
optimization problem is to identify and remove the most redundancies.
</p>
<p>Previous work introduced a greedy approach for the HAG problem and claimed a
1-1/e approximation factor. We show by example that this is not correct, and
one cannot hope for better than a 1/2 approximation factor. We prove that this
greedy algorithm does satisfy some (weaker) approximation guarantee, by showing
a new connection between the HAG problem and maximum matching problems in
hypergraphs. We also introduce a second greedy algorithm which can out-perform
the first one, and we show how to implement it efficiently in some parameter
regimes. Finally, we introduce some greedy heuristics that are much faster than
the above greedy algorithms, and we demonstrate that they perform well on
real-world graphs.
</p>
"
Provably Secure Federated Learning against Malicious Clients. (arXiv:2102.01854v2 [cs.CR] UPDATED),http://arxiv.org/abs/2102.01854,"<p>Federated learning enables clients to collaboratively learn a shared global
model without sharing their local training data with a cloud server. However,
malicious clients can corrupt the global model to predict incorrect labels for
testing examples. Existing defenses against malicious clients leverage
Byzantine-robust federated learning methods. However, these methods cannot
provably guarantee that the predicted label for a testing example is not
affected by malicious clients. We bridge this gap via ensemble federated
learning. In particular, given any base federated learning algorithm, we use
the algorithm to learn multiple global models, each of which is learnt using a
randomly selected subset of clients. When predicting the label of a testing
example, we take majority vote among the global models. We show that our
ensemble federated learning with any base federated learning algorithm is
provably secure against malicious clients. Specifically, the label predicted by
our ensemble global model for a testing example is provably not affected by a
bounded number of malicious clients. Moreover, we show that our derived bound
is tight. We evaluate our method on MNIST and Human Activity Recognition
datasets. For instance, our method can achieve a certified accuracy of 88% on
MNIST when 20 out of 1,000 clients are malicious.
</p>
"
Causal Collaborative Filtering. (arXiv:2102.01868v2 [cs.IR] UPDATED),http://arxiv.org/abs/2102.01868,"<p>Recommender systems are important and valuable tools for many personalized
services. Collaborative Filtering (CF) algorithms -- among others -- are
fundamental algorithms driving the underlying mechanism of personalized
recommendation. Many of the traditional CF algorithms are designed based on the
fundamental idea of mining or learning correlative patterns from data for
matching, including memory-based methods such as user/item-based CF as well as
learning-based methods such as matrix factorization and deep learning models.
However, advancing from correlative learning to causal learning is an important
problem, because causal/counterfactual modeling can help us to think outside of
the observational data for user modeling and personalization. In this paper, we
propose Causal Collaborative Filtering (CCF) -- a general framework for
modeling causality in collaborative filtering and recommendation. We first
provide a unified causal view of CF and mathematically show that many of the
traditional CF algorithms are actually special cases of CCF under simplified
causal graphs. We then propose a conditional intervention approach for
$do$-calculus so that we can estimate the causal relations based on
observational data. Finally, we further propose a general counterfactual
constrained learning framework for estimating the user-item preferences.
Experiments are conducted on two types of real-world datasets -- traditional
and randomized trial data -- and results show that our framework can improve
the recommendation performance of many CF algorithms.
</p>
"
Revealing Critical Characteristics of Mobility Patterns in New York City during the Onset of COVID-19 Pandemic. (arXiv:2102.01918v2 [physics.soc-ph] UPDATED),http://arxiv.org/abs/2102.01918,"<p>New York has become one of the worst-affected COVID-19 hotspots and a
pandemic epicenter due to the ongoing crisis. This paper identifies the impact
of the pandemic and the effectiveness of government policies on human mobility
by analyzing multiple datasets available at both macro and micro levels for the
New York City. Using data sources related to population density, aggregated
population mobility, public rail transit use, vehicle use, hotspot and
non-hotspot movement patterns, and human activity agglomeration, we analyzed
the inter-borough and intra-borough moment for New York City by aggregating the
data at the borough level. We also assessed the internodal population movement
amongst hotspot and non-hotspot points of interest for the month of March and
April 2020. Results indicate a drop of about 80% in people's mobility in the
city, beginning in mid-March. The movement to and from Manhattan showed the
most disruption for both public transit and road traffic. The city saw its
first case on March 1, 2020, but disruptions in mobility can be seen only after
the second week of March when the shelter in place orders was put in effect.
Owing to people working from home and adhering to stay-at-home orders,
Manhattan saw the largest disruption to both inter- and intra-borough movement.
But the risk of spread of infection in Manhattan turned out to be high because
of higher hotspot-linked movements. The stay-at-home restrictions also led to
an increased population density in Brooklyn and Queens as people were not
commuting to Manhattan. Insights obtained from this study would help
policymakers better understand human behavior and their response to the news
and governmental policies.
</p>
"
On Query-efficient Planning in MDPs under Linear Realizability of the Optimal State-value Function. (arXiv:2102.02049v2 [cs.LG] UPDATED),http://arxiv.org/abs/2102.02049,"<p>We consider the problem of local planning in fixed-horizon Markov Decision
Processes (MDPs) with a generative model under the assumption that the optimal
value function lies in the span of a feature map that is accessible through the
generative model. As opposed to previous work where linear realizability of all
policies was assumed, we consider the significantly relaxed assumption of a
single linearly realizable (deterministic) policy. A recent lower bound
established that the related problem when the action-value function of the
optimal policy is linearly realizable requires an exponential number of
queries, either in H (the horizon of the MDP) or d (the dimension of the
feature mapping). Their construction crucially relies on having an
exponentially large action set. In contrast, in this work, we establish that
poly$(H, d)$ learning is possible (with state value function realizability)
whenever the action set is small (i.e. O(1)). In particular, we present the
TensorPlan algorithm which uses poly$((dH/\delta)^A)$ queries to find a
$\delta$-optimal policy relative to any deterministic policy for which the
value function is linearly realizable with a parameter from a fixed radius ball
around zero. This is the first algorithm to give a polynomial query complexity
guarantee using only linear-realizability of a single competing value function.
Whether the computation cost is similarly bounded remains an interesting open
question. The upper bound is complemented by a lower bound which proves that in
the infinite-horizon episodic setting, planners that achieve constant
suboptimality need exponentially many queries, either in the dimension or the
number of actions.
</p>
"
Federated Learning on Non-IID Data Silos: An Experimental Study. (arXiv:2102.02079v2 [cs.LG] UPDATED),http://arxiv.org/abs/2102.02079,"<p>Machine learning services have been emerging in many data-intensive
applications, and their effectiveness highly relies on large-volume
high-quality training data. However, due to the increasing privacy concerns and
data regulations, training data have been increasingly fragmented, forming
distributed databases of multiple data silos (e.g., within different
organizations and countries). To develop effective machine learning services,
there is a must to exploit data from such distributed databases without
exchanging the raw data. Recently, federated learning (FL) has been a solution
with growing interests, which enables multiple parties to collaboratively train
a machine learning model without exchanging their local data. A key and common
challenge on distributed databases is the heterogeneity of the data
distribution (i.e., non-IID) among the parties. There have been many FL
algorithms to address the learning effectiveness under non-IID data settings.
However, there lacks an experimental study on systematically understanding
their advantages and disadvantages, as previous studies have very rigid data
partitioning strategies among parties, which are hardly representative and
thorough. In this paper, to help researchers better understand and study the
non-IID data setting in federated learning, we propose comprehensive data
partitioning strategies to cover the typical non-IID data cases. Moreover, we
conduct extensive experiments to evaluate state-of-the-art FL algorithms. We
find that non-IID does bring significant challenges in learning accuracy of FL
algorithms, and none of the existing state-of-the-art FL algorithms outperforms
others in all cases. Our experiments provide insights for future studies of
addressing the challenges in data silos.
</p>
"
BeFair: Addressing Fairness in the Banking Sector. (arXiv:2102.02137v2 [cs.LG] UPDATED),http://arxiv.org/abs/2102.02137,"<p>Algorithmic bias mitigation has been one of the most difficult conundrums for
the data science community and Machine Learning (ML) experts. Over several
years, there have appeared enormous efforts in the field of fairness in ML.
Despite the progress toward identifying biases and designing fair algorithms,
translating them into the industry remains a major challenge. In this paper, we
present the initial results of an industrial open innovation project in the
banking sector: we propose a general roadmap for fairness in ML and the
implementation of a toolkit called BeFair that helps to identify and mitigate
bias. Results show that training a model without explicit constraints may lead
to bias exacerbation in the predictions.
</p>
"
Circuit Complexity From Supersymmetric Quantum Field Theory With Morse Function. (arXiv:2101.12582v1 [hep-th] CROSS LISTED),http://arxiv.org/abs/2101.12582,"<p>Computation of circuit complexity has gained much attention in the
Theoretical Physics community in recent times to gain insights about the
chaotic features and random fluctuations of fields in the quantum regime.
Recent studies of circuit complexity take inspiration from the geometric
approach of Nielsen, which itself is based on the idea of optimal quantum
control in which a cost function is introduced for the various possible path to
determine the optimum circuit. In this paper, we study the relationship between
the circuit complexity and Morse theory within the framework of algebraic
topology using which we study circuit complexity in supersymmetric quantum
field theory describing both simple and inverted harmonic oscillators up to
higher orders of quantum corrections. The expression of circuit complexity in
quantum regime would then be given by the Hessian of the Morse function in
supersymmetric quantum field theory, and try to draw conclusion from their
graphical behaviour. We also provide a technical proof of the well known
universal connecting relation between quantum chaos and circuit complexity of
the supersymmetric quantum field theories, using the general description of
Morse theory.
</p>
"
